\documentclass[reprint,aip,jcp,superscriptaddress]{revtex4-1}
%\documentclass[aip,jcp,preprint,superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{tikz}
\usepackage{color}
\begin{document}




\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}




\title{Accelerating the weighted histogram analysis method
by direct inversion in the iterative subspace}

\begin{abstract}
The convergence of the weighted histogram analysis method for free energy calculation
can be improved by using the method of direct inversion in the iterative subspace.
\end{abstract}

\maketitle




\section{Introduction}





The multiple histogram method\cite{
ferrenberg1988, *ferrenberg1989}
or its generalization,
the weighted histogram analysis method (WHAM)\cite{
kumar1992,
bartels1997, *habeck2007, *habeck2012,
souaille2001, kastner2005,
chodera2007, bereau2009,
kim2011},
is a useful tool
of calculating free energy
in computer simulations\cite{
newman, *frenkel}.
%
Given several trajectories collected
for different thermodynamic states,
the method yields optimal estimates
of the free energy differences
among the states.
%
Here, the free energy
and the density of states
are determined
from a set of equations involving
the (energy) histograms,
hence the name of the method.
%
One can also avoid the histogram dependence
and reformulate WHAM
as the multistate Bennett acceptance ratio (MBAR) method\cite{
shirts2008}
as an extension of the Bennett acceptance ratio (BAR) method\cite{
bennett1976}.
%
The statistical efficiency
%of using data
makes WHAM and MBAR popular trajectory analysis tools
for enhanced-sampling simulations,
such as umbrella sampling\cite{
torrie1974, *laio2002},
and tempering\cite{
marinari1992, *lyubartsev1992,
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997},
and even single-molecule experiments\cite{
shirts2008}.



The straightforward implementation of WHAM or MBAR,
in which the equations regarding the free energy
are solved by direct iteration,
can suffer from
a slow convergence in later stages.
%
Several remedies were proposed\cite{
shirts2008, bereau2009, kim2011}.
%
For example, one may use the Newton-Raphson method,
which involves the Hessian matrix,
although the approach sometimes can be unstable\cite{
shirts2008}.
%
An ingenious non-iterative alternative is
the statistical-temperature WHAM (ST-WHAM)\cite{
kim2011},
which can also be considered as a refinement of
the umbrella integration method (UIM)\cite{
kastner2005}.
%
Here,
one non-iteratively determines the density of states
from integrating its logarithmic derivative,
or the statistical temperature,
which is computed from averaging the values
over the simulated temperatures
using the histogram heights as the weights.
%
This variant, however, gives
a slightly different estimate of the free energy
from the original WHAM,
and the extension to multiple-parameter ensembles,
e.g., the isothermal-isobaric ensemble,
can be numerically challenging\cite{kim2011}.



Here, we consider improving WHAM using
the method of direct inversion in the iterative subspace (DIIS)\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011}.
%
Although the method is still iterative,
we shall see that
the rate of convergence can often be
significantly improved in difficult cases.





\section{Method}





\subsection{WHAM}



WHAM is a method of
estimating the free energy differences
among a few thermodynamic states
with different parameters,
such as temperatures, pressures, etc.
%
For definiteness,
consider the special case of $K$ temperatures,
labeled by the inverse temperature,
$\beta = 1/(k_B T)$,
as
$\beta_1, \ldots, \beta_K$.
%
Suppose we have performed the respective
canonical ($NVT$) ensemble simulations
at the $K$ temperatures,
we wish to estimate the free energies
at the temperatures.



In WHAM,
we first estimate the density of states $g(E)$ from
%
\begin{equation}
g(E)
=
\frac{
  \sum_{j = 1}^K n_j(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
},
\label{eq:gE_WHAM}
\end{equation}
%
where,
$n_j(E)$,
the unnormalized energy distribution
observed from simulation $j$,
is usually estimated
from the energy histogram as
the number of independent trajectory frames
whose energies fall in the interval
$(E - \Delta E/2, E + \Delta E/2)$
divided by $\Delta E$
(we shall omit ``independent'' below for simplicity);
%
$N_k$
is the total number of frames
from simulation $k$;
%
$Z_k$
is the partition function,
defined as
%
\begin{equation}
Z_k
=
\int g(E) \, \exp(-\beta_k E) \, dE.
\label{eq:Z}
\end{equation}



To show Eq. \eqref{eq:gE_WHAM},
we first observe, for any $k$,
\begin{equation}
  g(E) = n_k(E) / d_k(E),
  \label{eq:gnk}
\end{equation}
where $d_k(E) \equiv N_k \exp(-\beta_k E)/Z_k$.
%denoting the $k$th term on the denominator.
%
To combine the estimates from different $k$'s,
the relative weight should be inversely
proportional to the variance\cite{
newman, *frenkel},
$\mathrm{var}(n_k/d_k) \approx n_k / d_k^2 \propto 1/d_k$,
where we have assumed $\mathrm{var}(n_k) = n_k$,
and $N_k \gg n_k$ so that
the relative error of $d_k$ is negligible
compared to that of $n_k$.
%
Linearly combining Eq. \eqref{eq:gnk}
using $d_k$ as the relative weight yields
Eq. \eqref{eq:gE_WHAM}\cite{souaille2001}.



From Eqs. \eqref{eq:gE_WHAM} and \eqref{eq:Z},
we find that the dimensionless free energies
$f_i \equiv -\log Z_i$
satisfy
\begin{align}
f_i
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K n_j(E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\label{eq:f_WHAM}
\\
&\equiv
-\log \Z_i(\{ f_k \}),
\notag
\end{align}
%
where,
$\Z_i$
denotes the integral on the right-hand side.
%
Once, $f_i$ and $g(E)$ are determined,
the free energy at a not simulated temperature, $\beta$,
can be found from Eq. \eqref{eq:Z}
by substituting $\beta$ for $\beta_k$.



In the above,
$n_j(E)$ is estimated from the energy histogram.
%
To avoid this dependency,
we notice from definition that\cite{
souaille2001}
%
\begin{equation}
n_j(E)
=
\sum_{\vx}^{(j)} \delta(\E(\vx) - E),
\label{eq:n_delta}
\end{equation}
%
where,
$\E(\vx)$
is the energy function,
and
$\sum_{\vx}^{(j)}$
denotes the sum over trajectory frames
of simulation $j$.
%
%
%
Using Eq. \eqref{eq:n_delta}
in Eq. \eqref{eq:f_WHAM} yields
the MBAR result\cite{
kumar1992, souaille2001, shirts2008}:
%
\begin{equation}
f_i
=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}.
\label{eq:f_MBAR}
\end{equation}
%
where,
$q_i(\vx) \equiv \exp[ -\beta_i \, \E(\vx) ]$
%
(see Appendix \ref{sec:deriveMBAR}
for a more general derivation).
%
Since both Eqs. \eqref{eq:f_WHAM} and \eqref{eq:f_MBAR}
are invariant under $f_i \rightarrow f_i + c$
for all $i$ and an arbitrary $c$,
$f_i$ are determined up to a constant shift.



We briefly mention a few extensions.
%
First,
Eq. \eqref{eq:f_MBAR}
is valid for a general Hamiltonian
with a linear bias
\[
\Ham_i(\vx) = \Ham_0(\vx) + \lambda_i \, \W(\vx),
\]
such that $q_i = \exp\left[ -\Ham_i(\vx) \right]$.
%
In this case,
inserting
$\int \delta(\W(\vx) - W) \, dW$
into Eq. \eqref{eq:f_MBAR}
yields
%
\begin{align}
f_i
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K n_j(W) \, \exp(-\lambda_j W)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\lambda_k W + f_k)
    }
    d W,
\label{eq:fx_WHAM}
\end{align}
%
where
$n_j(W)$
is understood as
the unnormalized distribution of
the bias $\W(\vx)$.
%
Equation \eqref{eq:fx_WHAM}
is reduced to Eq. \eqref{eq:f_WHAM}
if
$\Ham_0(\vx) = 0$,
$\W(\vx) = \E(\vx)$,
and $\lambda_i = \beta_i$.
%
Further,
$\lambda_i$ and $W$
can be generalized
to vectors as
$\bm{\lambda}_i$
and
$\vct W$, respectively.
%
For example,
for simulations
on multiple isothermal-isobaric ($NpT$) ensembles
with different temperatures and pressures,
%
we set
$\bm{\lambda}_i = (\beta_i, \beta_i p_i)$
and
$\vct W = (E, V)$
with
$p_i$ and $V$
being the pressure and volume,
respectively.
%
If the Hamiltonian $\Ham_i(\vx)$
depends nonlinearly on $\lambda_i$,
one prefers the MBAR method\cite{
shirts2008},
which is based directly on
Eq. \eqref{eq:f_MBAR},
instead of the histogram-based Eq. \eqref{eq:fx_WHAM}.
%
Besides,
non-Boltzmann
(e.g., the multicanonical\cite{
mezei1987, *berg1992, *lee1993},
Tsallis\cite{tsallis1988},
microcanonical\cite{
yan2003, *martin-mayor2007, *zhang2013})
weight
can be used in place of the Boltzmann one
for simulations in the corresponding ensembles.
%



To find $f_i$ in WHAM,
one often treats Eq. \eqref{eq:f_WHAM}
as an iterative equation,
%
\begin{equation*}
f_i^\mathrm{(new)}
=
-\log \Z_i\left(
  \{ f_k^\mathrm{(old)} \}
\right).
\end{equation*}
%
This approach, or direct WHAM below,
can suffer from a slow convergence
(cf. Appendix \ref{sec:convwham}),
resulting in thousands of iterations\cite{
bereau2009, kim2011}.
%
An elegant non-iterative alternative, ST-WHAM,
approximates the logarithmic derivative of $g(E)$
in Eq. \eqref{eq:gE_WHAM} as\cite{
kim2011}
%
\begin{equation}
(\log g)'(E)
\approx
\frac{
  \sum_j n_j'(E) + n_j(E) \, \beta_j
}
{
  \sum_k n_k(E)
},
\label{eq:beta_STWHAM}
\end{equation}
%
whose integral yields $\log g(E)$.
%
ST-WHAM, however,
is more sensitive to the bin size
and its multidimensional generalization
is numerically challenging
owing to the difficulty of
computing the derivative,
$n'_j(E)$\cite{
kim2011}.
%
Thus, UIM\cite{
kastner2005}
further approximates
the distribution $n'_j(E)$ by a Gaussian
(see Appendix \ref{sec:approxf}
for a discussion on the accuracy).



Because of the limitations of the alternatives,
below we shall still focus on the original WHAM,
and give a numerical technique
that accelerates the solution
of Eq. \eqref{eq:f_WHAM}.
%
The technique is equally applicable to MBAR.





\subsection{DIIS}



DIIS is a method of solving a set of
(nearly) linear equations\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011}.
%
Here, it is used
to solve Eq. \eqref{eq:f_WHAM}.
%
A schematic illustration
is shown in Fig. \ref{fig:scheme}.
%
We first represent an approximate solution
by a trial vector,
$\vct f = (f_1, \dots, f_K)$,
which is, in our case, the vector of
the dimensionless free energies.
%
The target equations can be written as
%
\begin{equation}
  R_i(\vct f) = 0  \quad i = 1, \ldots, K,
  \label{eq:R_f}
\end{equation}
%
whose left-hand side forms
a residual or correction vector
$\vct R = (R_1, \dots, R_K)$.
%
The magnitude
$\| \vct R \|$
represents the error.
%
The signs of $R_i$ are arranged such that
$\vct R(\vct f)$
normally gives a direction
of reducing the error of $\vct f$.
%
That is,
for $\vct f' = \vct f + \alpha \, \vct R(\vct f)$
with a sufficiently small $\alpha$,
we expect
%
$\| \vct R(\vct f') \| < \| \vct R(\vct f) \|$.



Suppose now we have a basis consisting of $M$ trial vectors
$\vct f_1$, \dots $\vct f_M$
(where $M$ can be much less than $K$),
%
and the residual vectors are
$\vct R_1$, \dots $\vct R_M$
[where $\vct R_j \equiv \vct R(\vct f_j)$
for $j = 1, \dots, M$],
%
we wish to construct a more accurate solution
from the vectors.



To do so, we first find the combination of the residual vectors
$\vct{\hat R} = \sum_i c_i \, \vct R_i$,
that minimizes the error
$\left\| \vct{\hat R} \right\|$
under the constraint
\begin{equation}
  \sum_i c_i = 1.
  \label{eq:c_normalize}
\end{equation}
%
That is,
we find $c_i$ from Eq. \eqref{eq:c_normalize} and
\begin{equation*}
  \sum_j \left( \vct R_i \cdot \vct R_j \right) \, c_j = \lambda,
  %\label{eq:cj_DIIS}
\end{equation*}
for all $i$,
with $\lambda$ being the Lagrange multiplier.
%
If Eq. \eqref{eq:R_f} is nearly linear
around the solutions,
%
$\vct{\hat R}$
is roughly the residual vector of
$\vct{\hat f} = \sum_i c_i \, \vct f_i$.
%
This means,
among all linear combinations of
$\{ \vct f_i \}$,
$\vct{\hat f}$
is roughly the least erroneous
and closest to the true solution,
$\vct f^*$.
%
Thus,
an iteration based on
$\vct{\hat f}$
would be most efficient.



We now construct a new trial vector $\vct f^{(n)}$ as
%
\begin{equation}
\vct f^{(n)}
=
\vct{\hat f}
+
\alpha \, \vct{\hat R}( \vct{\hat f} ),
\end{equation}
%
where the factor $\alpha$ is $1.0$ in this study
(although a smaller value is recommended
for other applications\cite{kovalenko1999, howard2011}).
%
The new vector $\vct f^{(n)}$
is used to update the basis as follows.





\tikzstyle{emptydot}=[inner sep=0pt,minimum size=0.0mm]
\tikzstyle{fRarrow1}=[->, very thick, draw={rgb:red,4;white,1;gray,1}]
\tikzstyle{fRarrow2}=[->, very thick, draw={rgb:blue,4;white,1;gray,1}]
\tikzstyle{fRarrowx}=[->, very thick]
\tikzstyle{fRarr}=[->, thin]
\tikzstyle{fRlabel}=[inner sep=0pt, text=black!80!white]

\begin{figure}
  \begin{tikzpicture}
    %
    %
    % The potential function to minimize is
    %
    % F = 3/8 (x - 3/5)^2 + 5/8 (y - 4/5)^2 + 1/4 (x - 3/5) (y - 4/5)
    %   = 7/2 R^2
    %
    % x = 3/5
    %   + (sqrt(2) - 1) sqrt(3 + sqrt(2)) cos t
    %   + (sqrt(2) + 1) sqrt(3 - sqrt(2)) sin t.
    %
    % y = 4/5
    %   + sqrt(3 + sqrt(2)) cos t
    %   - sqrt(3 - sqrt(2)) sin t.
    %
    % The long axis is achieved at t = Pi/2
    % its length is sqrt(8 + 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) + 1, -1),
    % which has an angle of -22.5 degrees
    % with the x axis.
    %
    % The short axis is achieved at t = 0,
    % its length is sqrt(8 - 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) - 1, 1),
    % which has an angle of 67.5 degrees
    % with the x axis.
    %
    %
    \newcommand{\sz}{4cm}
    \node (label-A) at (-0.10*\sz, 1.28*\sz) [label=below:{(a)}] {};
    \node (label-B) at ( 1.20*\sz, 1.28*\sz) [label=below:{(b)}] {};
    \begin{scope}
      %\clip ({-0.08*\sz}, {-0.08*\sz}) rectangle ({1.3*\sz}, {1.15*\sz});
      %
      % Draw ellipses
      %
      \foreach \i in {0,...,3}
      {
        \pgfmathsetmacro{\Fval}{0.13 - \i * 0.04};
        \pgfmathsetmacro{\Rval}{sqrt(\Fval*2/7)};
        \pgfmathsetmacro{\aval}{sqrt(8 + sqrt(8))*\Rval};
        \pgfmathsetmacro{\bval}{sqrt(8 - sqrt(8))*\Rval};
        \pgfmathsetmacro{\colora}{12 + 8 * \i};
        \draw[fill,
              gray!\colora!white,
              rotate around={67.5:(0.6*\sz, 0.8*\sz)}]
          (0.6*\sz, 0.8*\sz)
          ellipse ({\bval*\sz} and {\aval*\sz});
      }
      %
      %
      %
      \foreach \i in {1,...,19}
      {
        \pgfmathsetmacro\lambda{\i * 0.05};
        \pgfmathsetmacro\x{1 - \lambda};
        \pgfmathsetmacro\y{\lambda};
        \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda};
        \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda};
        \pgfmathsetmacro\xx{\x + \dx};
        \pgfmathsetmacro\yy{\y + \dy};
        \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
          (0, 0) -- (\x*\sz, \y*\sz) -- (\xx*\sz, \yy*\sz);
      }
      %
      %
      %
      % the minimum
      %
      %
      \draw
          (0.6*\sz, 0.8*\sz)
          node
            [ fill, circle, inner sep=0pt, minimum size=1.5mm,
              label={
                [inner sep=0.01*\sz]
                -45:{$\vct f^*$}
              }
            ] {};
      %
      % vector 1
      %
      \node (f1R1) at (0.9*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.05*\sz},
                  rotate=-83.7]
               20:{$\vct R_1$}}
        ] {};
      \node (f1) at (1.0*\sz, 0)
        [emptydot, label=below:{$\vct f_1$}]{}
        edge[fRarrow1] (f1R1);
      %
      % vector 2
      %
      \node (f2R2) at (0.4*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.03*\sz},
                  rotate=-14.0]
               93:{$\vct R_2$}}
        ] {};
      \node (f2) at (0, 1.0*\sz)
        [emptydot, label=left:{$\vct f_2$}]{}
        edge[fRarrow2] (f2R2);
      %
      %
      %
      \draw [gray, dashed, thin]
        (-0.1*\sz, 1.1*\sz) -- (1.1*\sz, -0.1*\sz);
      %
      %
      %
      % optimal vector
      %
      %
      \node (fhatRhat) at (0.52*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.02*\sz},
                  rotate=26.6]
                -150:{$\vct {\hat R}$}}
        ] {};
      \node (fhat) at (0.24*\sz, 0.76*\sz)
        [ emptydot, inner sep=0,
          label={
            [label distance={0.03*\sz}]
            180:{$\vct {\hat f}$}
          }
        ] {}
        edge[fRarrowx] (fhatRhat);
      %
      %
      %
      \node (origin) at (0, 0) [emptydot]{}
        edge[fRarrow1] (f1)
        edge[fRarrow2] (f2)
        edge[fRarrowx] (fhat);
    \end{scope}
    %
    %
    %
    %
    % origin of the residual vectors
    %
    %
    %
    \newcommand{\Rox}{1.45*\sz}
    \newcommand{\Roy}{0.13*\sz}
    %
    %
    %
    \foreach \i in {1,...,19}
    {
      \pgfmathsetmacro\lambda{\i * 0.05}
      \pgfmathsetmacro\x{1 - \lambda};
      \pgfmathsetmacro\y{\lambda};
      \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda}
      \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda}
      \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
        (\Rox, \Roy) -- (\Rox + \dx*\sz, \Roy + \dy*\sz);
    }
    %
    %
    %
    \node (R1) at ({\Rox - 0.1 * \sz}, {\Roy + 0.9 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_1$}}
      ] {};
    %
    %
    %
    \node (R2) at ({\Rox + 0.4 * \sz}, {\Roy - 0.1 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_2$}}
      ] {};
    %
    %
    %
    \draw [gray, dashed, thin]
         ({\Rox - 0.15 * \sz}, {\Roy + 1.0 * \sz})
      -- ({\Rox + 0.45 * \sz}, {\Roy - 0.2 * \sz});
    %
    %
    %
    % Draw the perpendicular sign
    %
    %
    %
    \draw [thick]
          ({\Rox + 0.24*\sz}, {\Roy + 0.12*\sz})
       -- ({\Rox + 0.22*\sz}, {\Roy + 0.16*\sz})
       -- ({\Rox + 0.26*\sz}, {\Roy + 0.18*\sz});
    %
    %
    %
    \node (Rhat) at ({\Rox+0.28*\sz}, {\Roy+0.14*\sz})
      [ emptydot,
        label={[fRlabel, label distance=0.1mm]
               10:{$\vct {\hat R}$}}
      ] {};
    %
    %
    %
    \node (Rorigin) at (\Rox, \Roy) [emptydot] {}
      edge[fRarrow1] (R1)
      edge[fRarrow2] (R2)
      edge[fRarrowx] (Rhat);
    %
    %
    %
  \end{tikzpicture}
  %
  %
  %
  \caption{\label{fig:scheme}
    Schematic illustration of the method of
    direct inversion of the iterative subspace (DIIS)
    for solving a set of equations.
    %
    Each trial vector $\vct f_i$
    represents an approximate solution,
    and the residual vector $\vct R_i$
    represents the correction.
    %
    Given a basis of a few (two here) trial vectors,
    DIIS seeks the combination
    $\vct {\hat R} = \sum_i c_i \, \vct R_i$
    that minimizes the magnitude
    $\left\| \vct{\hat R} \right\|$
    under the constraint
    $\sum_i c_i = 1$ [panel (b)].
    %
    The corresponding combination
    of the trial vectors,
    $\vct {\hat f} = \sum_i c_i \, \vct f_i$,
    is used to construct
    the new trial vector as
    $\vct f^{(n)} = \vct {\hat f} + \vct {\hat R}$.
    %
    Then, $\vct f^{(n)}$ is used to
    update the basis
    for the next round of iteration.
  }
\end{figure}




\subsection{Basis updating}



In each iteration of DIIS,
the basis is updated
by the new trial vector $\vct f^{(n)}$
from the above step.
%
Initially,
the basis contains a single vector.
%
As we add more vectors into the basis,
some old vectors are removed
to maintain a maximal size of $M$.



In a popular updating scheme\cite{kovalenko1999},
the basis is treated as a queue:
%
we add $\vct f^{(n)}$ to the basis,
if the latter contains fewer than $M$ vectors,
%
or substitute $\vct f^{(n)}$ for the earliest vector in the basis.
%
If, however, $\vct f^{(n)}$
produces an error greater than
$K_r$ times the error of
$\vct f_\mathrm{min}$,
the least erroneous vector in the basis,
%
we rebuild the basis
from $\vct f_\mathrm{min}$.
%
Here, the error of a vector $\vct f$ is defined as
$\left\| \vct R(\vct f) \right\|$,
and
$K_r = 10.0$ as recommended\cite{
kovalenko1999}.



We used the following modification
in this study.
%
% Howard-Pettitt can be problematic for villin-headpiece NVT
%
%%
%When the basis contains $M$ vectors,
%we find the most erroneous vector,
%$\vct f_\mathrm{max}$, from the basis,
%and replace it by $\vct f^{(n)}$.
%%
%If, however,
%$\vct f_\mathrm{max}$
%is the updated vector in the previous step,
%the process is stagnant.
%%
%In this case,
%we rebuild the basis from $\vct f^{(n)}$.
%
% Below is another variant
%
First, we find the most erroneous vector,
$\vct f_\mathrm{max}$, from the basis.
%
If the new vector, $\vct f^{(n)}$,
produces an error less than $\vct f_\mathrm{max}$,
we add $\vct f^{(n)}$ into the basis
or, if the basis is full,
substitute $\vct f^{(n)}$ for $\vct f_\mathrm{max}$.
%
Otherwise,
we remove $\vct f_\mathrm{max}$ from the basis,
and if this empties the basis,
we rebuild the basis from $\vct f^{(n)}$.



Since the DIIS process is reduced
to the direct iteration
if $M = 1$,
the method is effective
only if multiple bases are used.





\section{Results}





We tested DIIS WHAM and MBAR on three systems:
Ising model,
villin headpiece,
and
Lennard-Jones (LJ) fluid
(see Secs.
\ref{sec:results_Ising},
\ref{sec:results_villin},
and
\ref{sec:results_LJ},
respectively,
for details).
%
We tuned the parameters
such that direct WHAM and MBAR
would take thousands of iterations
to finish.
%
The main results
are summarized
in Fig. \ref{fig:nsnt},
from which one can see that
DIIS can speed up WHAM and MBAR
dramatically in these cases.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/nsnt.pdf}
  }
  \caption{
    \label{fig:nsnt}
    Number of iterations
    and run time
    versus the number of bases, $M$,
    in DIIS.
    %
    Here, the error tolerance
    $\epsilon$ is $10^{-8}$ for WHAM
    or $10^{-7}$ for MBAR.
    %
    The $M = 0$ points represent direct WHAM.
    %
    The run times are inversely scaled
    by a factor $\tau$
    for better comparison.
    %
    The four test cases are
    Ising model in the $NVT$ ensemble with WHAM,
    villin headpiece in the $NVT$ ensemble with WHAM and MBAR,
    and
    LJ fluid in the $NpT$ ensemble with WHAM.
    %
    The scaling factors $\tau$ are
    {\color{red} $9.8$, $9.8$, $1250$, and $1525$} seconds,
    respectively.
    %
    For the Ising model and LJ fluid,
    the results were averaged
    over many independent samples.
    %
    For the villin headpiece,
    roughly 10\% of the trajectory frames
    were randomly selected
    to build a sample each time.
    %
    The results were then averaged.
    %
    The lines are to guide the eyes.
  }
\end{figure}





\subsection{\label{sec:results_setup}
Set-up}



For simplicity,
we assumed equal autocorrelation times
from different temperatures (and pressures).
%
The approximation should not affect
the general behavior of the methods.



In testing direct and DIIS WHAM,
the initial free energies were obtained from
the single histogram method:
%
\begin{equation*}
f_i - f_{i-1}
=
\log
\left\langle
  \exp\left[
    (\beta_j - \beta_{j-1}) \, E
  \right]
\right\rangle_j.
\end{equation*}
%
Iterations are continued
until all $|R_i|$ are reduced
below a certain tolerance level $\epsilon$.



For ST-WHAM,
the $n_j'(E)$ in Eq. \eqref{eq:beta_STWHAM}
was estimated from
\begin{equation}
n_j'(E)
=
\frac{ n_j(E + \Delta E) - n_j(E - \Delta E) }
     { 2 \, \Delta E },
\label{eq:dn}
\end{equation}
with $\Delta E$ being the bin size
of the energy histogram.





\subsection{\label{sec:results_Ising}
Ising model}





The first system is
the two-dimensional $64\times64$ Ising model.
%
We used parallel tempering\cite{
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997,
*earl2005}
Monte Carlo (MC)
for
eighty temperatures: $T = 1.5$, $1.52$, \dots, $3.08$,
with $10^7$ single-site MC steps on each temperature.



Figure \ref{fig:is2ref} shows that
DIIS and direct WHAMs produced identical
dimensionless free energies, which also
agreed well with the exact results\cite{
ferdinand1969}.
%
On the other hand,
there was slight difference between
the values from ST-WHAM and WHAM,
which was likely due to
the accumulative difference
in the two statistical temperatures.
%
UIM behaved similarly to ST-WHAM,
although the latter is expected to superior
in the long run (cf. Appendix \ref{sec:approxf}).



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2ref.pdf}
  }
  \caption{
    \label{fig:is2ref}
    (a) Dimensionless free energies and
    (b) the logarithm of the density of states
    for the $N = 64\times64$ two-dimensional Ising model.
    %
    The insets show the errors, with
    $\varepsilon(a) \equiv a - a^\mathrm{(ref)}$.
    %
    The bin size $\Delta E = 4$.
    %
    The reference values of the free energies and
    the density of states were obtained
    from Refs. \onlinecite{ferdinand1969} and \onlinecite{beale1996},
    respectively.
  }
\end{figure}




Figure \ref{fig:is2trace}
shows a faster exponential decay of error
in DIIS WHAM
than in direct WHAM.
%
Optimally, DIIS WHAM can
deliver a speedup of two orders of magnitude.
%
As shown in Fig. \ref{fig:nsnt},
the real run time roughly matched
the number of iterations,
suggesting negligible overhead of DIIS.
%
This is unsurprising,
for it is often much more expensive to compute
the right-hand side of Eq. \eqref{eq:f_WHAM}.






\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2trace.pdf}
  }
  \caption{
    \label{fig:is2trace}
    Convergence error, $\max_i \{ |R_i| \}$,
    versus the number of iterations
    in direct and DIIS WHAMs
    for the $64\times64$ two-dimensional Ising model.
    %
    Results were geometrically averaged over independent samples.
    %
    The lines are to guide the eyes.
  }
\end{figure}





\subsection{\label{sec:results_villin}
Villin headpiece}



We tested the methods on a mini-protein:
villin headpiece\cite{duan1998}
(PDB ID: 1VII).
%
The protein was immersed in
a dodecahedron box with 1898 TIP3P water molecules and two chloride ions.
%
MD simulations were performed
using GROMACS\cite{
berendsen1995, *lindahl2001, *vanderspoel2005, *hess2008},
with a time step of 2 fs.
%
Velocity rescaling\cite{bussi2007}
was used as the thermostat with
the time constant being 0.1 ps.
%
The electronic interaction was
handled by the particle mesh Ewald method\cite{
essmann1995}.
%
The constraints were handled by the LINCS method\cite{
hess1997}
for hydrogen-related chemical bonds on the protein
and by the SETTLE method\cite{
miyamoto1992}
for water molecules.
%
Energy frames were registered every 0.1 ps.



We simulated the system under 12 temperatures
$T$ = 300 K, 310 K, \dots, 410 K,
each for about {\color{red} 110} ns.
%
The histogram bin size was $1.0$.
%
As shown in Fig. \ref{fig:nsnt},
direct WHAM suffers from a slow convergence,
while the DIIS version again
delivered a speedup of two orders of magnitude,
in the number of iterations or real run time.



The bin size dependency of WHAM and ST-WHAM
is compared in Fig. \ref{fig:whamcmp}.
%
WHAM was stable for $\Delta E \le 100.0$.
%
The error of ST-WHAM, however,
increased dramatically with decreasing bin size.
%
The bin-size sensitivity was partially due to
the relatively small sample size.
%
Note also that
we used Eq. \eqref{eq:dn} for differentiation
and the trapezoidal rule for integration.
%
Better handling of missing data,
numerical differentiation
and/or integration schemes\cite{
kim2011} may also help improve ST-WHAM.
%
With a sufficiently large bin size,
ST-WHAM and UIM produced similar and reasonable results.
%
The latter is, albeit more approximate,
insensitive to the bin size, and hence more stable.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/whamcmp.pdf}
  }
  \caption{
    \label{fig:whamcmp}
    Dimensionless free energies, $f_i$,
    from WHAM, ST-WHAM and UIM
    for the villin headpiece
    with different bin sizes, $\Delta E$.
    %
    $f_i$ at $T = 300$ K is fixed at zero.
    %
    The results of MBAR, which is
    the zero bin-size limit of WHAM,
    are used as the reference.
    %
    All frames in the trajectory are used.
    %
    The lines are to guide the eyes.
  }
\end{figure}




\subsection{\label{sec:results_LJ}
LJ fluid}


We tested the two-dimensional WHAM
on the 256-particle LJ fluid.
%
The potential was cutoff at half box size.
%
We simulated the system in isothermal-isobaric ($NpT$) ensembles,
both using parallel tempering MC.
%
We simulated the system
under $N_T \times N_p = 6\times 3$ conditions:
the temperatures were $T = 1.2, 1.3, \dots, 1.7$,
the pressures were $p = 0.1, 0.15, 0.2$.
%
The bin sizes for energy and volume
are $1.0$ and $2.0$, respectively.


As shown in Fig. \ref{fig:nsnt},
DIIS WHAM also effectively
reduced the run time significantly.





\section{Conclusions}



We showed that the DIIS technique
can often significantly accelerate
the free energy methods WHAM and MBAR
in difficult cases.
%
The technique achieves a fast convergence
by an optimal combination of the approximate
solutions obtained during iteration.
%
The method is stable and easy to implement
and has only a small run time overhead.



There are several
non-iterative alternatives to WHAM,
although they are less general.
%
For one-dimensional problems
one may use
the non-iterative ST-WHAM
given ample data
and a carefully-chosen bin size.
%
For scarce data,
UIM also offers a reasonable alternative.
%
Besides,
rough estimates
may be drawn from simple formulae
such as
Eqs. \eqref{eq:df_eav} and \eqref{eq:df_eavx}.




\section{Acknowledgments}





It is a pleasure to thank
Dr. Y. Zhao, Dr. M. R. Shirts and J. A. Drake
for many helpful discussions.
%
The authors gratefully acknowledge
the financial support of
the National Science Foundation (No. CHE-1152876)
and
the Robert A. Welch Foundation (No. H-0037).
%
Computer time on the Lonestar supercomputer
at the Texas Advanced Computing Center
at the University of Texas at Austin
is gratefully acknowledged.




\appendix





\section{\label{sec:deriveMBAR}
Probabilistic derivation}



We follow probabilistic approach\cite{
bartels1997, *habeck2007, *habeck2012} to derive
Eq. \eqref{eq:f_MBAR}.
%
We first assume
a (hypothetically nonuniform)
phase-space density of states
$g(\vy)$.
%
Then, the partition function can be written
as a functional of $g$:
%
\begin{equation}
Z_k[g]
=
\int g(\vy) \, q_k(\vy) \, d\vy.
\label{eq:Zg}
\end{equation}
%
Given a certain $g$,
the probability of observing the data set,
$\{ \vx \}$,
is given by
%
\begin{equation*}
p\left( \{ \vx \} | g \right)
=
\prod_{k = 1}^K
\prod_{\vx}^{(k)}
g(\vx) \, q_k(\vx) / Z_k[g],
%\label{eq:pg}
\end{equation*}
%
If we assume a uniform prior distribution of $g$,
then by the Bayes' theorem\cite{leonard},
the posterior distribution of $g$,
given the observed data set,
$p\left( g | \{ \vx \} \right)$,
is also proportional to
$p\left( \{ \vx \} | g \right)$.
%
To find the most probable $g(\vy)$,
we maximize
$p\left( g | \{ \vx \} \right)$
by taking the functional derivative
with respect to $g(\vy)$
and setting it to zero,
which yields
%
\begin{equation}
g(\vy)
=
\frac{
  \sum_{j = 1}^K \sum_{\vx}^{(j)} \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
}.
\label{eq:gy}
\end{equation}
%
Using Eq. \eqref{eq:gy}
in Eq. \eqref{eq:Zg},
and finally setting the $g$ in $Z[g]$ to 1.0
yields Eq. \eqref{eq:f_MBAR}.




\section{\label{sec:convwham}
Model for the convergence of direct WHAM and MBAR}



Here, we use a simple analytical model
to study the convergence of direct WHAM and MBAR.
%
Consider two temperatures,
denoted by $\beta_1$ and $\beta_2$
with distributions $\rho_1(E)$ and $\rho_2(E)$
normalized as
%
\begin{equation}
\int \rho_i(E) \, dE
= 1
\quad
(i = 1, 2).
\label{eq:rho_normalize}
\end{equation}
%
We assume that the two distributions
are symmetric with respect to some $\bar E$,
which is set to zero
by a shift of the origin of energy.
%
Thus,
\begin{equation}
\rho_1(E)
=
\rho_2(-E).
\label{eq:rho_symm}
\end{equation}



One can readily show that,
in terms of $\Delta f = f_2 - f_1$,
Eq. \eqref{eq:f_WHAM} or \eqref{eq:f_MBAR}
can be written as
%
\begin{align}
\Delta f
&=
-\log \int
\frac{ \rho_1(E) + \rho_2(E) }
{ \exp(\Delta \beta \, E) + \exp \Delta f }
dE
\notag \\
&=
\Delta f
-
\log
  \int
    \frac{ y + \exp \Delta f }
         { y + \cosh \Delta f }
    \rho_1(E) \, dE,
\label{eq:delf_WHAM}
\end{align}
%
where $y \equiv \cosh(\Delta \beta \, E)$,
and we have used
Eqs. \eqref{eq:rho_symm} and
\eqref{eq:rho_normalize}
in the second step.



The solution of Eq. \eqref{eq:delf_WHAM}
is $\Delta f^* = 0$,
which is also apparent from the symmetry.
%
In direct WHAM or MBAR,
Eq. \eqref{eq:delf_WHAM}
is treated as an iterative equation,
with $\Delta f$
on the left- and right-hand sides
being the new and old values,
respectively.
%
For a sufficiently small $\Delta f$,
we can linearize it as
%
\begin{align}
\Delta f^\mathrm{(new)}
&=
\left[
  \int
    \frac{ y } { 1 + y }
    \rho_1(E) \, dE
\right] \,
\Delta f^\mathrm{(old)}
\notag\\
&\equiv
\gamma \,
\Delta f^\mathrm{(old)}.
\label{eq:delf_lin}
\end{align}
%
We expect
$\Delta f$
to decay asymptotically as $\gamma^n$
with the number of iterations, $n$.
%
Since
\[
0 < \gamma < \int
\frac{1 + y}
{1 + y}
\, \rho_1(E) \, dE = 1,
\]
Eq. \eqref{eq:delf_lin}
is always convergent,
and a smaller $\gamma$ means
a faster convergence.
%
However,
if the distribution
is dominated by a large $|E|$,
at which $y \gg 1$,
%
then $\gamma$
can be very close 1.
%
This suggests that
a wide temperature, hence energy, range
can lead to a slow convergence
for direct WHAM or MBAR.




\section{\label{sec:approxf}
Approximate formulae for the free energy}



Since UIM is an approximate method,
we wish to study its accuracy
by a comparison with some simpler formulae.
%
For a pair of temperatures,
$\beta_i$ and $\beta_{i+1}$,
the free energy difference
can be approximated as\cite{park2007}
%
\begin{equation}
\Delta f_i
\approx
\bar E_i \, \Delta \beta_i,
\label{eq:df_eav}
\end{equation}
where
$\Delta f_i = f_{i+1} - f_i$,
$\bar E_i = (\langle E \rangle_{i+1} + \langle E \rangle_i)/2$,
and
$\Delta \beta_i = \beta_{i + 1} - \beta_i$.
%
More accurately,
%
\begin{equation}
\Delta f_i
\approx
\bar E_i \, \Delta \beta_i
+
\frac{1}{12} \left(
  \langle \Delta E^2 \rangle_{i + 1}
  -
  \langle \Delta E^2 \rangle_{i}
\right)
\Delta \beta_i^2.
\label{eq:df_eavx}
\end{equation}
%
The dimensionless free energy, $f_i$,
can then be computed as
$f_1 + \sum_{j = 1}^{i - 1} \Delta f_j$.
%
These equations follow from
the Euler-Maclaurin formula\cite{
arfken}:
%
\begin{align*}
\Delta f_i
&= \int_{\beta_i}^{\beta_{i+1}} f'(\beta) \, d\beta \\
&\approx
\frac{\Delta \beta_i}{2}
\left[
  f'(\beta_{i+1}) + f'(\beta_i)
\right]
\\
&\phantom{=}
-
\sum_{s = 1}^{S}
  \frac{ (\Delta \beta_i)^{2s} } { (2 s)! }
  B_{2s}
  \left[
    f^{(2s)}(\beta_{i+1})
    -
    f^{(2s)}(\beta_i)
  \right],
\end{align*}
%
where $B_{2s}$ is the Bernoulli numbers,
$B_2 = 1/6$, $B_4 = -1/30$, \ldots.
%
Equations \eqref{eq:df_eav} and \eqref{eq:df_eavx}
are the $S = 0$ and $S = 1$ cases,
respectively.



We compared Eqs. \eqref{eq:df_eav} and \eqref{eq:df_eavx}
with UIM\cite{kastner2005}
(which is also an approximate method)
on the $32\times 32$ Ising model,
%
using the exact
$\langle E \rangle_i$
and
$\langle \Delta E^2 \rangle_i$\cite{
ferdinand1969}
(since we wish to study
the accuracy, not precision).
%
Figure \ref{fig:is2approx}
shows that
Eq. \eqref{eq:df_eavx}
was more accurate than
Eq. \eqref{eq:df_eav}.
%
With $\Delta T = 0.2$,
the error of Eq. \eqref{eq:df_eav}
was up to $O(1)$.
%
Generally, the UIM results were more accurate
than those of Eq. \eqref{eq:df_eav},
and less sensitive to the spacing $\Delta T$.
%
However, for a small spacing,
Eq. \eqref{eq:df_eavx} was superior.
%
Thus, with ample data,
UIM should be replaced
by a more accurate alternative, e.g., ST-WHAM.




\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2approx.pdf}
  }
  \caption{
    \label{fig:is2approx}
    Errors of the dimensionless free energies, $f_i$,
    from Eqs. \eqref{eq:df_eav}, \eqref{eq:df_eavx},
    and UIM for the $32\times 32$ Ising model.
    %
    $f_i$ at $T = 1.5$ is fixed at zero.
    %
    Lines are to guide the eyes.
  }
\end{figure}



Finally, we give a cautionary comment
on using Eq. \eqref{eq:df_eav}
for the on-the-fly weight estimates
in simulated tempering\cite{park2007}.
%
Equation \eqref{eq:df_eavx} shows that
the error of Eq. \eqref{eq:df_eav}
is of order $N \, \Delta \beta_i^3$
of a system of size $N$,
since
$\partial \langle \Delta E^2 \rangle / \partial \beta
= -\partial^2 \langle E \rangle / \partial \beta^2$
is extensive.
%
Thus,
if Eq. \eqref{eq:df_eav} is applied to
an $O(1)$ temperature range
with an average temperature separation $\Delta \beta$,
the accumulative error of $f_i$
is $O(N \, \Delta \beta^2)$.
%
If the simulation temperatures
are arranged
such that the energy distributions
of neighboring temperatures
barely overlap,
%
the average energy difference,
$\Delta E = \left| \langle E \rangle_{i+1} - \langle E \rangle_i \right|$,
between neighboring temperatures
is roughly a multiple of the width of
the energy distribution
$\sqrt{ \langle \Delta E^2 \rangle_i } \propto O(\sqrt{N})$.
%
So $\Delta \beta$,
given by $\Delta E / |\partial E/\partial \beta|$,
is $O(1/\sqrt{N})$,
%
and the accumulative error
of $f_i$ is $O(1)$
(as illustrated in Fig. \ref{fig:is2approx}).
%
Thus, if Eq. \eqref{eq:df_eav}
are used for the weights
in simulated tempering\cite{park2007},
the resulting temperature distribution
may deviate appreciably
from the flat one.
%
%One can fix this by choosing
%a finer spacing or
%use Eq. \eqref{eq:df_eavx}
%instead.



\bibliography{simul}
\end{document}
