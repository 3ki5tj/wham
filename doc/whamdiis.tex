\documentclass[reprint,aip,jcp,superscriptaddress]{revtex4-1}
%\documentclass[aip,jcp,preprint,superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\begin{document}




\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\A}{\mathcal{A}}

% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{OliveGreen}\small [\textbf{Comment.} #1]}}



\title{Accelerating the weighted histogram analysis method
by direct inversion in the iterative subspace}


\author{Cheng Zhang}
\author{Chun-Liang Lai}
\author{B. Montgomery Pettitt}
\email{bmpettitt@utmb.edu}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}



\begin{abstract}
The weighted histogram analysis method (WHAM)
for free energy calculations
is a valuable tool to produce
free energy differences with the minimal \repl{bias}{errors}.
\note{
WHAM is supposed to be ``unbiased'';
maybe ``error'' or ``uncertainty'' is a better word?
}
%
\repl{
If we have two (or more) simulations,
we want the best statistical estimator
of the overlap of the density of states.
}
{
Given multiple simulations,
WHAM obtains
from the distribution overlaps
the optimal statistical estimator
of the density of states,
from which the free energy differences
can be computed.
}
%
\note{
We may be introducing too many new things in this sentence,
and it also feels a bit detached from the previous sentence.
The phrase ``overlap of the density of states'' is somewhat vague.
}
%
\repl{This is most often found}{The WHAM equations are often solved}
by an iterative procedure.
%
In this work,
we use a well-known linear algebra algorithm
which allows for
\repl{a}{more}
rapid convergence to the \repl{optimum}{solution}.
%
\note{
We haven't related WHAM to an optimization problem,
so the word ``optimum'' may be a bit confusing.
}
%
We find \add{that} the computational complexity of
the iterative solution to WHAM
can be improved by using the method of
direct inversion in the iterative subspace.
%
We give examples from
\add{lattice models,}
simple liquids
and aqueous protein solutions.
\end{abstract}

\maketitle




\section{Introduction}





An important problem in computational physics and chemistry
is to obtain the best estimate of a quantity of interest
from a given set of data\cite{
shirts2008}.
%
For free energy calculations,
the multiple histogram method\cite{
ferrenberg1988, *ferrenberg1989,
newman, frenkel}
or its generalization,
the weighted histogram analysis method (WHAM)\cite{
kumar1992, roux1995,
bartels1997, *gallicchio2005, *habeck2007, *habeck2012,
souaille2001,
chodera2007, shirts2008, bereau2009,
hub2010, zhu2012},
is an effective tool for
addressing such a problem.
%
WHAM is statistically efficient in using the data
acquired from molecular simulations,
and it has become
a standard free energy analysis tool,
particularly popular
for enhanced-sampling simulations,
such as umbrella sampling\cite{
torrie1974, *laio2002},
and simulated\cite{
marinari1992, *lyubartsev1992}
and parallel\cite{
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997, *earl2005}
tempering.
%and even single-molecule experiments\cite{
%shirts2008}.




Given several distributions collected
at different thermodynamic states,
WHAM obtains the optimal estimate
of the free energies of the states.
%
This problem arises in the calculation of potentials of mean force and a variety of free energy difference methods.
%
The central idea \add{of WHAM} is to find an optimal estimate
of the density of states,
or the unbiased distribution,
which then allows
the free energies to be evaluated
as weighted integrals.
%
The optimal density of states
is computed from a weighted average
of the reweighted energy histograms
(hence the name of the method)
from different distribution realizations or trajectories.
%
The weights, however,
depend on the free energies,
so that
the free energies
and the density of states
must be determined self-consistently.



WHAM can be
\repl{reformulated in the limit of
continuous functions (bins of zero width)\cite{
souaille2001, shirts2008}
}
{
reformulated\cite{
shirts2008}
in the limit of zero histogram bin width
}
as an extension
of the Bennett acceptance ratio (BAR) method\cite{
bennett1976}.
%
\note{
While Kumar's\cite{kumar1992} and Souaille's\cite{souaille2001} papers
definitely contain the key formula, Eq. \eqref{eq:f_MBAR},
Shirts's paper appears to be the one (if I'm not mistaken)
that manages to cast it as an extension of BAR
in the case of more than one temperature.
In this sense, only Shirts's paper should be cited here.
The citation anchor point is adjusted to reflect this.
The citations of the other two papers
are moved to the end of the paragraph.
}
%
This form,
adopted by the multistate BAR (MBAR) method\cite{
shirts2008},
avoids the histogram dependency\cite{
kumar1992, souaille2001, shirts2008}.



The straightforward implementation of WHAM or MBAR,
in which the equations regarding the free energies
are solved by direct iteration,
can suffer from
slow convergence in the later stages.
%
Several remedies have been proposed\cite{
shirts2008, fenwick2008, bereau2009, kim2011, zhu2012}.
%
For example, one may use the Newton-Raphson method,
which involves a \repl{Hessian}{Hessian-like} matrix,
although the approach sometimes can be unstable\cite{
shirts2008}.
%
\note{
The term ``Hessian'' from optimization problems
seems to be imply the matrix is symmetric.
%
So maybe ``Jacobian of the gradient'' is a better word?
%
``Hessian-like'' is an expedient compromise.
}
%
Other more advanced techniques include
the trust region and Broyden-Fletcher-Goldfarb-Shanno (BFGS) methods\cite{zhu2012}.



An elegant non-iterative alternative is
the statistical-temperature WHAM (ST-WHAM)\cite{
fenwick2008, kim2011}, which
determines the density of states
through its logarithmic derivative,
or the statistical temperature.
%
In this way,
the method estimates
the density of states non-iteratively
with minimal approximation.
%by averaging the values
%from different trajectories
%using the histogram heights as the weights.
%
ST-WHAM can be regarded as a refinement of
the more approximate umbrella integration method (UIM)\cite{
kastner2005, *kastner2009}.
%
However, the extension to multidimensional ensembles,
e.g., the isothermal-isobaric ensemble,
can be numerically challenging\cite{kim2011}.



Here, we discuss a numerical improvement of
the implementation of WHAM using
the method of direct inversion in the iterative subspace (DIIS)\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011}.
%
DIIS shares characteristics with other optimization techniques which use a limited (non-spanning) basis of vectors which produce the most gain towards the optimum.
%
Although still iterative in nature,
this implementation
can often improve
the convergence rate
significantly in difficult cases.





\section{Method}





\subsection{WHAM}



WHAM is a method of
estimating the free energies
of multiple thermodynamic states
with different parameters,
such as temperatures, pressures, etc.
%
\add{
Below, we shall first review WHAM
in the particular case of a temperature scan,
since it permits simpler mathematics
without much abstraction.
%
Generalizations to umbrella sampling
and other ensembles are discussed
afterwards.
}




Consider \del{the particular case of }$K$ temperatures,
labeled by\del{ the inverse temperature,}
$\beta = 1/(k_B T)$,
as
$\beta_1, \ldots, \beta_K$.
%
Suppose we have performed the respective
canonical ($NVT$) ensemble simulations
at the $K$ temperatures, and
we wish to estimate the free energies
at those temperatures.



In WHAM,
we first estimate the density of states, $g(E)$, from
%
\begin{equation}
g(E)
=
\frac{
  \sum_{k = 1}^K n_k(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
},
\label{eq:gE_WHAM}
\end{equation}
%
where
$n_k(E)$
is
the unnormalized energy distribution
observed from trajectory $k$,
which
is usually estimated
from the energy histogram as
the number of independent trajectory frames
whose energies fall in the interval
$(E - h/2, E + h/2)$
divided by $h$
(we shall omit ``independent'' below for simplicity);
%
\repl{
$N_k$
is the total number of trajectory frames
}
{
thus, $n_k(E)/N_k$
is the normalized distribution
with $N_k$ being
the total number of trajectory frames
from simulation $k$;
}
%
and finally,
%
\begin{equation}
\exp( - f_k )
=
Z_k
=
\int g(E) \, \exp(-\beta_k E) \, dE,
\label{eq:Z}
\end{equation}
with
$f_k$
and
$Z_k$
being
the dimensionless free energy
and
partition function,
respectively.



To understand Eq. \eqref{eq:gE_WHAM},
we first observe from the definition,
%
\begin{equation}
g(E)
=
\frac{ n_k(E) }
     { d_k(E) }
\qquad
\mbox{for $k = 1, \dots, K$},
\label{eq:gnk}
\end{equation}
where $d_k(E) \equiv N_k \exp(-\beta_k E + f_k)$.
%
\repl{
Then, Eq. \eqref{eq:gE_WHAM}
can be regarded as
a linear combination
of the values from
Eq. \eqref{eq:gnk}\cite{
roux1995, souaille2001, newman, frenkel}.
}
{
That is,
the observed distribution, $n_k(E)/N_k$
should match the exact one, $g(E) \, w_k(E)$,
where $w_k(E) \equiv \exp(-\beta_k E + f_k)$
is the normalized weight of the canonical ensemble.
%
The values of Eq. \eqref{eq:gnk} from different $k$
can be combined to improve the precision,
and Eq. \eqref{eq:gE_WHAM}
is the optiomal combination\cite{
roux1995, souaille2001, newman, frenkel}.
}
%
\repl{
Now in an optimal combination,
}
{
To see this,
recall that in an optimal combination,
}
the relative weight is inversely
proportional to the variance.
%
Assuming a Poisson distribution
so that $\mathrm{var}(n_k) = n_k$,
%
\del{
and $N_k \gg n_k$ so that
the relative error of $d_k$ is negligible
in comparison to that of $n_k$,
}
%
\note{
This phrase is deleted
because $N_k$ is really not a statistical variable,
unless you are running a simulated tempering.
%
In replica exchange and the traditional umbrella sampling,
it is simply the number of simulation steps
spent on simulation $k$.
%
There is, however, a constraint:
$\int n_k(E) \, dE = N_k$,
but it does not affect the result.
%
We probably can get away from such subtleties here,
for this supposedly quick-and-dirty derivation.
}
%
we have
$\mathrm{var}(n_k/d_k) \approx n_k / d_k^2 \propto 1/d_k$.
%
Averaging the values from Eq. \eqref{eq:gnk}
using $(1/d_k)^{-1}$ as the relative weight yields
Eq. \eqref{eq:gE_WHAM}.
%
\footnote{
\add{We can also view Eq. \eqref{eq:gE_WHAM}
as the result of applying Eq. \eqref{eq:gnk}
to the composite ensemble of the $K$ canonical ensembles,
using the following substitutions:
$n_k(E) \rightarrow \sum_{k = 1}^K n_k(E)$,
$N_k \rightarrow \sum_{k = 1}^K N_k \equiv N_\mathrm{tot}$,
and
$w_k(E) \rightarrow \sum_{k = 1}^K (N_k/N_\mathrm{tot}) \, w_k(E)$.
%
The last expression is the normalized ensemble weight
in the composite ensemble.}}
%
\note{
This new footnote contains a new and cleaner,
albeit less known (if correct at all),
derivation of WHAM.
%
Is it appropriate to placed as a footnote?
%
Or should we type it in the main text,
or even replace the tradition derivation?
%
Or should we delete it?
}
%
Since, for a fixed $E$,
$d_k(E)$ is proportional to $n_k(E)$,
several variants of WHAM
may be derived
by using $n_k(E)$ in place of $d_k(E)$
as the relative weight for
$g(E)$\cite{
shen1991}
or related quantities\cite{
woolf1994, *crouzy1994, roux1995,
kastner2005, *kastner2009, fenwick2008, kim2011}.
%




From Eqs. \eqref{eq:gE_WHAM} and \eqref{eq:Z},
we find that $f_i$ satisfies
%
\begin{align}
f_i
&=
-\log
  \int
    \frac{
      \sum_{k = 1}^K n_k(E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\label{eq:f_WHAM}
\\
&\equiv
-\log \Z_i(\vct f),
\notag
\end{align}
%
where,
$\Z_i(\vct f)$
denotes the integral on the right-hand side,
as a function of $\vct f = (f_1, \dots, f_K)$.
%
Once all $f_i$ and $g(E)$ are determined,
the free energy at a temperature not simulated, $\beta$,
can be found from Eq. \eqref{eq:Z}
by substituting $\beta$ for $\beta_k$.



\repl{
In the above,
$n_k(E)$ is estimated from the energy histogram.
%
To avoid this dependency,
we notice from definition that\cite{
souaille2001}
}
{
\paragraph*{Histogram-free form.}



The histogram dependency of WHAM
[in using $n_k(E)$]
can be avoided
by noticing from definition that\cite{
souaille2001}
}
%
\begin{equation}
n_k(E)
=
\sum_{\vx}^{(k)} \delta(\E(\vx) - E),
\label{eq:n_delta}
\end{equation}
%
where,
$\E(\vx)$
is the energy function,
and
$\sum_{\vx}^{(k)}$
denotes a sum over trajectory frames
of simulation $k$.
%
%
%
Using Eq. \eqref{eq:n_delta}
in Eq. \eqref{eq:f_WHAM} yields
the MBAR result\cite{
kumar1992, souaille2001, shirts2008}:
%
\begin{equation}
f_i
=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}.
\label{eq:f_MBAR}
\end{equation}
%
where
$q_i(\vx) \equiv \exp[ -\beta_i \, \E(\vx) ]$.
%
The $K = 2$ case is the BAR result\cite{
bennett1976},
and Eq. \eqref{eq:f_MBAR}
also holds for a general \add{setting, which permits, e.g., a nonlinear} parameter dependence
(see Appendix \ref{sec:deriveMBAR}
for a derivation).
%
Since both Eqs. \eqref{eq:f_WHAM} and \eqref{eq:f_MBAR}
are invariant under $f_i \rightarrow f_i + c$
for all $i$ and an arbitrary $c$,
$f_i$ are determined only up to a constant shift.



\paragraph*{\add{
Extensions to umbrella sampling.}}



We briefly mention a few extensions.
%
First,
for a general Hamiltonian
with a linear bias
\[
\Ham(\vx; \lambda_i) = \Ham_0(\vx) + \lambda_i \, \W(\vx),
\]
such that $q_i(\vx) = \exp\left[ -\Ham(\vx; \lambda_i) \right]$,
%
we can show,
by inserting
$1 = \int \delta(\W(\vx) - W) \, dW$
into Eq. \eqref{eq:f_MBAR},
that
%
\begin{align}
f_i
&=
-\log
  \int
    \frac{
      \sum_{k = 1}^K n_k(W) \, \exp(-\lambda_i W)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\lambda_k W + f_k)
    }
    d W,
\notag
%\label{eq:fx_WHAM}
\end{align}
%
where
$n_k(W)$
is understood to be
the unnormalized distribution of
the bias $\W(\vx)$.
%
Equation \eqref{eq:f_WHAM}
is the special case of
$\Ham_0(\vx) = 0$,
$\W(\vx) = \E(\vx)$,
and $\lambda_i = \beta_i$.
%
Another common example
is a system under a quadratic restraint (umbrella)
$\E_i = \E_0 + \frac 1 2 A \, (\xi - \lambda_i)^2$
for some reaction coordinate $\xi \equiv \xi(\vx)$.
%
In this case,
$\Ham_0(\vx) = \beta (\E_0 + \frac 1 2 A \, \xi^2)$,
and
$\W(\vx) = -\beta A \, \xi$.
%
The configuration independent term,
$\frac 1 2 \beta \, A \, \lambda_i^2$,
can be added back to $f_i$ after the analysis.



\paragraph*{\add{
Extensions to other ensembles.}}



Further,
$\lambda_i$ and $W$
can be generalized
to vectors as
$\bm{\lambda}_i$
and
$\vct W$, respectively.
%
For example,
for simulations
on multiple isothermal-isobaric ($NpT$) ensembles
with different temperatures and pressures,
%
we set
$\bm{\lambda}_i = (\beta_i, \beta_i p_i)$
and
$\vct W = (E, V)$
with
$p_i$ and $V$
being the pressure and volume,
respectively.
%
However, if the vector dimension is high,
or if the Hamiltonian $\Ham(\vx; \lambda_i)$
depends nonlinearly on $\lambda_i$,
the histogram-free form\add{, Eq. \eqref{eq:f_MBAR},} is more convenient\cite{
shirts2008}.
%which is based directly on
%Eq. \eqref{eq:f_MBAR},
%instead of the histogram-based Eq. \eqref{eq:fx_WHAM}.
%
Besides,
the factor $\exp(-\beta_i \, E)$
can be replaced by
a non-Boltzmann
(e.g., the multicanonical\cite{
mezei1987, *berg1992, *lee1993},
Tsallis\cite{tsallis1988},
microcanonical\cite{
yan2003, *martin-mayor2007, *zhang2013})
weight
for multiple non-canonical simulations\cite{
kim2011}.
%



\paragraph*{\add{
Solution by iteration.}}



Numerically, the
$f_i$ are most often determined by treating Eq. \eqref{eq:f_WHAM}
as an iterative equation,
%
\begin{equation*}
f_i^\mathrm{(new)}
=
-\log \Z_i( \vct f^\mathrm{(old)} ).
\end{equation*}
%
However, this approach, referred to as direct WHAM below,
can take thousands of iterations to finish\cite{
bereau2009, kim2011, zhu2012}
(cf. Appendix \ref{sec:convwham}).
%
In Secs. \ref{sec:DIIS} and \ref{sec:basis_updating},
we give a numerical technique to
accelerate the solution
of Eq. \eqref{eq:f_WHAM} or \eqref{eq:f_MBAR}.





\subsection{ST-WHAM and UIM}



For comparison,
we briefly discuss
two non-iterative alternatives, ST-WHAM\cite{
fenwick2008, kim2011}
and UIM\cite{
kastner2005, *kastner2009}.
%
By taking the logarithmic derivative of
the denominator of Eq. \eqref{eq:gE_WHAM},
\del{
and using Eq. \eqref{eq:gnk}
}
we \repl{have}{get},
%
\begin{align}
\frac{d}{dE}
\log
  \sum_{k = 1}^K d_k(E)
&=
-
\frac{
  \sum_{k = 1}^K d_k(E) \, \beta_k
}
{
  \sum_{k = 1}^K d_k(E)
}
%\notag\\
%&
=
-
\frac{
  \sum_{k = 1}^K n_k(E) \, \beta_k
}
{
  \sum_{k = 1}^K n_k(E)
},
\notag
%\label{eq:beta_STWHAM}
\end{align}
%
\repl{
which is independent of $d_k(E)$, hence $f_k$.
}
{
where
we have used Eq. \eqref{eq:gnk}
to make the final expression
independent of $d_k(E)$, hence $f_k$.
}
%
So
\begin{align}
g(E)
=
\left[
  \sum_{k = 1}^K n_k(E)
\right]
\,
\exp
\left[
\int^E
    \frac{ \sum_{k = 1}^K n_k(E') \, \beta_k }
         { \sum_{k = 1}^K n_k(E') }
  dE'
\right].
\notag
%\label{eq:g_STWHAM}
\end{align}
%
This is the ST-WHAM result.
%
In evaluating the integral,
we may encounter an empty bin
with $\sum_{k = 1}^K n_k(E') = 0$,
which leaves the integrand indeterminate.
%
An expedient fix is to let the integrand borrow
the value from the nearest nonempty bin
%
[note, however, setting the integrand to zero
would cause a larger error in $g(E)$].
%
ST-WHAM is most convenient in one dimension,
and its results usually differ only slightly
from those of WHAM\cite{kim2011}.
%
In UIM\cite{
kastner2005, *kastner2009},
the distribution $n_k(E)$
is further approximated as a Gaussian.
%
We note that ST-WHAM is difficult when we have more
thermodynamic variables in the ensemble of interest.
%
Below we show \add{that WHAM with} the DIIS method can
handle such a case
for the Lennard-Jones (LJ) fluid, readily.
%This, however, may limit the long-term accuracy.




\subsection{\label{sec:DIIS}DIIS}



DIIS is a technique useful for solving equations\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011},
and we use it
to solve Eq. \eqref{eq:f_WHAM} here.
%
A schematic illustration
is shown in Fig. \ref{fig:scheme}.
%
We first represent an approximate solution
by a trial vector,
$\vct f = (f_1, \dots, f_K)$,
which is, in our case, the vector of
the dimensionless free energies.
%
The target equations can be written as
%
\begin{equation}
  R_i(\vct f) = 0  \quad i = 1, \ldots, K,
  \label{eq:R_f}
\end{equation}
%
which is
$-\log \Z_i\left( \vct f \right) - f_i$
in our case.
%
The left-hand side of Eq. \eqref{eq:R_f}
also forms a $K$-dimensional vector,
$\vct R = (R_1, \dots, R_K)$,
which is referred to as the residual or correction vector.
%
The magnitude
$\| \vct R \|$
represents the error,
%
and
$\vct R(\vct f)$
should optimally point in a direction
\repl{reducing}{that reduces} the error of $\vct f$.
%
%That is,
%for $\vct f' = \vct f + \alpha \, \vct R(\vct f)$
%with a sufficiently small $\alpha$,
%we expect
%%
%$\| \vct R(\vct f') \| < \| \vct R(\vct f) \|$.



If Eq. \eqref{eq:R_f} is solved
by direct iteration,
$\vct f$ is replaced by $\vct f + \vct R(\vct f)$
in each time.
%
This can be a slow process
because the residual vector $\vct R$
does not always have
the proper direction and/or magnitude
to bring $\vct f$
close to the true solution $\vct f^*$.
%
The magnitude of $\vct R$, however,
can be used as a reliable measure
of the error of $\vct f$.
%
Thus, in DIIS, we try to find a vector $\hat{\vct f}$
with minimal error $\| \vct R(\hat{\vct f}) \|$,
which would be more suitable
for direct iteration.



Suppose now we have a basis of $M$ trial vectors
$\vct f_1$, \dots $\vct f_M$
(where $M$ can be much less than $K$),
%
and the residual vectors are
$\vct R_1$, \dots $\vct R_M$
[where $\vct R_j \equiv \vct R(\vct f_j)$
for $j = 1, \dots, M$],
respectively.
%
We wish to construct a vector $\hat{\vct f}$
with minimal error
from a linear combination of the trial vectors.



To do so, we first find the combination of the residual vectors
$\vct{\hat R} = \sum_i c_i \, \vct R_i$,
that minimizes the error
$\| \vct{\hat R} \|$
under the constraint
\begin{equation}
  \sum_i c_i = 1.
  \label{eq:c_normalize}
\end{equation}
%
Mathematically,
this means \add{that} we solve for the $c_i$ simultaneously
from Eq. \eqref{eq:c_normalize} and
%
\begin{equation}
  \sum_j \left( \vct R_i \cdot \vct R_j \right) \, c_j = \lambda,
  \label{eq:cj_DIIS}
\end{equation}
%
for all $i$,
with $\lambda$ being the Lagrange multiplier.
%
Now the corresponding combination
of the trial vectors,
$\vct{\hat f} = \sum_i c_i \, \vct f_i$,
should be close to the desired minimal-error vector.
%
This is because
around the true solution $\vct f^*$,
Eq. \eqref{eq:R_f} should be nearly linear;
so the residual vector
%
\[
\vct R(\vct{\hat f})
=
\vct R\left(
  \sum_{i=1}^M c_i \, \vct f_i
\right)
\approx
\sum_{i=1}^M c_i \, \vct R(\vct f_i)
=\vct{\hat R},
\]
%
has the minimal magnitude.
%
In other words,
$\vct{\hat f}$,
among all linear combinations of $\{ \vct f_i \}$,
is the closest to the true solution,
%$\vct f^*$,
under the linear approximation.
%
Thus,
an iteration based on
$\vct{\hat f}$
should be efficient.



With $M = K + 1$ independent bases,
one can show that
it is possible to
find a combination with zero $\vct{\hat R}$,
which means that
$\vct{\hat f}$ would be the true solution
if the equations were linear.
%
A particularly instructive case is that of
two vectors ($M = 2$)
in one dimension ($K = 1$)\footnote{
This is the case for solving the BAR equation,
because $f_i$ are determined up to a constant shift,
the virtual dimension is 1.}.
%
We then recover
the secant method\cite{press},
as shown in Fig. \ref{fig:scheme}(c).
%
The number of bases, $M$, however,
should not exceed $K + 1$
(or, in our case, $K$
because of the arbitrary shift constant)
to keep Eqs. \eqref{eq:cj_DIIS}
independent,
%
although this restriction may be relaxed
by using certain numerical techniques\cite{
press}.




We now construct a new trial vector $\vct f^{(n)}$ as
%
\begin{equation*}
\vct f^{(n)}
=
\vct{\hat f}
+
\alpha \, \vct{\hat R}( \vct{\hat f} ),
\end{equation*}
%
where the factor $\alpha$ is $1.0$ in this study
(although a smaller value is recommended
for other applications\cite{kovalenko1999, howard2011}).
%
The new vector $\vct f^{(n)}$
is used to update the basis as shown next.





\tikzstyle{emptydot}=[inner sep=0pt,minimum size=0.0mm]
\tikzstyle{fRarrow1}=[->, very thick, draw={rgb:red,4;white,1;gray,1}]
\tikzstyle{fRarrow2}=[->, very thick, draw={rgb:blue,4;white,1;gray,1}]
\tikzstyle{fRarrowx}=[->, very thick]
\tikzstyle{fRarr}=[->, thin]
\tikzstyle{fRlabel}=[inner sep=0pt, text=black!80!white]

\begin{figure}[h]
  \begin{tikzpicture}
    %
    %
    % The potential function to minimize is
    %
    % F = 3/8 (x - 3/5)^2 + 5/8 (y - 4/5)^2 + 1/4 (x - 3/5) (y - 4/5)
    %   = 7/2 R^2
    %
    % x = 3/5
    %   + (sqrt(2) - 1) sqrt(3 + sqrt(2)) cos t
    %   + (sqrt(2) + 1) sqrt(3 - sqrt(2)) sin t.
    %
    % y = 4/5
    %   + sqrt(3 + sqrt(2)) cos t
    %   - sqrt(3 - sqrt(2)) sin t.
    %
    % The long axis is achieved at t = Pi/2
    % its length is sqrt(8 + 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) + 1, -1),
    % which has an angle of -22.5 degrees
    % with the x axis.
    %
    % The short axis is achieved at t = 0,
    % its length is sqrt(8 - 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) - 1, 1),
    % which has an angle of 67.5 degrees
    % with the x axis.
    %
    %
    \newcommand{\sz}{4cm}
    \node (label-A) at (-0.10*\sz, 1.28*\sz) [label=below:{(a)}] {};
    \node (label-B) at ( 1.20*\sz, 1.28*\sz) [label=below:{(b)}] {};
    \begin{scope}
      %\clip ({-0.08*\sz}, {-0.08*\sz}) rectangle ({1.3*\sz}, {1.15*\sz});
      %
      % Draw ellipses
      %
      \foreach \i in {0,...,3}
      {
        \pgfmathsetmacro{\Fval}{0.13 - \i * 0.04};
        \pgfmathsetmacro{\Rval}{sqrt(\Fval*2/7)};
        \pgfmathsetmacro{\aval}{sqrt(8 + sqrt(8))*\Rval};
        \pgfmathsetmacro{\bval}{sqrt(8 - sqrt(8))*\Rval};
        \pgfmathsetmacro{\colora}{12 + 8 * \i};
        \draw[fill,
              gray!\colora!white,
              rotate around={67.5:(0.6*\sz, 0.8*\sz)}]
          (0.6*\sz, 0.8*\sz)
          ellipse ({\bval*\sz} and {\aval*\sz});
      }
      %
      %
      %
      \foreach \i in {1,...,19}
      {
        \pgfmathsetmacro\lambda{\i * 0.05};
        \pgfmathsetmacro\x{1 - \lambda};
        \pgfmathsetmacro\y{\lambda};
        \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda};
        \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda};
        \pgfmathsetmacro\xx{\x + \dx};
        \pgfmathsetmacro\yy{\y + \dy};
        \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
          (0, 0) -- (\x*\sz, \y*\sz) -- (\xx*\sz, \yy*\sz);
      }
      %
      %
      %
      % the minimum
      %
      %
      \draw
          (0.6*\sz, 0.8*\sz)
          node
            [ fill, circle, inner sep=0pt, minimum size=1.5mm,
              label={
                [inner sep=0.01*\sz]
                -45:{$\vct f^*$}
              }
            ] {};
      %
      % vector 1
      %
      \node (f1R1) at (0.9*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.05*\sz},
                  rotate=-83.7]
               20:{$\vct R_1$}}
        ] {};
      \node (f1) at (1.0*\sz, 0)
        [emptydot, label=below:{$\vct f_1$}]{}
        edge[fRarrow1] (f1R1);
      %
      % vector 2
      %
      \node (f2R2) at (0.4*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.03*\sz},
                  rotate=-14.0]
               93:{$\vct R_2$}}
        ] {};
      \node (f2) at (0, 1.0*\sz)
        [emptydot, label=left:{$\vct f_2$}]{}
        edge[fRarrow2] (f2R2);
      %
      %
      %
      \draw [gray, dashed, thin]
        (-0.1*\sz, 1.1*\sz) -- (1.1*\sz, -0.1*\sz);
      %
      %
      %
      % optimal vector
      %
      %
      \node (fhatRhat) at (0.52*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.02*\sz},
                  rotate=26.6]
                -150:{$\vct {\hat R}$}}
        ] {};
      \node (fhat) at (0.24*\sz, 0.76*\sz)
        [ emptydot, inner sep=0,
          label={
            [label distance={0.03*\sz}]
            180:{$\vct {\hat f}$}
          }
        ] {}
        edge[fRarrowx] (fhatRhat);
      %
      %
      %
      \node (origin) at (0, 0) [emptydot]{}
        edge[fRarrow1] (f1)
        edge[fRarrow2] (f2)
        edge[fRarrowx] (fhat);
    \end{scope}
    %
    %
    %
    %
    % origin of the residual vectors
    %
    %
    %
    \newcommand{\Rox}{1.45*\sz}
    \newcommand{\Roy}{0.13*\sz}
    %
    %
    %
    \foreach \i in {1,...,19}
    {
      \pgfmathsetmacro\lambda{\i * 0.05}
      \pgfmathsetmacro\x{1 - \lambda};
      \pgfmathsetmacro\y{\lambda};
      \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda}
      \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda}
      \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
        (\Rox, \Roy) -- (\Rox + \dx*\sz, \Roy + \dy*\sz);
    }
    %
    %
    %
    \node (R1) at ({\Rox - 0.1 * \sz}, {\Roy + 0.9 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_1$}}
      ] {};
    %
    %
    %
    \node (R2) at ({\Rox + 0.4 * \sz}, {\Roy - 0.1 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_2$}}
      ] {};
    %
    %
    %
    \draw [gray, dashed, thin]
         ({\Rox - 0.15 * \sz}, {\Roy + 1.0 * \sz})
      -- ({\Rox + 0.45 * \sz}, {\Roy - 0.2 * \sz});
    %
    %
    %
    % Draw the perpendicular sign
    %
    %
    %
    \draw [thick]
          ({\Rox + 0.24*\sz}, {\Roy + 0.12*\sz})
       -- ({\Rox + 0.22*\sz}, {\Roy + 0.16*\sz})
       -- ({\Rox + 0.26*\sz}, {\Roy + 0.18*\sz});
    %
    %
    %
    \node (Rhat) at ({\Rox+0.28*\sz}, {\Roy+0.14*\sz})
      [ emptydot,
        label={[fRlabel, label distance=0.1mm]
               10:{$\vct {\hat R}$}}
      ] {};
    %
    %
    %
    \node (Rorigin) at (\Rox, \Roy) [emptydot] {}
      edge[fRarrow1] (R1)
      edge[fRarrow2] (R2)
      edge[fRarrowx] (Rhat);
    %
    %
    %
    %
    % Secant plot
    %
    %
    %
    %
    %
    \newcommand{\Ssz}{0.6*\sz}
    \newcommand{\Ssx}{\Ssz} % x scaling factor
    \newcommand{\Ssy}{0.6*\Ssz} % y scaling factor
    \newcommand{\Sox}{0.9*\sz}
    \newcommand{\Soy}{-1.4*\Ssy}
    %
    %
    %
    %
    \node (label-C) at ({\Sox - 1.60*\Ssx}, {\Soy + 1.00*\Ssy}) [label=below:{(c)}] {};
    %
    \node (Sx) at ({\Sox + 1.4*\Ssx}, {\Soy})
      [ emptydot,
        label={below:{$\vct f$}}] {};
    \node (Sy) at({\Sox}, {\Soy + 0.7*\Ssy})
      [ emptydot,
        label={180:{$\vct R$}}] {};
    %
    % x axis
    %
    \draw [->] (\Sox - 1.50*\Ssx, \Soy) -- (\Sox + 1.50*\Ssx, \Soy);
    %
    % y axis
    %
    \draw [->] (\Sox, \Soy - 1.4*\Ssy) -- (\Sox, \Soy + 0.8*\Ssy);
    %
    %
    % y = 1 - 2^x
    %
    %
    \draw [very thick, gray] plot [smooth, tension=0.8]
      coordinates {
        ({\Sox - 1.20*\Ssx}, {\Soy + 0.564*\Ssy})
        ({\Sox - 1.00*\Ssx}, {\Soy + 0.500*\Ssy})
        ({\Sox - 0.50*\Ssx}, {\Soy + 0.292*\Ssy})
        ({\Sox}, {\Soy})
        ({\Sox + 0.50*\Ssx}, {\Soy - 0.414*\Ssy})
        ({\Sox + 1.00*\Ssx}, {\Soy - 1.000*\Ssy})
        ({\Sox + 1.20*\Ssx}, {\Soy - 1.298*\Ssy})
      };
    %
    \node (Sf1R1) at ({\Sox + 1.0*\Ssx}, {\Soy - 1.000*\Ssy})
      [emptydot,
        label={[label distance=0.1mm]
               0:{$\vct R_1$}}
      ] {};
    \node (Sf1) at ({\Sox + 1.0*\Ssx}, {\Soy})
      [emptydot,
        label={[label distance=0.1mm]
               90:{$\vct f_1$}}
      ] {}
      edge[fRarrow1] (Sf1R1);
    %
    \node (Sf2R2) at ({\Sox - 1.0*\Ssx}, {\Soy + 0.500*\Ssy})
      [emptydot,
        label={[label distance=0.1mm]
               90:{$\vct R_2$}}
      ] {};
    \node (Sf2) at ({\Sox - 1.0*\Ssx}, {\Soy})
      [emptydot,
        label={[label distance=0.1mm]
               270:{$\vct f_2$}}
      ] {}
      edge[fRarrow2] (Sf2R2);
    %
    \node (Sfhat) at ({\Sox - \Ssx/3}, {\Soy})
      [emptydot,
        label={[label distance=0.1mm]
               270:{$\vct {\hat f}$}}
      ] {};
    %
    \draw [gray, dashed, thin]
      ({\Sox + 4*\Ssx/3}, {\Soy - 1.25*\Ssy}) -- ({\Sox - 4*\Ssx/3}, {\Soy + 0.75*\Ssy});
    %
    \node (Sorigin) at (\Sox, \Soy) [emptydot] {}
      edge[fRarrow1] (Sf1)
      edge[fRarrow2] (Sf2)
      edge[fRarrowx] (Sfhat);
  \end{tikzpicture}
  %
  %
  %
  \caption{\label{fig:scheme}
    Schematic illustrations of the method of
    direct inversion of the iterative subspace (DIIS)
    for solving a set of equations\add{,
    $\vct R(\vct f) = \vct 0$}.
    %
    Each trial vector $\vct f_i$
    represents an approximate solution,
    and the residual vector\repl{ $\vct R_i$}{,
    $\vct R_i \equiv \vct R(\vct f_i)$,}
    represents the correction.
    %
    Given a basis of a few (two here) trial vectors,
    DIIS seeks the combination
    $\vct {\hat R} = \sum_i c_i \, \vct R_i$
    that minimizes the magnitude
    $\| \vct{\hat R} \|$
    under the constraint
    $\sum_i c_i = 1$ [panel (b)].
    %
    The corresponding combination
    of the trial vectors,
    $\vct {\hat f} = \sum_i c_i \, \vct f_i$,
    is expected to be close to true solution, $\vct f^*$.
    %
    Then,
    we construct
    the new trial vector as
    $\vct f^{(n)} = \vct {\hat f} + \vct {\hat R}$
    and use it to update the basis
    for the next round of iteration.
    %
    (c) If the vectors are one-dimensional (i.e., scalars),
    it is possible to find a vanishing $\vct{\hat R}$, and
    DIIS is equivalent to the secant method.
  }
\end{figure}




\subsection{\label{sec:basis_updating}Basis updating}



In each iteration of DIIS,
the basis is updated
by the new trial vector $\vct f^{(n)}$
from the above step.
%
Initially,
the basis contains a single vector.
%
As we add more vectors into the basis in subsequent iterations,
some old vectors may be removed
to maintain a convenient and efficient maximal size of $M$.



A simple updating scheme\cite{kovalenko1999}
is to treat the basis as a queue:
%
we add $\vct f^{(n)}$ to the basis,
if the latter contains fewer than $M$ vectors,
%
or substitute $\vct f^{(n)}$ for the earliest vector in the basis.
%
If, however, $\vct f^{(n)}$
produces an error greater than
$K_r$ times the error of
$\vct f_\mathrm{min}$,
the least erroneous vector in the basis,
%
we rebuild the basis
from $\vct f_\mathrm{min}$.
%
Here, the error of a vector $\vct f$ is defined as
$\| \vct R(\vct f) \|$,
and
$K_r = 10.0$ is recommended\cite{
kovalenko1999}.



We used the following modification
in this study.
%
% Howard-Pettitt can be problematic for villin-headpiece NVT
%
%%
%When the basis contains $M$ vectors,
%we find the most erroneous vector,
%$\vct f_\mathrm{max}$, from the basis,
%and replace it by $\vct f^{(n)}$.
%%
%If, however,
%$\vct f_\mathrm{max}$
%is the updated vector in the previous step,
%the process is stagnant.
%%
%In this case,
%we rebuild the basis from $\vct f^{(n)}$.
%
% Below is another variant
%
First, we find the most erroneous vector,
$\vct f_\mathrm{max}$, from the basis.
%
If the new vector, $\vct f^{(n)}$,
produces an error less than $\vct f_\mathrm{max}$,
we add $\vct f^{(n)}$ into the basis
or, if the basis is full,
substitute $\vct f^{(n)}$ for $\vct f_\mathrm{max}$.
%
Otherwise,
we remove $\vct f_\mathrm{max}$ from the basis,
and if this empties the basis,
we rebuild the basis from $\vct f^{(n)}$.



Since the DIIS process is reduced
to the direct iteration
if $M = 1$,
the method is effective
only if multiple basis vectors are used.
%




\section{Results}





We tested DIIS WHAM and MBAR on three systems:
Ising model, LJ fluid,
and the villin headpiece, a small protein in aqueous solution
(see Secs.
\ref{sec:results_Ising}, \ref{sec:results_LJ}, and
\ref{sec:results_villin},
respectively,
for details).
%
We tuned the parameters
such that direct WHAM and MBAR
would take thousands of iterations
to finish.



The main results
are summarized
in Fig. \ref{fig:nsnt},
from which one can see that
DIIS can speed up WHAM and MBAR
dramatically in these cases.
%
The real run time roughly matched
the number of iterations,
suggesting a negligible overhead for using DIIS.
%
This is unsurprising,
for it is often much more expensive to compute
the right-hand side of
Eq. \eqref{eq:f_WHAM} or \eqref{eq:f_MBAR}.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig/nsnt.pdf}
  }
  \caption{
    \label{fig:nsnt}
    Number of iterations
    and run time
    versus the number of bases, $M$,
    in DIIS.
    %
    \note{In the Ising model, the label $NVT$ is removed,
      because the volume, $V$, is ill-defined.
      %
      There are people who insist $V$ as the magnetization
      and say it is actually an $NpT$ ensemble.}%
    %
    The four test cases are
    (1) WHAM on the Ising model,
    (2) WHAM on the LJ fluid in the $NpT$ ensemble,
    (3) WHAM on the mini-protein villin headpiece in the $NVT$ ensemble
    (with the bin width of energy histograms being 1.0), and
    (4) MBAR on the same protein system.
    %
    The $M = 0$ points represent direct WHAM.
    %
    The run times are inversely scaled
    by a factor $\tau$
    for better comparison.
    %
    The scaling factors $\tau$ for cases 1-4 are
    $12.54$, $1866$, $14.91$, and $252.2$ seconds,
    respectively.
    %
    The results were averaged
    over independent samples
    for the Ising model and LJ fluid.
    %
    For the villin headpiece,
    the results were averaged over
    bootstrap\cite{
    newman, efron1979, hub2010}
    samples for WHAM,
    or over random subsamples with about 1\% of the trajectory frames
    for MBAR.
    %
    Here, the error tolerance
    $\max \{ |R_i| \}$ is $10^{-8}$.
    %
    The lines are a guide for the eyes.
  }
\end{figure}





\subsection{\label{sec:results_setup}
\repl{Set-up}{Setup}}



For simplicity,
we assumed equal autocorrelation times
from different temperatures (and pressures).
%
The approximation should not affect
the \repl{general}{convergence} behavior of the methods.



In testing WHAM and MBAR,
the initial free energies were obtained from
the single histogram method:
%
\begin{equation*}
\Delta f_i
=
\log
\left\langle
  \exp\left(
    \Delta \beta_i \, E
  \right)
\right\rangle_{i+1},
\end{equation*}
%
where $\Delta A_i \equiv A_{i+1} - A_i$,
for any quantity $A$,
and
$\langle\dots\rangle_{i + 1}$
denotes an average over frames in trajectory $i + 1$.
%
Then, $f_i = f_1 + \sum_{j = 1}^{i - 1} \Delta f_j$.
%
%
Iterations are continued
until all $|R_i|$ are reduced
below a certain value.



For comparison,
we also computed \repl{the dimensionless free energies}{$f_i$}
from \repl{two}{three} approximate formulae.
%
\repl{First}{The first is}\cite{park2007}
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i,
\label{eq:df_eav}
\end{equation}
where
$\overline{ A }_i \equiv (A_{i+1} + A_i)/2$.
%
\repl{This formula can be improved}{The second is the improvement}
by an Euler-Maclaurin expansion\cite{
arfken, *abramowitz, *wang_specfunc, whittaker}:
\del{as}
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
+
\left(
  \langle \delta E^2 \rangle_{i+1}
-
  \langle \delta E^2 \rangle_i
\right)
\frac{ \Delta \beta_i^2 }{ 12 },
\label{eq:df_eavb}
\end{equation}
%
where
$\langle \delta E^2 \rangle_k
\equiv \langle (E - \langle E \rangle_k)^2 \rangle_k$
for $k = i$ and $i + 1$.
%
\add{%
The third formula is derived from the same expansion
but using $E$ instead of $\beta$ as the independent variable
(which requires integration by parts,
$\int E \, d\beta = E \, \beta - \int \beta \, dE$):
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
-
\left(
  \langle \delta E^2 \rangle_{i+1}^{-1}
-
  \langle \delta E^2 \rangle_i^{-1}
\right)
\frac{ \left( \Delta \langle E \rangle_i \right)^2 }{ 12 }.
\label{eq:df_tg}
\end{equation}
}





\subsection{\label{sec:results_Ising}
Ising model}





The first system is
a $64\times64$ Ising model.
%
We used parallel tempering\cite{
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997, *earl2005}
Monte Carlo (MC)
for
eighty temperatures: $T = 1.5$, $1.52$, \dots, $3.08$,



To study the accuracy,
we generated a large sample with
$10^9$ single-site MC steps for each temperature.
%
Figure \ref{fig:is2ref} shows that
DIIS and direct WHAMs produced identical
dimensionless free energies.
%
The differences between the ST-WHAM and WHAM results
were subtle,
whereas the approximate UIM
produced more deviation in the results,
especially around the critical region.
%
Nonetheless,
the errors were of the same order of magnitude.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2ref.pdf}
  }
  \caption{
    \label{fig:is2ref}
    Errors of
    (a) the dimensionless free energies, $f_i$, and
    (b) the logarithmic density of states, $\log g(E)$,
    for the $N = 64\times64$ two-dimensional Ising model.
    %
    Here,
    $\varepsilon(a) \equiv a - a^\mathrm{ref}$,
    %
    and the reference values for $f_i$ and $g(E)$
    were computed using the methods in
    Refs. \onlinecite{ferdinand1969} and \onlinecite{beale1996},
    respectively.
    %
    The bin size of the energy histograms is $h = 4$.
  }
\end{figure}




To study the convergence rate,
we generated independent samples with
$10^7$ MC steps at each temperature.
%
Figure \ref{fig:is2trace}
shows a faster decay of the error
in DIIS WHAM
than in direct WHAM.






\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2trace.pdf}
  }
  \caption{
    \label{fig:is2trace}
    Convergence error, $\max \{ |R_i| \}$,
    versus the number of iterations
    in direct and DIIS WHAMs
    for the $64\times64$ two-dimensional Ising model.
    %
    Results were geometrically averaged over independent samples.
  }
\end{figure}




\subsection{\label{sec:results_LJ}
LJ fluid}


We tested the DIIS method
on the 256-particle LJ fluid in the isothermal-isobaric ($NpT$) ensemble.
%
This is a \add{case for }the two-dimensional (\repl{p \& T}{$p$ and $T$}) WHAM,
which is difficult for some implementations.
%
The potential interaction between particles was cutoff at half the box size.
%
We simulated the system \del{in $NpT$ ensembles}
using parallel tempering MC.
%
We considered the system
at $N_T \times N_p = 6\times 3$ conditions:
the temperatures were $T = 1.2, 1.3, \dots, 1.7$,
the pressures were $p = 0.1, 0.15, 0.2$.
%
The bin sizes for energy and volume
were $1.0$ and $2.0$, respectively.



As shown in Fig. \ref{fig:nsnt},
DIIS WHAM effectively
reduced the run time significantly.
%
This demonstration also shows that
the efficiency of DIIS does not
always increase with
the number of basis set members, $M$.




\subsection{\label{sec:results_villin}
Villin headpiece}



We tested the methods on a small protein in aqueous solution,
the villin headpiece
(PDB ID: 1VII), which is a well-known test system\cite{duan1998}.
%
The protein was immersed in
a dodecahedron box with 1898 TIP3P water molecules and two chloride ions.
%
MD simulations were performed
using GROMACS\cite{
berendsen1995, *lindahl2001, *vanderspoel2005, *hess2008},
with a time step of 2 fs.
%
Velocity rescaling\cite{bussi2007}
was used as the thermostat with
the time constant being 0.1 ps.
%
The electronic interaction was
handled by the particle mesh Ewald method\cite{
essmann1995}.
%
The constraints were handled by the LINCS method\cite{
hess1997}
for hydrogen-related chemical bonds on the protein
and by the SETTLE method\cite{
miyamoto1992}
for water molecules.
%



We simulated the system at 12 temperatures
$T$ = 300 K, 310 K, \dots, 410 K,
each for approximately 200 ns.
%
The energy distributions were properly overlapped,
as shown in Fig. \ref{fig:whamcmp}(e).
%
The energies of individual trajectory frames were saved every 0.1 ps,
so that there were about 2 million frames for analysis
at each temperature.



As shown in Fig. \ref{fig:nsnt},
direct WHAM suffered from slow convergence,
while the DIIS methodology again
delivered a speedup of two orders of magnitude,
in the number of iterations or in real time.
%
The MBAR case was similar.
%
%It also appeared to be counterproductive
%to use more bases than the vector dimension,
%which is 11.



To \repl{test}{compare} the errors of the methods,
we prepared two types of samples of different sizes.
%
A larger sample was randomly selected from roughly $r = 1\%$
of all trajectory frames from every temperature.
%
In a smaller one, $r$ was reduced to $0.01\%$.
%
The reference values of $f_i$
were computed from all trajectory frames using MBAR.
%
In terms of accuracy
[Figs. \ref{fig:whamcmp}(a) and \ref{fig:whamcmp}(c)],
WHAM, MBAR and BAR were the best,
followed by \repl{Eq.}{Eqs.} \eqref{eq:df_eavb}\repl{,}{ and \eqref{eq:df_tg}},
\add{then by} ST-WHAM and UIM,
then by Eq. \eqref{eq:df_eav}.
%
The accuracy was largely independent of the sample size,
and we suspect that the difference
between ST-WHAM and WHAM
might reflect some intrinsic sampling errors.
%
\note{
We still don't know what caused the difference.
However, we know that
it was not because of the precision (single or double),
integrator (leapfrog or velocity Verlet),
or the constraint algorithm of the proteins.
}
%
In terms of precision
[Figs. \ref{fig:whamcmp}(b) and \ref{fig:whamcmp}(d)],
the differences were small.
%
\del{
With a denser temperature setup, however,
multiple-temperature-based methods
may further gain advantage.
}
%
Generally,
WHAM and ST-WHAM were insensitive to the bin size,
although the precision of ST-WHAM was slightly affected
by a small bin size for smaller samples.



%\begin{figure*}[t]
\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/whamcmp.pdf}
  }
  \caption{
    \label{fig:whamcmp}
    Accuracy [(a) and (c)] and precision [(b) and (d)] of
    the dimensionless free energies, $f_i$,
    from WHAM, BAR, ST-WHAM, UIM, and approximate formulae
    for the villin headpiece in solution.
    %
    Two types of samples were used.
    %
    A large sample [(a) and (b)]
    and
    a small sample [(c) and (d)]
    contain roughly
    $1\%$ and $0.01\%$ of all trajectory frames,
    respectively.
    %
    The results were averaged over random samples.
    %
    The MBAR results computed from all trajectory frames
    were used as the reference.
    %
    $f_i$ at $T = 300$ K is fixed at zero.
    %
    The lines are a guide for the eyes.
    %
    (e) Energy histograms collected
    with bin width $h = 1.0$.
  }
\end{figure}
%\end{figure*}




\section{Conclusions}



In this work, we showed that the DIIS technique
can often significantly accelerate
the WHAM and MBAR methods to produce free energy difference.
%
The technique achieves rapid convergence
by an optimal combination of the approximate
solutions obtained during iteration.
%
DIIS does not require computing
the \repl{Hessian}{Hessian-like} matrix,
\add{$\partial \vct R(\vct f)/\partial \vct f$,}
and is numerically stable
with minimal run time overhead.
%
Compared to other advanced techniques\cite{
shirts2008, zhu2012},
DIIS is relatively simple and easy to implement.
%although better equation solvers may exist.
\add{However, methods based on Hessian matrices
may further accelerate the solution process
for final stages.}
%
Other related free energy methods\cite{
shen1991, woolf1994, *crouzy1994, roux1995}
may also benefit from this technique.
%



There are some non-iterative alternatives to WHAM,
although they may be less general and/or accurate
in some aspect.
%
The use of DIIS makes scanning more than one thermodynamic state variable
\add{or a nonlinear variable}
computationally convenient.
%
This was demonstrated here in the $NpT$ LJ fluid case.
%
Problems with only one thermodynamic variable to scan, such as temperature,
are amenable to the non-iterative ST-WHAM,
or the more approximate UIM.
%
%Besides, rough estimates
%can be obtained from simpler formulae.




\section{Acknowledgments}





It is a pleasure to thank
Dr. Y. Zhao,
\add{J. A. Drake,}
Dr. M. R. Shirts,
Dr. M. W. Deem,
Dr. J. Ma,
Dr. J. Perkyns\repl{,}{ and} Dr. G. Lynch\del{ and J. A. Drake}
for helpful discussions.
%
The authors acknowledge
partial financial support from
the National Science Foundation (CHE-1152876),
the National Institutes of Health (GM037657),
and
the Robert A. Welch Foundation (H-0037).
%
Computer time on the Lonestar supercomputer
at the Texas Advanced Computing Center
at the University of Texas at Austin
is gratefully acknowledged.




\appendix





\section{\label{sec:deriveMBAR}
Probabilistic derivation of Eq. \eqref{eq:f_MBAR}}



Here we show that
Eq. \eqref{eq:f_MBAR}
is a generalization of Eq. \eqref{eq:f_WHAM}
from the energy space to the configuration space.
%
We follow the probabilistic argument\cite{
bartels1997, *gallicchio2005, *habeck2007, *habeck2012, zhu2012}
for simplicity.
%
We assume that the system is subject to
an unknown underlying configuration-space field, $g(\vy)$,
such that the distribution of state $k$ is
\repl{$g(\vy) \, q_k(\vy) / Z_k[g]$}{
$w_k(\vy) \equiv g(\vy) \, q_k(\vy) / Z_k[g]$}, with
%the partition function being
%
\begin{equation}
Z_k[g]
=
\int g(\vy) \, q_k(\vy) \, d\vy.
\label{eq:Zg}
\end{equation}



We now seek the most probable $g(\vy)$
from the observed trajectory.
%
Given a certain $g(\vy)$,
the probability of observing the trajectories,
$\{ \vx \}$,
is given by
%
\begin{equation*}
p\left( \{ \vx \} | g \right)
\propto
\prod_{k = 1}^K
\prod_{\vx}^{(k)}
\frac{ g(\vx) \, q_k(\vx) }
     { Z_k[g] }.
%\label{eq:pg}
\end{equation*}
%
Assuming a uniform prior distribution,
$p[g(\vy)]$, of the value of $g(\vy)$,
at each configuration-space point $\vy$,
Bayes' theorem\cite{leonard} states that
given the observed trajectories, $\{\vx\}$,
the posterior distribution of $g$,
$p\left( g | \{ \vx \} \right)$,
is proportional to
$p\left( \{ \vx \} | g \right) \, p(g) \propto p\left( \{ \vx \} | g \right)$.
%
Thus,
to find the most probable $g(\vy)$,
we only need to maximize
$\log p\left( g | \{ \vx \} \right)$
by taking the functional derivative
with respect to $g(\vy)$
and setting it to zero,
which yields
%
\begin{equation}
g(\vy)
=
\frac{
  \sum_{j = 1}^K \sum_{\vx}^{(j)} \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
},
\label{eq:gy}
\end{equation}
%
where we have used
$\delta \log g(\vx) / \delta g(\vy) = \delta(\vx - \vy)/g(\vy)$,
and
$\delta Z_k[g] / \delta g(\vy) = q_k(\vy)$.
%
Using Eq. \eqref{eq:gy}
in Eq. \eqref{eq:Zg},
and then setting $g$ to $1.0$
yields Eq. \eqref{eq:f_MBAR}.
%
\footnote{\add{
We can also show Eq. \eqref{eq:gy}
by considering the composite ensemble of the $K$ states.
%
The number of visits to a phase-space element at $\vy$
is given by the product of
%
the total sample size,
$N_\mathrm{tot} = \sum_{k = 1}^K N_k$,
the distribution of the composite ensemble
$w_\mathrm{tot} = \sum_{k = 1}^K (N_k/N_\mathrm{tot}) \, w_k(\vy)$,
and the volume $d\vy$.
%
This gives
$g(\vy) \, \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g] \, d\vy$,
and it should match the observed number of visits
to the element,
$\sum_{j = 1}^K n_j(\vy) \, d\vy$,
with $n_j(\vy) \equiv \sum_{\vx}^{(j)} \delta(\vx - \vy)$.}}




\section{\label{sec:convwham}
Models for the convergence of direct WHAM}



\note{
This Appendix has been rewritten and expanded.
}

Here, we use simple models
to study the convergence of direct WHAM.
%
We shall show that
slow convergence is
associated to a wide temperature range,
especially with a large spacing.





\subsection{Linearized WHAM equation}



Consider $K$ distributions, $\rho_i(E)$,
at different temperatures,
$\beta_i$ ($i = 1, \dots, K$),
%
normalized as
%
%\begin{equation}
$
\int \rho_i(E) \, dE = 1.
$
%\notag
%\label{eq:rho_normalize}
%\end{equation}
%
%
%
For simplicity,
we assume equal sample sizes, $N_i$.
%
Then,
Eq. \eqref{eq:f_WHAM},
can be written
in the iterative form as
%
\begin{align}
f_i^\mathrm{(new)}
=
-\log
\int
\frac
{
  \sum_{k=1}^K \rho_k(E) \, \exp(-\beta_i \, E)
}
{
  \sum_{k=1}^K
  \exp(
    -\beta_k \, E + f_k^\mathrm{(old)}
  )
}
dE.
%\notag
\label{eq:f_WHAM1}
\end{align}
%
Around the true solution,
$\vct f^* = (f_1^*, \dots, f_K^*)$,
the equation can be linearized as
%
\begin{align}
\delta f_i^\mathrm{(new)}
=
\sum_{j = 1}^K
A_{ij} \,
\delta f_j^\mathrm{(old)},
\notag
\end{align}
%
where
%
$\delta f_i^\mathrm{(new/old)}
\equiv f_i^\mathrm{(new/old)} - f_i^*$,
%
and
\begin{align}
  A_{ij}
=
\int
\frac
{
  \sum_{k=1}^K \rho_k(E) \,
    %\exp\left[
    e^{
      -(\beta_i + \beta_j) \, E + f_i^* + f_j^*
    }
    %\right]
}
{
  D^2(E; \vct f^*)
}
dE.
%\notag
\label{eq:Aij}
\end{align}
%
with
%\begin{align*}
$
D(E; \vct f^*)
=
\sum_{k = 1}^K
  e^{
  %\exp\left(
    -\beta_k \, E + f_k
  %\right).
  }.
$
%\end{align*}
%
In matrix form, we have
%
%\begin{align*}
$
\delta \vct f^\mathrm{(new)}
=
\vct A \,
\delta \vct f^\mathrm{(old)}.
$
%\end{align*}



The elements of matrix $\vct A$
are positive, i.e.,
%
$A_{ij} > 0$,
and normalized, i.e.,
%
\begin{align}
%A_{ij} &> 0,
%\label{eq:Aij_positive}
%\\
\sum_{i = 1}^K A_{ij} &= 1.
\label{eq:Aij_normalization}
\end{align}
%
The latter can be seen from
Eq. \eqref{eq:f_WHAM1}
with
$f_i^\mathrm{(new/old)} \rightarrow f_i^*$.
%
Besides,
$\vct A$ is symmetric:
%
\begin{align}
  A_{ij} = A_{ji}.
\label{eq:Aij_symmetry}
\end{align}
%



Thus,
$\vct A$
can be regarded as a transition matrix\cite{
newman}.
%
Each left eigenvector
$\vct c = (c_1, \dots, c_K)$
of $\vct A$
is associated to a mode of $\delta \vct f$
and the eigenvalue $\lambda$
gives the rate of error reduction
during iteration.
%
That is,
$\sum_{i = 1}^K c_i \, \delta f_i$
decays as $\lambda^n$
asymptotically
with the number of iterations, $n$.




The largest eigenvalue is $\lambda_0 = 1.0$,
and its eigenvector
$\vct c = (1, \dots, 1)$
corresponds to a uniform shift of all
$\delta f_i$,
which is unaffected by the iteration,
%
i.e.,
$
\sum_{i=1}^K \delta f_i^\mathrm{(new)}
=
\sum_{i=1}^K \delta f_i^\mathrm{(old)},
$
as a consequence of
Eq. \eqref{eq:Aij_normalization}.
%
The next largest eigenvalue,
$\lambda_1$,
determines the rate of convergence
of the slowest mode,
%
and a larger value of
$1 - \lambda_1$
means faster convergence.
%
Below,
we determine $\lambda_1$
in a few solvable cases.



\subsection{Exact distribution approximation (EDA)}



To proceed, we assume that
the observed distributions are exact,
%
and thus the solution,
$(f_1^*, \dots, f_K^*)$,
is also exact.
%
Thus, for any $k$, we have
%
\begin{equation}
\rho_k(E)
=
g(E) \,
e^{-\beta_k \, E + f_k^*},
\label{eq:rho_EDA}
\end{equation}
%
with $g(E)$ being the density of states.
%
%\begin{align}
%\sum_{k = 1}^K
%\rho_k(E)
%=
%g(E) \,
%D(E; f_1^*, \dots, f_K^*).
%\notag
%\end{align}
%%
%
This simplifies Eq. \eqref{eq:f_WHAM1} as
%
\begin{align}
A_{ij}
=
\int
\frac
{
  g(E) \, e^{ -(\beta_i + \beta_j) \, E + f_i^* + f_j^*}
}
{
  D(E; \vct f^*)
}
dE.
\label{eq:Aij_EDA}
\end{align}




\subsection{Gaussian density of states}



Further, we approximate $g(E)$
as a Gaussian.
%
With a proper choice of the shift constant of
$\log g(E)$, we have,
%
\begin{equation}
g(E)
=
\sqrt{
  \frac{ a } { 2 \, \pi }
}
\exp\left[ -\frac 1 2 a (E - E_c)^2 \right].
\label{eq:gE_Gaussian}
\end{equation}
%
It follows
%
\begin{align}
f_i
=
-\log
\int
g(E) \, \exp(-\beta_i \, E) \, dE
=
\beta_i \, E_c - \frac{ \beta_i^2 }{ 2 \, a },
\label{eq:f_Gaussian}
\end{align}
%
and
\begin{align}
\rho_i
=
\sqrt{ \frac a { 2 \, \pi} }
\exp\left[
  -\frac 1 2
  a \left(
    E - E_c + \frac { \beta_i } { a }
  \right)^2
\right].
\notag
%\label{eq:rho_Gaussian}
\end{align}
%
Thus,
the distribution at any temperature
is a Gaussian of the same width
$\sigma_E = 1/\sqrt{a}$.
%
The inverse temperature $\beta_i$
affects the energy distributions
only as a linear shift
to the distribution center.
%
It follows that both the origin of $\beta$
and \repl{$E_c$}{that of $E$, represented by $E_c$,}
can be set to any value
that facilitates calculations,
without affecting the value of $A_{ij}$.



\subsection{Two-temperature case}



For the two-temperature case,
the matrix $\vct A$ has only one free variable
because of Eqs. \eqref{eq:Aij_normalization}
and \eqref{eq:Aij_symmetry},
and it can be written as
%
\begin{align*}
\vct A
=
\left(
\begin{array}{ccc}
  1 - A_{12} & A_{12} \\
  A_{12}     & 1 - A_{12}
\end{array}
\right).
\end{align*}
%
Thus,
the second largest eigenvalue is
$\lambda_1 = 1 - 2 \, A_{12}$.



Under EDA,
we have, from
Eqs. \eqref{eq:rho_EDA} and \eqref{eq:Aij_EDA},
%
\begin{align}
A_{12}
&=
\int
\frac{ \rho_1(E) \, \rho_2(E) }
{ \rho_1(E) + \rho_2(E) }
dE
\notag \\
&\le
\int
\frac{ \rho_1(E) + \rho_2(E) }
{ 4 }
\, dE
= \frac 1 2,
\notag
\end{align}
where
the equality is achieved
only for identical distributions.
%
Geometrically,
$A_{12}$
represents the degree of overlap
of the distributions,
as shown in Fig. \ref{fig:gausconv}(a),
%
and it decreases
with the separation
of the two distributions,
from the maximal value, $1/2$,
achieved at $\beta_1 = \beta_2$.



\begin{figure}[h]
  %\newcommand{\xmax}{4.0}
  %\newcommand{\ymax}{2.0}
  %\begin{tikzpicture}[scale=1.0]
  %%
  %%
  %%
  %\draw[] (0, 0) -- (0, \ymax + 0.4);
  %\node at (0, \ymax + 0.6) {$y$};
  %%
  %% A_12
  %%
  %\draw[fill=gray!50!white, fill opacity=0.9]
  %  plot[smooth,samples=100,domain=-\xmax:\xmax] (\x,{\ymax*exp(-\x*\x/2-1/8)/(2*cosh(\x/2)) }) --
  %  plot[smooth,samples=100,domain=-\xmax:\xmax] (\x,{0});
  %%
  %% distribution 1
  %%
  %\draw[]
  %  plot[smooth,samples=100,domain=-\xmax:\xmax] (\x,{\ymax*exp(-(\x-0.5)*(\x-0.5)/2)});
  %%
  %% distribution 2
  %%
  %\draw[]
  %  plot[smooth,samples=100,domain=-\xmax:\xmax] (\x,{\ymax*exp(-(\x+0.5)*(\x+0.5)/2)});
  %%
  %\node at (0, 0.5) {$A_{12}$};
  %\node at (\xmax + 0.3, 0) {$x$};
  %\node at ( 0.8, \ymax + 0.21) {$\rho_1(E)$};
  %\node at (-0.7, \ymax + 0.18) {$\rho_2(E)$};
  %\end{tikzpicture}
  %
  %
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig/gausconv.pdf}
  }
  %
  \caption{\label{fig:gausconv}
    (a) $A_{12}$,
    which determines the rate of convergence
    in the two-temperature case,
    as a measure of overlap
    of the two energy distributions.
    %
    (b) Comparison of the rates of convergence
    in the two-, three-,
    and continuous-temperature cases,
    Eqs. \eqref{eq:lambda1_twoT},
    \eqref{eq:lambda1_threeT},
    and
    \eqref{eq:lambda1_continuous},
    respectively.
    %under the assumption
    %of exact Gaussian energy distributions.
    %
    Here,
    $\sigma_\beta$
    is the standard deviation of the $\beta$ distribution;
    $\sigma_E$
    is that of the energy distribution
    at any temperature.
    %
    A larger value of $1 - \lambda_1$
    means faster convergence.
  }
\end{figure}



By further assuming Gaussian density of states,
we can\del{ set}, without loss of generality,
\add{ set}
$\beta_1 = -\sigma_\beta$,
$\beta_2 = \sigma_\beta$,
and $E_c = 0$.
%
Then, from
Eqs. \eqref{eq:Aij_EDA},
\eqref{eq:gE_Gaussian}
and \eqref{eq:f_Gaussian},
we have
%
\begin{align}
1 - \lambda_1
&= 2 \, A_{12}
=
\sqrt \frac{ a } { 2 \, \pi }
\int
\frac
{
  \exp\left[
    -a \, E^2/2
    -\sigma_\beta^2 / (2 \, a)
  \right]
}
{ \cosh( \sigma_\beta \, E ) }
\, dE
\notag \\
%
&\approx\frac{ e^{-\sigma_\beta^2 \, \sigma_E^2/2} }
{ \sqrt{ 1 + \sigma_\beta^2 \, \sigma_E^2 } }
\left[
  1 + \frac{ \sigma_\beta^4 \, \sigma_E^4 }
  {4\,(1 + \sigma_\beta^2 \, \sigma_E^2)^2 }
\right],
\label{eq:lambda1_twoT}
%\\
%&\xrightarrow{\sigma_\beta \, \sigma_E \gg 1}
%\frac{5}{8}
%\frac{ e^{-\sigma_\beta^2 \sigma_E^2/2} }
%{ \sigma_\beta \, \sigma_E },
%\notag
\end{align}
%
where
$\sigma_E = 1/\sqrt{a}$,
%
and we have used
$1/\cosh x \approx \exp(-x^2/2)(1 + x^4/12)$.
%
This model shows
a rapid decrease in
the convergence of direct WHAM
with the temperature separation.



\subsection{Three-temperature case}



Similarly,
for three \add{evenly-spaced} temperatures,
\del{with an even spacing,}
we can\del{ set}, without loss of generality,
\add{ set}
$\beta_1 = -\Delta \beta$,
$\beta_2 = 0$,
and
$\beta_3 = \Delta \beta$,
with
$\Delta \beta = \sqrt{\frac 3 2} \sigma_\beta$.
%
Then,\footnote{
%
For $A_{12}$,
we set $E_c = -\Delta \beta/(2 \, a)$,
and
\begin{align*}
D(E; \vct f^*)
&\propto
2 \cosh\left(
  \tfrac{1}{2} \Delta \beta E
\right)
+
\exp\left(
  -\tfrac{3}{2} \Delta \beta E
  -\tfrac{1}{2} \Delta \beta^2
\right)
\\
&\approx
2 \cosh\left(
  \tfrac{1}{2} \Delta \beta E
\right)
+
\exp\left(
  -\tfrac{ u \Delta \beta^2 }{ a }
\right)
\\
&\approx
2 \, \theta
\left.
  \exp\left(
    \tfrac{\Delta \beta^2 E^2} { 8 \, \theta }
  \right)
\middle/
  \left[
    1
    +
    \tfrac{ (3 - \theta) \Delta \beta^4 E^4 }
    { 24 \cdot 2^4 \cdot \theta^2 }
  \right]
\right.,
\end{align*}
where
$\theta = 1 + \frac 1 2 \exp\left(-u \Delta \beta^2 E^2\right/4)$.
%
The parameter $u$ should be $5/2$
to match the small $\Delta \beta$ behavior,
although the overall accuracy can be improved
by a value of $u \approx 2.11$.
}
%
\begin{align}
1 - \lambda_1
&= A_{12} + 2 A_{13}
\notag \\
&\approx
\A(\sqrt{\tfrac 3 8} \, \sigma_\beta \sigma_E, -\tfrac 5 2)
+
2 \, \A(\sqrt{\tfrac 3 2} \, \sigma_\beta \sigma_E, \tfrac 1 2),
\label{eq:lambda1_threeT}
\end{align}
where
\begin{align*}
\A(b, t)
=
\frac{
  e^{-b^2/2} / (1 + \frac{1}{2}e^{tb^2})
}
{
  2 \sqrt{1 + b^2/(1 + \frac{1}{2} e^{tb^2})}
}
\left[
  1
  +
  \frac{(2 - \frac{1}{2} e^{tb^2}) \, b^4/8}
  { (1 + b^2 + \frac{1}{2} e^{tb^2})^2 }
\right].
\end{align*}




\subsection{Continuous-temperature case}



If there are a large number $K$ of temperatures
in a finite range,
\repl{they can be approximated}
{we can approximate them}
by a continuous distribution, $w(\beta)$.
%
In this case,
the sum over temperatures can be
replaced by an integral:
%
%\begin{equation}
$
\sum_{k = 1}^K
\rightarrow
K \int d\beta \, w(\beta).
$
%\notag
%\end{equation}

The eigenvalue $\lambda_l$
and eigenvector $c_l(\beta)$
are now determined from the integral equation
%
\begin{equation}
K \int w(\beta') \, c_l(\beta') \, A(\beta, \beta') \, d\beta'
= \lambda_l \, c_l(\beta),
\label{eq:eig_continuous}
\end{equation}
%
where
$A(\beta, \beta')$
is $A_{ij}$
with \repl{$\beta_i$ and $\beta_j$
replaced by $\beta$ and $\beta'$, respectively.}
{$\beta_i \rightarrow \beta$ and $\beta_j \rightarrow \beta'$.}



Equation \eqref{eq:eig_continuous}
can be solved in a special case, in which
we assume EDA, Eq. \eqref{eq:Aij_EDA},
a Gaussian density of states,
Eq. \eqref{eq:gE_Gaussian},
and
a Gaussian $\beta$ distribution:
%
\begin{align}
w(\beta)
=
\frac{1}{\sqrt{2 \, \pi \, a'}}
\exp\left[
  -\frac{ (\beta - \beta_c)^2 }{ 2 \, a' }
\right],
\notag
%\label{eq:wbeta_Gaussian}
\end{align}
%
with width $\sigma_\beta = \sqrt{a'}$.
%
%
%
The physical solution is
%
\begin{align}
\lambda_l
&=
\left(
  \frac { a' } { a + a' }
\right)^l,
\notag
\\
c_l(\beta)
&=
H_l\left(
  \frac{ \beta - \beta_c }
  { \sqrt{ 2 \, a' } }
\right),
\notag
\end{align}
%
where
$H_l(x)$
is the Hermite polynomial\cite{
arfken, *abramowitz, *wang_specfunc},
generated as
$e^{-t^2 + 2x t} = \sum_{l = 0}^\infty H_l(x) \, t^l/l!$.
%
Thus, for the second largest eigenvector,
we have
%
\begin{equation}
1 - \lambda_1
=
\frac{1}
{ 1 + \sigma_\beta^2 \, \sigma_E^2},
\label{eq:lambda1_continuous}
\end{equation}
%
which decreases
with increasing temperature range,
$\sigma_\beta$,
albeit more slowly than
the two- and three-temperature values,
as shown in Fig. \ref{fig:gausconv}(b).
%
Thus,
Eq. \eqref{eq:lambda1_continuous}
only gives a upper bound,
and nonzero spacing between temperatures
can further slow down the convergence.



\bibliography{simul}
\end{document}
