\documentclass[reprint,aip,jcp,superscriptaddress]{revtex4-1}
%\documentclass[aip,jcp,preprint,superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{tikz}
\usepackage{color}
\begin{document}




\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}




\title{Accelerating the weighted histogram analysis method
by direct inversion in the iterative subspace}

\begin{abstract}
The convergence of the weighted histogram analysis method for free energy calculation
can be improved by using the method of direct inversion in the iterative subspace.
\end{abstract}

\maketitle




\section{Introduction}





A central subject in computational physics and chemistry
is to extract the best estimate of some quantity of interest
from a given set of simulation data.
%
For free energy calculations,
the multiple histogram method\cite{
ferrenberg1988, *ferrenberg1989,
newman, frenkel}
or its generalization,
the weighted histogram analysis method (WHAM)\cite{
kumar1992, roux1995,
bartels1997, *gallicchio2005, *habeck2007, *habeck2012,
souaille2001,
chodera2007, shirts2008, bereau2009,
hub2010, zhu2012},
is such an efficient method.
%
The statistical efficiency
makes WHAM a popular trajectory analysis tool
for enhanced-sampling simulations,
such as umbrella sampling\cite{
torrie1974, *laio2002},
and tempering\cite{
marinari1992, *lyubartsev1992,
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997, *earl2005},
and even single-molecule experiments\cite{
shirts2008}.




Given several trajectories collected
for different thermodynamic states,
WHAM is able to draw the optimal estimate
of the free energies of the states.
%
The key is to seek an optimal estimate
of the density of states,
or the unbiased distribution,
which allows
the free energy to be evaluated
as a weighted integral.
%
The optimal density of states
is computed from a weighted average
of the reweighted energy histograms
(hence the name of the method)
from different trajectories.
%
The weights, however,
depend on the free energies,
so that
the free energies
and the density of states
must determined self-consistently.
%
Interestingly,
WHAM can be reformulated
as an extension
of the Bennett acceptance ratio (BAR) method\cite{
bennett1976}.
%
This form,
adopted by the multistate BAR (MBAR) method\cite{
shirts2008},
avoids the histogram dependency.



The straightforward implementation of WHAM or MBAR,
in which the equations regarding the free energies
are solved by direct iteration,
can suffer from
a slow convergence in late stages.
%
Several remedies were proposed\cite{
shirts2008, bereau2009, kim2011}.
%
For example, one may use the Newton-Raphson method,
which involves the Hessian matrix,
although the approach sometimes can be unstable\cite{
shirts2008}.
%
Other more advanced techniques include
the trust region and BFGS methods\cite{zhu2012}.



An ingenious non-iterative alternative is
the statistical-temperature WHAM (ST-WHAM)\cite{
kim2011}, which
determines the density of states
through its logarithmic derivative,
or the statistical temperature.
%
In this way,
it manages to estimate
the density of states non-iteratively
with minimal approximation.
%by averaging the values
%from different trajectories
%using the histogram heights as the weights.
%
ST-WHAM can be regarded as a refinement of
the more approximate umbrella integration method (UIM)\cite{
kastner2005, *kastner2009}.
%
However, the extension to multidimensional ensembles,
e.g., the isothermal-isobaric ensemble,
can be numerically challenging\cite{kim2011}.



Here, we discuss a numerical improvement of
the implementation of WHAM using
the method of direct inversion in the iterative subspace (DIIS)\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011}.
%
Although still iterative in nature,
this implementation
can often improve
the convergence rate
significantly in difficult cases.





\section{Method}





\subsection{WHAM}



WHAM is a method of
estimating the free energy differences
among a few thermodynamic states
with different parameters,
such as temperatures, pressures, etc.
%
For definiteness,
consider the special case of $K$ temperatures,
labeled by the inverse temperature,
$\beta = 1/(k_B T)$,
as
$\beta_1, \ldots, \beta_K$.
%
Suppose we have performed the respective
canonical ($NVT$) ensemble simulations
at the $K$ temperatures,
we wish to estimate the free energies
at the temperatures.



In WHAM,
we first estimate the density of states, $g(E)$, from
%
\begin{equation}
g(E)
=
\frac{
  \sum_{k = 1}^K n_k(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
},
\label{eq:gE_WHAM}
\end{equation}
%
where,
$n_k(E)$,
the unnormalized energy distribution
observed from trajectory $k$,
is usually estimated
from the energy histogram as
the number of independent trajectory frames
whose energies fall in the interval
$(E - h/2, E + h/2)$
divided by $h$
(we shall omit ``independent'' below for simplicity);
%
$N_k$
is the total number of frames
from simulation $k$;
%
$Z_k$
is the partition function:
%
\begin{equation}
Z_k
=
\exp( - f_k )
=
\int g(E) \, \exp(-\beta_k E) \, dE,
\label{eq:Z}
\end{equation}
and
$f_k$
is the dimensionless free energy.



To understand Eq. \eqref{eq:gE_WHAM},
we first observe from definition,
%
\begin{equation}
g(E)
=
\frac{ n_k(E) }
     { d_k(E) }
\qquad
\mbox{for $k = 1, \dots, K$},
\label{eq:gnk}
\end{equation}
where $d_k(E) \equiv N_k \exp(-\beta_k E + f_k)$.
%
Then, Eq. \eqref{eq:gE_WHAM}
can be regarded as
a linear combination
of the values from
Eq. \eqref{eq:gnk}\cite{
roux1995, souaille2001, frenkel}.
%
Now in an optimal combination,
the relative weight is inversely
proportional to the variance.
%
Assuming a Poisson distribution
so that $\mathrm{var}(n_k) = n_k$,
and $N_k \gg n_k$ so that
the relative error of $d_k$ is negligible
in comparison to that of $n_k$,
we have
$\mathrm{var}(n_k/d_k) \approx n_k / d_k^2 \propto 1/d_k$.
%
Averaging the values from Eq. \eqref{eq:gnk}
using $(1/d_k)^{-1}$ as the relative weight yields
Eq. \eqref{eq:gE_WHAM}.
%
Since, for a fixed $E$,
$d_k(E)$ is proportional to $n_k(E)$,
several variants of WHAM use $n_k(E)$
as the relative weight for
$g(E)$\cite{
shen1991}
or related quantities\cite{
woolf1994, *crouzy1994, roux1995,
kastner2005, *kastner2009, kim2011}.
%




From Eqs. \eqref{eq:gE_WHAM} and \eqref{eq:Z},
we find that each $f_i$ satisfies
%
\begin{align}
f_i
&=
-\log
  \int
    \frac{
      \sum_{k = 1}^K n_k(E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\label{eq:f_WHAM}
\\
&\equiv
-\log \Z_i(\{ f_k \}),
\notag
\end{align}
%
where,
$\Z_i$
denotes the integral on the right-hand side.
%
Once all $f_i$ and $g(E)$ are determined,
the free energy at a not simulated temperature, $\beta$,
can be found from Eq. \eqref{eq:Z}
by substituting $\beta$ for $\beta_k$.



In the above,
$n_k(E)$ is estimated from the energy histogram.
%
To avoid this dependency,
we notice from definition that\cite{
souaille2001}
%
\begin{equation}
n_k(E)
=
\sum_{\vx}^{(k)} \delta(\E(\vx) - E),
\label{eq:n_delta}
\end{equation}
%
where,
$\E(\vx)$
is the energy function,
and
$\sum_{\vx}^{(k)}$
denotes a sum over trajectory frames
of simulation $k$.
%
%
%
Using Eq. \eqref{eq:n_delta}
in Eq. \eqref{eq:f_WHAM} yields
the MBAR result\cite{
kumar1992, souaille2001, shirts2008}:
%
\begin{equation}
f_i
=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}.
\label{eq:f_MBAR}
\end{equation}
%
where,
$q_i(\vx) \equiv \exp[ -\beta_i \, \E(\vx) ]$.
%
The $K = 2$ case is the BAR result\cite{
bennett1976},
and Eq. \eqref{eq:f_MBAR}
is also applicable to a nonlinear parameter dependence
(see Appendix \ref{sec:deriveMBAR}
for a derivation).
%
Since both Eqs. \eqref{eq:f_WHAM} and \eqref{eq:f_MBAR}
are invariant under $f_i \rightarrow f_i + c$
for all $i$ and an arbitrary $c$,
$f_i$ are determined only up to a constant shift.



We briefly mention a few extensions.
%
First,
for a general Hamiltonian
with a linear bias
\[
\Ham_i(\vx) = \Ham_0(\vx) + \lambda_i \, \W(\vx),
\]
such that $q_i(\vx) = \exp\left[ -\Ham_i(\vx) \right]$,
%
we can show,
by inserting
$\int \delta(\W(\vx) - W) \, dW$
into Eq. \eqref{eq:f_MBAR},
that
%
\begin{align}
f_i
&=
-\log
  \int
    \frac{
      \sum_{k = 1}^K n_k(W) \, \exp(-\lambda_i W)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\lambda_k W + f_k)
    }
    d W,
\notag
%\label{eq:fx_WHAM}
\end{align}
%
where
$n_k(W)$
is understood as
the unnormalized distribution of
the bias $\W(\vx)$.
%
Equation \eqref{eq:f_WHAM}
is the special case of
$\Ham_0(\vx) = 0$,
$\W(\vx) = \E(\vx)$,
and $\lambda_i = \beta_i$.
%
Another common example
is a quadratic potential
$\E_i = \frac 1 2 A \, (\xi - \lambda_i)^2$
for some reaction coordinate $\xi \equiv \xi(\vx)$.
%
In this case,
$\Ham_0(\vx) = \beta (\E_0 + \frac 1 2 A \, \xi^2)$,
and
$\W(\vx) = -\beta A \, \xi$.
%
The $\frac 1 2 \beta \, A \, \lambda_i^2$ term
is independent of coordinates
and thus can be added back after the analysis.



Further,
$\lambda_i$ and $W$
can be generalized
to vectors as
$\bm{\lambda}_i$
and
$\vct W$, respectively.
%
For example,
for simulations
on multiple isothermal-isobaric ($NpT$) ensembles
with different temperatures and pressures,
%
we set
$\bm{\lambda}_i = (\beta_i, \beta_i p_i)$
and
$\vct W = (E, V)$
with
$p_i$ and $V$
being the pressure and volume,
respectively.
%
If the Hamiltonian $\Ham_i(\vx)$
depends nonlinearly on $\lambda_i$,
one prefers the histogram-free MBAR form\cite{
shirts2008}.
%which is based directly on
%Eq. \eqref{eq:f_MBAR},
%instead of the histogram-based Eq. \eqref{eq:fx_WHAM}.
%
Besides,
a non-Boltzmann
(e.g., the multicanonical\cite{
mezei1987, *berg1992, *lee1993},
Tsallis\cite{tsallis1988},
microcanonical\cite{
yan2003, *martin-mayor2007, *zhang2013})
weight
can be used in place of the Boltzmann one
for simulations in the corresponding ensembles.
%



To find $f_i$ in WHAM,
one often treats Eq. \eqref{eq:f_WHAM}
as an iterative equation,
%
\begin{equation*}
f_i^\mathrm{(new)}
=
-\log \Z_i\left(
  \{ f_k^\mathrm{(old)} \}
\right).
\end{equation*}
%
This approach, or direct WHAM below,
can suffer from a slow convergence
(cf. Appendix \ref{sec:convwham}),
resulting in thousands of iterations\cite{
bereau2009, kim2011, zhu2012}.



An elegant non-iterative alternative is ST-WHAM\cite{
kim2011}.
%
By taking the logarithmic derivative of
the denominator of Eq. \eqref{eq:gE_WHAM}
and using Eq. \eqref{eq:gnk},
we have,
%
\begin{align}
\frac{d}{dE}
\log
  \sum_{k = 1}^K d_k(E)
%&=
%-
%\frac{
%  \sum_{k = 1}^K d_k(E) \, \beta_k
%}
%{
%  \sum_{k = 1}^K d_k(E)
%}
%%\notag\\
%%&
=
-
\frac{
  \sum_{k = 1}^K n_k(E) \, \beta_k
}
{
  \sum_{k = 1}^K n_k(E)
},
\notag
%\label{eq:beta_STWHAM}
\end{align}
%
which is independent of $d_k(E)$, hence $f_k$.
%
So
\begin{align}
g(E)
=
\left[
  \sum_{k = 1}^K n_k(E)
\right]
\,
\exp
\left[
\int^E
    \frac{ \sum_{k = 1}^K n_k(E') \, \beta_k }
         { \sum_{k = 1}^K n_k(E') }
  dE'
\right].
\notag
%\label{eq:g_STWHAM}
\end{align}
%
In evaluating the integral,
we may encounter an empty bin,
which leaves the integrand indeterminant.
%
In this case, we let integrand inherit
the value from the last nonempty bin
%
[note, however, setting the integrand to zero
would cause a larger error in $g(E)$].
%
ST-WHAM is most convenient in one dimension,
and its result is slightly different from WHAM\cite{kim2011}.
%
A related method, UIM\cite{
kastner2005, *kastner2009},
further approximates
the distribution $n_k(E)$ as a Gaussian.
%
This, however, may limit the long-term accuracy.



Because of the limitations of the alternatives,
below we shall still focus on the original WHAM,
and give a numerical technique
that accelerates the solution
of Eq. \eqref{eq:f_WHAM}.
%
The technique is equally applicable to MBAR.





\subsection{DIIS}



DIIS is a method of solving a set of
(nearly) linear equations\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011}.
%
Here, it is used
to solve Eq. \eqref{eq:f_WHAM}.
%
A schematic illustration
is shown in Fig. \ref{fig:scheme}.
%
We first represent an approximate solution
by a trial vector,
$\vct f = (f_1, \dots, f_K)$,
which is, in our case, the vector of
the dimensionless free energies.
%
The target equations can be written as
%
\begin{equation}
  R_i(\vct f) = 0  \quad i = 1, \ldots, K,
  \label{eq:R_f}
\end{equation}
%
whose left-hand side forms
a residual or correction vector
$\vct R = (R_1, \dots, R_K)$.
%
The magnitude
$\| \vct R \|$
represents the error.
%
The signs of $R_i$ are arranged such that
$\vct R(\vct f)$
normally gives a direction
of reducing the error of $\vct f$.
%
That is,
for $\vct f' = \vct f + \alpha \, \vct R(\vct f)$
with a sufficiently small $\alpha$,
we expect
%
$\| \vct R(\vct f') \| < \| \vct R(\vct f) \|$.



Suppose now we have a basis consisting of $M$ trial vectors
$\vct f_1$, \dots $\vct f_M$
(where $M$ can be much less than $K$),
%
and the residual vectors are
$\vct R_1$, \dots $\vct R_M$
[where $\vct R_j \equiv \vct R(\vct f_j)$
for $j = 1, \dots, M$],
%
we wish to construct a more accurate solution
from the vectors.



To do so, we first find the combination of the residual vectors
$\vct{\hat R} = \sum_i c_i \, \vct R_i$,
that minimizes the error
$\left\| \vct{\hat R} \right\|$
under the constraint
\begin{equation}
  \sum_i c_i = 1.
  \label{eq:c_normalize}
\end{equation}
%
That is,
we find $c_i$ from Eq. \eqref{eq:c_normalize} and
\begin{equation*}
  \sum_j \left( \vct R_i \cdot \vct R_j \right) \, c_j = \lambda,
  %\label{eq:cj_DIIS}
\end{equation*}
for all $i$,
with $\lambda$ being the Lagrange multiplier.
%
If Eq. \eqref{eq:R_f} is nearly linear
around the solutions,
%
$\vct{\hat R}$
is roughly the residual vector of
$\vct{\hat f} = \sum_i c_i \, \vct f_i$.
%
This means,
among all linear combinations of
$\{ \vct f_i \}$,
$\vct{\hat f}$
is roughly the least erroneous
and closest to the true solution,
$\vct f^*$.
%
Thus,
an iteration based on
$\vct{\hat f}$
would be most efficient.



For an instructive special case,
consider the two-vector case ($M = 2$)
in the one-dimensional space ($K = 1$)\footnote{
This is the case for solving the BAR equation,
because $f_i$ are determined up to a constant shift,
the virtual dimension is 1.}.
%
The above procedure
is then reduced to the secant method\cite{press},
as shown in Fig. \ref{fig:scheme}(c).



We now construct a new trial vector $\vct f^{(n)}$ as
%
\begin{equation}
\vct f^{(n)}
=
\vct{\hat f}
+
\alpha \, \vct{\hat R}( \vct{\hat f} ),
\end{equation}
%
where the factor $\alpha$ is $1.0$ in this study
(although a smaller value is recommended
for other applications\cite{kovalenko1999, howard2011}).
%
The new vector $\vct f^{(n)}$
is used to update the basis as follows.





\tikzstyle{emptydot}=[inner sep=0pt,minimum size=0.0mm]
\tikzstyle{fRarrow1}=[->, very thick, draw={rgb:red,4;white,1;gray,1}]
\tikzstyle{fRarrow2}=[->, very thick, draw={rgb:blue,4;white,1;gray,1}]
\tikzstyle{fRarrowx}=[->, very thick]
\tikzstyle{fRarr}=[->, thin]
\tikzstyle{fRlabel}=[inner sep=0pt, text=black!80!white]

\begin{figure}
  \begin{tikzpicture}
    %
    %
    % The potential function to minimize is
    %
    % F = 3/8 (x - 3/5)^2 + 5/8 (y - 4/5)^2 + 1/4 (x - 3/5) (y - 4/5)
    %   = 7/2 R^2
    %
    % x = 3/5
    %   + (sqrt(2) - 1) sqrt(3 + sqrt(2)) cos t
    %   + (sqrt(2) + 1) sqrt(3 - sqrt(2)) sin t.
    %
    % y = 4/5
    %   + sqrt(3 + sqrt(2)) cos t
    %   - sqrt(3 - sqrt(2)) sin t.
    %
    % The long axis is achieved at t = Pi/2
    % its length is sqrt(8 + 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) + 1, -1),
    % which has an angle of -22.5 degrees
    % with the x axis.
    %
    % The short axis is achieved at t = 0,
    % its length is sqrt(8 - 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) - 1, 1),
    % which has an angle of 67.5 degrees
    % with the x axis.
    %
    %
    \newcommand{\sz}{4cm}
    \node (label-A) at (-0.10*\sz, 1.28*\sz) [label=below:{(a)}] {};
    \node (label-B) at ( 1.20*\sz, 1.28*\sz) [label=below:{(b)}] {};
    \begin{scope}
      %\clip ({-0.08*\sz}, {-0.08*\sz}) rectangle ({1.3*\sz}, {1.15*\sz});
      %
      % Draw ellipses
      %
      \foreach \i in {0,...,3}
      {
        \pgfmathsetmacro{\Fval}{0.13 - \i * 0.04};
        \pgfmathsetmacro{\Rval}{sqrt(\Fval*2/7)};
        \pgfmathsetmacro{\aval}{sqrt(8 + sqrt(8))*\Rval};
        \pgfmathsetmacro{\bval}{sqrt(8 - sqrt(8))*\Rval};
        \pgfmathsetmacro{\colora}{12 + 8 * \i};
        \draw[fill,
              gray!\colora!white,
              rotate around={67.5:(0.6*\sz, 0.8*\sz)}]
          (0.6*\sz, 0.8*\sz)
          ellipse ({\bval*\sz} and {\aval*\sz});
      }
      %
      %
      %
      \foreach \i in {1,...,19}
      {
        \pgfmathsetmacro\lambda{\i * 0.05};
        \pgfmathsetmacro\x{1 - \lambda};
        \pgfmathsetmacro\y{\lambda};
        \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda};
        \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda};
        \pgfmathsetmacro\xx{\x + \dx};
        \pgfmathsetmacro\yy{\y + \dy};
        \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
          (0, 0) -- (\x*\sz, \y*\sz) -- (\xx*\sz, \yy*\sz);
      }
      %
      %
      %
      % the minimum
      %
      %
      \draw
          (0.6*\sz, 0.8*\sz)
          node
            [ fill, circle, inner sep=0pt, minimum size=1.5mm,
              label={
                [inner sep=0.01*\sz]
                -45:{$\vct f^*$}
              }
            ] {};
      %
      % vector 1
      %
      \node (f1R1) at (0.9*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.05*\sz},
                  rotate=-83.7]
               20:{$\vct R_1$}}
        ] {};
      \node (f1) at (1.0*\sz, 0)
        [emptydot, label=below:{$\vct f_1$}]{}
        edge[fRarrow1] (f1R1);
      %
      % vector 2
      %
      \node (f2R2) at (0.4*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.03*\sz},
                  rotate=-14.0]
               93:{$\vct R_2$}}
        ] {};
      \node (f2) at (0, 1.0*\sz)
        [emptydot, label=left:{$\vct f_2$}]{}
        edge[fRarrow2] (f2R2);
      %
      %
      %
      \draw [gray, dashed, thin]
        (-0.1*\sz, 1.1*\sz) -- (1.1*\sz, -0.1*\sz);
      %
      %
      %
      % optimal vector
      %
      %
      \node (fhatRhat) at (0.52*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.02*\sz},
                  rotate=26.6]
                -150:{$\vct {\hat R}$}}
        ] {};
      \node (fhat) at (0.24*\sz, 0.76*\sz)
        [ emptydot, inner sep=0,
          label={
            [label distance={0.03*\sz}]
            180:{$\vct {\hat f}$}
          }
        ] {}
        edge[fRarrowx] (fhatRhat);
      %
      %
      %
      \node (origin) at (0, 0) [emptydot]{}
        edge[fRarrow1] (f1)
        edge[fRarrow2] (f2)
        edge[fRarrowx] (fhat);
    \end{scope}
    %
    %
    %
    %
    % origin of the residual vectors
    %
    %
    %
    \newcommand{\Rox}{1.45*\sz}
    \newcommand{\Roy}{0.13*\sz}
    %
    %
    %
    \foreach \i in {1,...,19}
    {
      \pgfmathsetmacro\lambda{\i * 0.05}
      \pgfmathsetmacro\x{1 - \lambda};
      \pgfmathsetmacro\y{\lambda};
      \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda}
      \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda}
      \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
        (\Rox, \Roy) -- (\Rox + \dx*\sz, \Roy + \dy*\sz);
    }
    %
    %
    %
    \node (R1) at ({\Rox - 0.1 * \sz}, {\Roy + 0.9 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_1$}}
      ] {};
    %
    %
    %
    \node (R2) at ({\Rox + 0.4 * \sz}, {\Roy - 0.1 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_2$}}
      ] {};
    %
    %
    %
    \draw [gray, dashed, thin]
         ({\Rox - 0.15 * \sz}, {\Roy + 1.0 * \sz})
      -- ({\Rox + 0.45 * \sz}, {\Roy - 0.2 * \sz});
    %
    %
    %
    % Draw the perpendicular sign
    %
    %
    %
    \draw [thick]
          ({\Rox + 0.24*\sz}, {\Roy + 0.12*\sz})
       -- ({\Rox + 0.22*\sz}, {\Roy + 0.16*\sz})
       -- ({\Rox + 0.26*\sz}, {\Roy + 0.18*\sz});
    %
    %
    %
    \node (Rhat) at ({\Rox+0.28*\sz}, {\Roy+0.14*\sz})
      [ emptydot,
        label={[fRlabel, label distance=0.1mm]
               10:{$\vct {\hat R}$}}
      ] {};
    %
    %
    %
    \node (Rorigin) at (\Rox, \Roy) [emptydot] {}
      edge[fRarrow1] (R1)
      edge[fRarrow2] (R2)
      edge[fRarrowx] (Rhat);
    %
    %
    %
    %
    % Secant plot
    %
    %
    %
    %
    %
    \newcommand{\Ssz}{0.6*\sz}
    \newcommand{\Ssx}{\Ssz} % x scaling factor
    \newcommand{\Ssy}{0.6*\Ssz} % y scaling factor
    \newcommand{\Sox}{0.9*\sz}
    \newcommand{\Soy}{-1.4*\Ssy}
    %
    %
    %
    %
    \node (label-C) at ({\Sox - 1.60*\Ssx}, {\Soy + 1.00*\Ssy}) [label=below:{(c)}] {};
    %
    \node (Sx) at ({\Sox + 1.4*\Ssx}, {\Soy})
      [ emptydot,
        label={below:{$\vct f$}}] {};
    \node (Sy) at({\Sox}, {\Soy + 0.7*\Ssy})
      [ emptydot,
        label={180:{$\vct R$}}] {};
    %
    % x axis
    %
    \draw [->] (\Sox - 1.50*\Ssx, \Soy) -- (\Sox + 1.50*\Ssx, \Soy);
    %
    % y axis
    %
    \draw [->] (\Sox, \Soy - 1.4*\Ssy) -- (\Sox, \Soy + 0.8*\Ssy);
    %
    %
    % y = 1 - 2^x
    %
    %
    \draw [very thick, gray] plot [smooth, tension=0.8]
      coordinates {
        ({\Sox - 1.20*\Ssx}, {\Soy + 0.564*\Ssy})
        ({\Sox - 1.00*\Ssx}, {\Soy + 0.500*\Ssy})
        ({\Sox - 0.50*\Ssx}, {\Soy + 0.292*\Ssy})
        ({\Sox}, {\Soy})
        ({\Sox + 0.50*\Ssx}, {\Soy - 0.414*\Ssy})
        ({\Sox + 1.00*\Ssx}, {\Soy - 1.000*\Ssy})
        ({\Sox + 1.20*\Ssx}, {\Soy - 1.298*\Ssy})
      };
    %
    \node (Sf1R1) at ({\Sox + 1.0*\Ssx}, {\Soy - 1.000*\Ssy})
      [emptydot,
        label={[label distance=0.1mm]
               0:{$\vct R_1$}}
      ] {};
    \node (Sf1) at ({\Sox + 1.0*\Ssx}, {\Soy})
      [emptydot,
        label={[label distance=0.1mm]
               90:{$\vct f_1$}}
      ] {}
      edge[fRarrow1] (Sf1R1);
    %
    \node (Sf2R2) at ({\Sox - 1.0*\Ssx}, {\Soy + 0.500*\Ssy})
      [emptydot,
        label={[label distance=0.1mm]
               90:{$\vct R_2$}}
      ] {};
    \node (Sf2) at ({\Sox - 1.0*\Ssx}, {\Soy})
      [emptydot,
        label={[label distance=0.1mm]
               270:{$\vct f_2$}}
      ] {}
      edge[fRarrow2] (Sf2R2);
    %
    \node (Sfhat) at ({\Sox - \Ssx/3}, {\Soy})
      [emptydot,
        label={[label distance=0.1mm]
               270:{$\vct {\hat f}$}}
      ] {};
    %
    \draw [gray, dashed, thin]
      ({\Sox + 4*\Ssx/3}, {\Soy - 1.25*\Ssy}) -- ({\Sox - 4*\Ssx/3}, {\Soy + 0.75*\Ssy});
    %
    \node (Sorigin) at (\Sox, \Soy) [emptydot] {}
      edge[fRarrow1] (Sf1)
      edge[fRarrow2] (Sf2)
      edge[fRarrowx] (Sfhat);
  \end{tikzpicture}
  %
  %
  %
  \caption{\label{fig:scheme}
    Schematic illustrations of the method of
    direct inversion of the iterative subspace (DIIS)
    for solving a set of equations.
    %
    Each trial vector $\vct f_i$
    represents an approximate solution,
    and the residual vector $\vct R_i$
    represents the correction.
    %
    Given a basis of a few (two here) trial vectors,
    DIIS seeks the combination
    $\vct {\hat R} = \sum_i c_i \, \vct R_i$
    that minimizes the magnitude
    $\left\| \vct{\hat R} \right\|$
    under the constraint
    $\sum_i c_i = 1$ [panel (b)].
    %
    The corresponding combination
    of the trial vectors,
    $\vct {\hat f} = \sum_i c_i \, \vct f_i$,
    is used to construct
    the new trial vector as
    $\vct f^{(n)} = \vct {\hat f} + \vct {\hat R}$.
    %
    Then, $\vct f^{(n)}$ is used to
    update the basis
    for the next round of iteration.
    %
    (c) If the vectors are one-dimensional (i.e., scalars),
    it is possible to find a vanishing $\vct{\hat R}$, and
    DIIS is reduced to the secant method.
  }
\end{figure}




\subsection{Basis updating}



In each iteration of DIIS,
the basis is updated
by the new trial vector $\vct f^{(n)}$
from the above step.
%
Initially,
the basis contains a single vector.
%
As we add more vectors into the basis,
some old vectors are removed
to maintain a maximal size of $M$.



In a popular updating scheme\cite{kovalenko1999},
the basis is treated as a queue:
%
we add $\vct f^{(n)}$ to the basis,
if the latter contains fewer than $M$ vectors,
%
or substitute $\vct f^{(n)}$ for the earliest vector in the basis.
%
If, however, $\vct f^{(n)}$
produces an error greater than
$K_r$ times the error of
$\vct f_\mathrm{min}$,
the least erroneous vector in the basis,
%
we rebuild the basis
from $\vct f_\mathrm{min}$.
%
Here, the error of a vector $\vct f$ is defined as
$\left\| \vct R(\vct f) \right\|$,
and
$K_r = 10.0$ as recommended\cite{
kovalenko1999}.



We used the following modification
in this study.
%
% Howard-Pettitt can be problematic for villin-headpiece NVT
%
%%
%When the basis contains $M$ vectors,
%we find the most erroneous vector,
%$\vct f_\mathrm{max}$, from the basis,
%and replace it by $\vct f^{(n)}$.
%%
%If, however,
%$\vct f_\mathrm{max}$
%is the updated vector in the previous step,
%the process is stagnant.
%%
%In this case,
%we rebuild the basis from $\vct f^{(n)}$.
%
% Below is another variant
%
First, we find the most erroneous vector,
$\vct f_\mathrm{max}$, from the basis.
%
If the new vector, $\vct f^{(n)}$,
produces an error less than $\vct f_\mathrm{max}$,
we add $\vct f^{(n)}$ into the basis
or, if the basis is full,
substitute $\vct f^{(n)}$ for $\vct f_\mathrm{max}$.
%
Otherwise,
we remove $\vct f_\mathrm{max}$ from the basis,
and if this empties the basis,
we rebuild the basis from $\vct f^{(n)}$.



Since the DIIS process is reduced
to the direct iteration
if $M = 1$,
the method is effective
only if multiple bases are used.





\section{Results}





We tested DIIS WHAM and MBAR on three systems:
Ising model,
villin headpiece,
and
Lennard-Jones (LJ) fluid
(see Secs.
\ref{sec:results_Ising},
\ref{sec:results_villin},
and
\ref{sec:results_LJ},
respectively,
for details).
%
We tuned the parameters
such that direct WHAM and MBAR
would take thousands of iterations
to finish.
%
The main results
are summarized
in Fig. \ref{fig:nsnt},
from which one can see that
DIIS can speed up WHAM and MBAR
dramatically in these cases.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/nsnt.pdf}
  }
  \caption{
    \label{fig:nsnt}
    Number of iterations
    and run time
    versus the number of bases, $M$,
    in DIIS.
    %
    The four test cases are
    (1) Ising model in the $NVT$ ensemble with WHAM,
    (2) mini-protein villin headpiece in the $NVT$ ensemble with WHAM
    (with the bin width of energy histograms being 1.0),
    (3) the same protein system with MBAR,
    and
    (4) LJ fluid in the $NpT$ ensemble with WHAM.
    %
    The $M = 0$ points represent direct WHAM.
    %
    The run times are inversely scaled
    by a factor $\tau$
    for better comparison.
    %
    The scaling factors $\tau$ are
    {\color{red} $9.8$, $9.8$, $1250$, and $1525$} seconds,
    respectively.
    %
    The results were averaged
    over independent samples
    for the Ising model and LJ fluid.
    %
    For the villin headpiece,
    bootstrap\cite{
    newman, efron1979, hub2010}
    samples were used for WHAM,
    and random subsamples of about 1\% of the trajectory frames
    were used for MBAR.
    %
    Here, the error tolerance
    $\max \{ |R_i| \}$ is $10^{-8}$.
    %
    The lines are to guide the eyes.
  }
\end{figure}





\subsection{\label{sec:results_setup}
Set-up}



For simplicity,
we assumed equal autocorrelation times
from different temperatures (and pressures).
%
The approximation should not affect
the general behavior of the methods.



In testing direct and DIIS WHAM,
the initial free energies were obtained from
the single histogram method:
%
\begin{equation*}
\Delta f_i
=
\log
\left\langle
  \exp\left(
    \Delta \beta_i \, E
  \right)
\right\rangle_{i+1},
\end{equation*}
%
where $\Delta A_i \equiv A_{i+1} - A_i$,
for any quantity $A$,
and
$\langle\dots\rangle_{i + 1}$
denotes an average over frames in trajectory $i + 1$.
%
Then, $f_i = f_1 + \sum_{j = 1}^{i - 1} \Delta f_j$.
%
%
Iterations are continued
until all $|R_i|$ are reduced
below a certain value.



We also tested two approximate formulae.
%
First\cite{park2007}
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i,
\label{eq:df_eav}
\end{equation}
where
$\overline{ A }_i \equiv (A_{i+1} + A_i)/2$.
%
This formula can be improved
by the Euler-Maclaurin formula\cite{
arfken, *whittaker, *wang_specfunc}
as
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
+
\left(
  \langle \delta E^2 \rangle_{i+1}
-
  \langle \delta E^2 \rangle_i
\right)
\frac{ \Delta \beta_i^2 }{ 12 },
\label{eq:df_eavb}
\end{equation}
%
where
$\langle \delta E^2 \rangle_k
\equiv \langle (E - \langle E \rangle_k)^2 \rangle_k$
for $k = i$ and $i + 1$.



\subsection{\label{sec:results_Ising}
Ising model}





The first system is
the two-dimensional $64\times64$ Ising model.
%
We used parallel tempering\cite{
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997, *earl2005}
Monte Carlo (MC)
for
eighty temperatures: $T = 1.5$, $1.52$, \dots, $3.08$,



To study the accuracy,
we generated a sample with
$10^9$ single-site MC steps on each temperature.
%
Figure \ref{fig:is2ref} shows that
DIIS and direct WHAMs produced identical
dimensionless free energies, which also
agreed well with the exact results\cite{
ferdinand1969}.
%
The differences between the ST-WHAM and WHAM results
were subtle,
whereas the approximate UIM 
produced more different results,
especially around the critical region.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2ref.pdf}
  }
  \caption{
    \label{fig:is2ref}
    Errors of
    (a) the dimensionless free energies, $f_i$, and
    (b) the logarithmic density of states, $\log g(E)$,
    for the $N = 64\times64$ two-dimensional Ising model.
    %
    Here,
    $\varepsilon(a) \equiv a - a^\mathrm{ref}$,
    %
    and the reference values for $f_i$ and $g(E)$
    were computed using the methods in
    Refs. \onlinecite{ferdinand1969} and \onlinecite{beale1996},
    respectively.
    %
    The bin size of the energy histograms is $h = 4$.
  }
\end{figure}




To investigate the convergence rate,
we generated independent samples with
$10^7$ MC steps on each temperature.
%
Figure \ref{fig:is2trace}
shows a faster exponential decay of error
in DIIS WHAM
than in direct WHAM.
%
Optimally, DIIS
delivered a speedup of two orders of magnitude,
as shown in Fig. \ref{fig:nsnt}.
%
The real run time roughly matched
the number of iterations,
suggesting negligible overhead of DIIS.
%
This is unsurprising,
for it is often much more expensive to compute
the right-hand side of Eq. \eqref{eq:f_WHAM}.






\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2trace.pdf}
  }
  \caption{
    \label{fig:is2trace}
    Convergence error, $\max \{ |R_i| \}$,
    versus the number of iterations
    in direct and DIIS WHAMs
    for the $64\times64$ two-dimensional Ising model.
    %
    Results were geometrically averaged over independent samples.
  }
\end{figure}





\subsection{\label{sec:results_villin}
Villin headpiece}



We tested the methods on a mini-protein:
villin headpiece\cite{duan1998}
(PDB ID: 1VII).
%
The protein was immersed in
a dodecahedron box with 1898 TIP3P water molecules and two chloride ions.
%
MD simulations were performed
using GROMACS\cite{
berendsen1995, *lindahl2001, *vanderspoel2005, *hess2008},
with a time step of 2 fs.
%
Velocity rescaling\cite{bussi2007}
was used as the thermostat with
the time constant being 0.1 ps.
%
The electronic interaction was
handled by the particle mesh Ewald method\cite{
essmann1995}.
%
The constraints were handled by the LINCS method\cite{
hess1997}
for hydrogen-related chemical bonds on the protein
and by the SETTLE method\cite{
miyamoto1992}
for water molecules.
%



We simulated the system under 12 temperatures
$T$ = 300 K, 310 K, \dots, 410 K,
each for about 200 ns.
%
The energy distributions were properly overlapped,
as shown in Fig. \ref{fig:whamcmp}(b).
%
The energies of trajectory frames were registered every 0.1 ps,
so that there were about 2 million frames
in each temperature.



As shown in Fig. \ref{fig:nsnt},
direct WHAM suffered from a slow convergence,
while the DIIS version again
delivered a speedup of two orders of magnitude,
in the number of iterations or real run time.
%
The MBAR case was similar.
%
%It also appeared to be counterproductive
%to use more bases than the vector dimension,
%which is 11. 



We compared the errors of various methods,
using the MBAR results as the reference.
%
As shown in Fig. \ref{fig:whamcmp}(a),
both WHAM and ST-WHAM were insensitive to the bin size.
%
ST-WHAM and UIM produced similar results,
while BAR,
which was applied to successive temperature pairs $(i, i+1)$,
produced closer results to those of WHAM and MBAR.
%
For the approximate formulae,
the accuracy of
Eq. \eqref{eq:df_eav}
appeared to be improved by
Eq. \eqref{eq:df_eavb}.




\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/whamcmp.pdf}
  }
  \caption{
    \label{fig:whamcmp}
    (a) Errors of
    the dimensionless free energies, $f_i$,
    from WHAM, BAR, ST-WHAM, UIM, and approximate formulae
    for the villin headpiece.
    %
    $f_i$ at $T = 300$ K is fixed at zero.
    %
    The results of MBAR, which is
    the zero bin-size limit of WHAM,
    were used as the reference.
    %
    The integration step of UIM was $h = 0.1$.
    %
    %All trajectory frames were used in this test.
    %
    %The random error of WHAM (gray line)
    %was estimated using bootstrapping
    %from the $h = 1.0$ case.
    %
    The lines are to guide the eyes.
    %
    (b) Energy histograms collected
    with bin width $h = 1.0$.
  }
\end{figure}




\subsection{\label{sec:results_LJ}
LJ fluid}


We tested the two-dimensional WHAM
on the 256-particle LJ fluid.
%
The potential was cutoff at half box size.
%
We simulated the system in isothermal-isobaric ($NpT$) ensembles,
both using parallel tempering MC.
%
We simulated the system
under $N_T \times N_p = 6\times 3$ conditions:
the temperatures were $T = 1.2, 1.3, \dots, 1.7$,
the pressures were $p = 0.1, 0.15, 0.2$.
%
The bin sizes for energy and volume
are $1.0$ and $2.0$, respectively.


As shown in Fig. \ref{fig:nsnt},
DIIS WHAM also effectively
reduced the run time significantly.
%
In this case,
gradient-based methods\cite{
kim2011, kastner2005, *kastner2009}
are less convenient.





\section{Conclusions}



We showed that the DIIS technique
can often significantly accelerate
the free energy methods WHAM and MBAR.
%
The technique achieves a fast convergence
by an optimal combination of the approximate
solutions obtained during iteration.
%
DIIS does not require computing the Hessian matrix
and is numerically stable
with minimal run time overhead.
%
Compared to more advanced optimization techniques\cite{
shirts2008, zhu2012},
it is easier to implement.



There are several alternatives to WHAM,
although they may be less general and/or accurate
in some aspect.
%
One-dimensional problems
may benefit from the non-iterative ST-WHAM,
or the more approximate cousin UIM.
%
Besides,
rough estimates
can be obtained from simpler formulae.




\section{Acknowledgments}





It is a pleasure to thank
Dr. Y. Zhao, Dr. M. R. Shirts and J. A. Drake
for helpful discussions.
%
The authors gratefully acknowledge
the financial support of
the National Science Foundation (No. CHE-1152876)
and
the Robert A. Welch Foundation (No. H-0037).
%
Computer time on the Lonestar supercomputer
at the Texas Advanced Computing Center
at the University of Texas at Austin
is gratefully acknowledged.




\appendix





\section{\label{sec:deriveMBAR}
Probabilistic derivation of Eq. \eqref{eq:f_MBAR}}



Here we show that
Eq. \eqref{eq:f_MBAR}
is a generalization of Eq. \eqref{eq:f_WHAM}
from the energy space to the configuration space.
%
We follow the probabilistic argument\cite{
bartels1997, *gallicchio2005, *habeck2007, *habeck2012, zhu2012}
for simplicity.
%
We assume that the system is subject to
an unknown underlying configuration-space field, $g(\vy)$,
such that the distribution of state $k$ is
$g(\vy) \, q_k(\vy) / Z_k[g]$, with
%the partition function being
%
\begin{equation}
Z_k[g]
=
\int g(\vy) \, q_k(\vy) \, d\vy.
\label{eq:Zg}
\end{equation}



We now seek the most probable $g(\vy)$
from the observed trajectory.
%
Given a certain $g(\vy)$,
the probability of observing the trajectories,
$\{ \vx \}$,
is given by
%
\begin{equation*}
p\left( \{ \vx \} | g \right)
\propto
\prod_{k = 1}^K
\prod_{\vx}^{(k)}
\frac{ g(\vx) \, q_k(\vx) }
     { Z_k[g] }.
%\label{eq:pg}
\end{equation*}
%
Assuming a uniform prior distribution,
$p[g(\vy)]$, of the value of $g(\vy)$,
at each phase-space point $\vy$,
Bayes' theorem\cite{leonard} states that
given the observed trajectories, $\{\vx\}$,
the posterior distribution of $g$,
$p\left( g | \{ \vx \} \right)$,
is proportional to
$p\left( \{ \vx \} | g \right) \, p(g) \propto p\left( \{ \vx \} | g \right)$.
%
Thus,
to find the most probable $g(\vy)$,
we only need to maximize
$\log p\left( g | \{ \vx \} \right)$
by taking the functional derivative
with respect to $g(\vy)$
and setting it to zero,
which yields
%
\begin{equation}
g(\vy)
=
\frac{
  \sum_{j = 1}^K \sum_{\vx}^{(j)} \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
},
\label{eq:gy}
\end{equation}
%
where we have used
$\delta \log g(\vx) / \delta g(\vy) = \delta(\vx - \vy)/g(\vy)$,
and
$\delta Z_k[g] / \delta g(\vy) = q_k(\vy)$.
%
Using Eq. \eqref{eq:gy}
in Eq. \eqref{eq:Zg},
and then setting $g$ to $1.0$
yields Eq. \eqref{eq:f_MBAR}.




\section{\label{sec:convwham}
Model for the convergence of direct WHAM and MBAR}



Here, we use a simple analytic model
to study the convergence of direct WHAM and MBAR.
%
Consider two temperatures,
denoted by $\beta_1$ and $\beta_2$
with distributions $\rho_1(E)$ and $\rho_2(E)$
normalized as
%
\begin{equation}
\int \rho_i(E) \, dE
= 1
\quad
(i = 1, 2).
\label{eq:rho_normalize}
\end{equation}
%
We assume that the two distributions
are symmetric with respect to some $\bar E$,
which is set to zero
by a shift of the origin of energy.
%
Thus,
\begin{equation}
\rho_1(E)
=
\rho_2(-E).
\label{eq:rho_symm}
\end{equation}



One can readily show that,
in terms of $\Delta f = f_2 - f_1$,
Eq. \eqref{eq:f_WHAM} or \eqref{eq:f_MBAR}
can be written as
%
\begin{align}
\Delta f
&=
-\log \int
\frac{ \rho_1(E) + \rho_2(E) }
{ \exp(\Delta \beta \, E) + \exp \Delta f }
dE
\notag \\
&=
\Delta f
-
\log
  \int
    \frac{ y + \exp \Delta f }
         { y + \cosh \Delta f }
    \rho_1(E) \, dE,
\label{eq:delf_WHAM}
\end{align}
%
where $y \equiv \cosh(\Delta \beta \, E)$,
and we have used
Eqs. \eqref{eq:rho_symm} and
\eqref{eq:rho_normalize}
in the second step.



The solution of Eq. \eqref{eq:delf_WHAM}
is $\Delta f^* = 0$,
which is also apparent from the symmetry.
%
In direct WHAM or MBAR,
Eq. \eqref{eq:delf_WHAM}
is treated as an iterative equation,
with $\Delta f$
on the left- and right-hand sides
being the new and old values,
respectively.
%
For a sufficiently small $\Delta f$,
we can linearize it as
%
\begin{align}
\Delta f^\mathrm{(new)}
&=
\left[
  \int
    \frac{ y } { 1 + y }
    \rho_1(E) \, dE
\right] \,
\Delta f^\mathrm{(old)}
\notag\\
&\equiv
\gamma \,
\Delta f^\mathrm{(old)}.
\label{eq:delf_lin}
\end{align}
%
We expect
$\Delta f$
to decay asymptotically as $\gamma^n$
with the number of iterations, $n$.
%
Since
\[
0 < \gamma < \int
\frac{1 + y}
{1 + y}
\, \rho_1(E) \, dE = 1,
\]
Eq. \eqref{eq:delf_lin}
is convergent,
and a smaller $\gamma$ means
a faster convergence.
%
However,
if the distribution
is dominated by a large $|E|$,
at which $y \gg 1$,
%
then $\gamma$
can be very close 1.
%
This suggests that
a wide temperature, hence energy, range
can lead to a slow convergence
for direct WHAM or MBAR.




\bibliography{simul}
\end{document}
