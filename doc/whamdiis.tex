%
\documentclass[reprint,superscriptaddress]{revtex4-1}
%\documentclass[aip,jcp,preprint,superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{color}
\begin{document}




\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}




\title{Accelerating the weighted histogram analysis method
by direct inversion in the iterative subspace}

\begin{abstract}
It is shown that the convergence of the weighted histogram analysis method for free energy calculation
can be improved by using the method of direct inversion in the iterative subspace.
\end{abstract}

\maketitle




\section{Introduction}





The multiple histogram method\cite{
ferrenberg1988, *ferrenberg1989}
or its generalization,
the weighted histogram analysis method (WHAM)\cite{
kumar1992, souaille2001, kastner2005,
chodera2007, bereau2009,
kim2011},
is a useful tool
of calculating free energy
in computer simulations\cite{
newman, *frenkel}.
%
Given several trajectories collected
for different thermodynamic states,
the method yields optimal estimates
of the free energy differences
among the states.
%
Here, the free energy
and the density of states
are determined
from a set of equations involving
the (energy) histograms,
hence the name of the method.
%
One can also avoid the histogram dependence
and reformulate WHAM
as the multistate Bennett acceptance ratio (MBAR) method\cite{
shirts2008}
as an extension of the Bennett acceptance ratio (BAR) method\cite{
bennett1976}.
%
The statistical efficiency of using data
makes WHAM and MBAR popular trajectory analysis tools
for enhanced-sampling simulations,
such as umbrella sampling\cite{
torrie1974, *laio2002},
and tempering\cite{
marinari1992, *lyubartsev1992,
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997},
and even single-molecule experiments\cite{
shirts2008}.



The straightforward implementation of WHAM or MBAR,
in which the equations regarding the free energy
are solved by direct iteration,
can suffer from
a slow convergence in later stages.
%
Several remedies were proposed\cite{
shirts2008, bereau2009, kim2011}.
%
For example, one may use the Newton-Raphson method,
which involves computing the Hessian matrix,
although the approach sometimes can be unstable\cite{
shirts2008}.
%
An ingenious non-iterative alternative is
the statistical-temperature WHAM (ST-WHAM)\cite{
kim2011},
which can also be considered as a refinement of
the umbrella integration method\cite{
kastner2005}.
%
Here,
one non-iteratively determines the density of states
from integrating its logarithmic derivative,
or the statistical temperature,
which is computed from averaging the values
over the simulated temperatures
using the histogram heights as the weights.
%
This variant, however, gives
a slightly different estimate of free energy
from the original WHAM,
and the extension to multiple-parameter ensembles,
e.g., the isothermal-isobaric ensemble,
can be numerically challenging\cite{kim2011}.



Here, we consider improving WHAM using
the method of direct inversion in the iterative subspace (DIIS)\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011}.
%
Although the method is still iterative,
we shall see that
the rate of convergence can often be
significantly improved.





\section{Method}





\subsection{WHAM}



WHAM is a method of
estimating the free energy differences
among a few thermodynamic states
with different parameters,
such as temperatures, pressures, etc.
%
For definiteness,
consider the special case of $K$ temperatures,
labeled by the inverse temperature,
$\beta = 1/(k_B T)$,
as
$\beta_1, \ldots, \beta_K$.
%
Suppose we have performed the respective
canonical-ensemble simulations
at the $K$ temperatures,
we wish to estimate the free energies
at the temperatures.



In WHAM,
we first estimate the density of states $g(E)$ from
%
\begin{equation}
g(E)
=
\frac{
  \sum_{j = 1}^K n_j(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
},
\label{eq:gE_WHAM}
\end{equation}
%
where,
$n_j(E)$
is the unnormalized energy distribution
observed from simulation $j$.
%
This quantity is usually estimated
from the energy histogram as
the number of independent trajectory frames
whose energies fall in the interval
$(E - \Delta E/2, E + \Delta E/2)$
divided by $\Delta E$
(we shall omit ``independent'' below for simplicity).
%
$N_k$
is the total number of frames
from simulation $k$.
%
$Z_k$
is the partition function,
defined as
%
\begin{equation}
Z_k
=
\int g(E) \, \exp(-\beta_k E) \, dE.
\label{eq:Z}
\end{equation}




From Eqs. \eqref{eq:gE_WHAM} and \eqref{eq:Z},
we find the dimensionless free energy
$f_i \equiv -\log Z_i$
satisfies
\begin{align}
f_i
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K n_j(E) \, \exp(-\beta_j E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\label{eq:f_WHAM}
\\
&\equiv
-\log \Z_i(\{ f_k \}),
\notag
\end{align}
%
where,
$\Z_i$
denotes the integral on the right-hand side.
%
Once, $f_i$ and $g(E)$ are determined,
the free energy at a not simulated temperature, $\beta$,
can be found from Eq. \eqref{eq:Z}
by substituting $\beta$ for $\beta_k$.



In the above,
$n_j(E)$ is estimated from the energy histogram.
%
To avoid this dependency,
we notice from definition that
%
\begin{equation}
n_j(E)
=
\sum_{\vx}^{(j)} \delta(\E(\vx) - E),
\label{eq:n_delta}
\end{equation}
%
where,
$\E$
is the energy function,
and
$\sum_{\vx}^{(j)}$
denotes the sum over trajectory frames
of simulation $j$.
%
%
%
Using Eq. \eqref{eq:n_delta} in Eq. \eqref{eq:f_WHAM} yields
the MBAR\cite{shirts2008} result:
%
\begin{equation}
f_i
=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}.
\label{eq:f_MBAR}
\end{equation}
%
where,
$q_i(\vx) \equiv \exp[ -\beta_i \, \E(\vx) ]$.
%the denominator has been moved into the sum
%as it now depends on the configuration, $\vx$.



We briefly mentions a few extensions.
%
First,
for simulations
on multiple isothermal-isobaric ensembles
at different temperature and pressure,
%
we replace $E$ by $(E, V)$
and the Boltzmann weight
$\exp(-\beta_k \, E)$
by
$\exp(-\beta_k \, E - \beta_k \, p_k \, V)$,
with
$V$ and $p_k$
being the volume and pressure,
respectively.
%
If the Hamiltonian difference among the ensembles
cannot be traced down to a few quantities,
e.g., $E$ or $(E, V)$ in the above,
the MBAR method is preferred\cite{
shirts2008}
because of the inconvenience
of constructing a high-dimensional histogram.
%
Besides,
non-Boltzmann
(e.g., the multicanonical\cite{
mezei1987, *berg1992, *lee1993},
Tsallis\cite{tsallis1988},
microcanonical\cite{
yan2003, *martin-mayor2007, *zhang2013})
weight
can be used in place of the Boltzmann one
for simulations in the corresponding ensembles.
%



To numerically determine $f_i$ in WHAM or MBAR,
Eq. \eqref{eq:f_WHAM} or \eqref{eq:f_MBAR}
is usually treated as an iterative equation
%
\begin{equation*}
f_i^\mathrm{(new)}
=
-\log \Z_i\left(
  \{ f_k^\mathrm{(old)} \}
\right).
\end{equation*}
%
This is referred to as
direct WHAM below.
%
It can suffer from a slow convergence,
resulting in thousands of iterations\cite{
bereau2009, kim2011}.
%
An elegant non-iterative alternative, ST-WHAM,
approximates the logarithmic derivative of $g(E)$
in Eq. \eqref{eq:gE_WHAM} as
%
\begin{equation}
(\log g)'(E)
=
\frac{
  \sum_j n_j'(E) + n_j(E) \, \beta_j
}
{
  \sum_k n_k(E)
},
\label{eq:beta_STWHAM}
\end{equation}
%
whose integral yields $\log g(E)$.
%
ST-WHAM, however,
depends more strongly on the bin size.
%
Below we give a method to accelerate
the original WHAM and MBAR.





\subsection{DIIS}



DIIS is a method of solving a set of
(nearly) linear equations\cite{
pulay1980, *pulay1982, *hamilton1986,
kovalenko1999, howard2011}.
%
Here, it is used
to solve Eq. \eqref{eq:f_WHAM}.
%
A schematic illustration
is shown in Fig. \ref{fig:scheme}.
%
We first represent an approximate solution
by a trial vector,
$\vct f = (f_1, \dots, f_K)$,
which is, in our case, the vector of
the dimensionless free energies.
%
The target equations can be written as
%
\begin{equation}
  R_i(\vct f) = 0  \quad i = 1, \ldots, K,
  \label{eq:R_f}
\end{equation}
%
whose left-hand side forms
a residual or correction vector
$\vct R = (R_1, \dots, R_K)$.
%
The magnitude
$\| \vct R \|$
represents the error.
%
The signs of $R_i$ are arranged such that
$\vct R(\vct f)$
normally gives a direction
of reducing the error of $\vct f$.
%
That is,
for $\vct f' = \vct f + \alpha \, \vct R(\vct f)$
with a sufficiently small $\alpha$,
we expect
%
$\| \vct R(\vct f') \| < \| \vct R(\vct f) \|$.



Suppose now we have a basis consisting of $M$ trial vectors
$\vct f_1$, \dots $\vct f_M$
(where $M$ can be much less than $K$),
%
and the residual vectors are
$\vct R_1$, \dots $\vct R_M$
[where $\vct R_j \equiv \vct R(\vct f_j)$
for $j = 1, \dots, M$],
%
we wish to construct a more accurate solution
from the vectors.



To do so, we first find the combination of the residual vectors
$\vct{\hat R} = \sum_i c_i \, \vct R_i$,
that minimizes the error
$\| \vct{\hat R} \|$
under the constraint
\begin{equation}
  \sum_i c_i = 1.
  \label{eq:c_normalize}
\end{equation}
%
That is,
we find $c_i$ and the Lagrange multiplier $\lambda$
from Eq. \eqref{eq:c_normalize} and
\begin{equation*}
  \sum_j \left( \vct R_i \cdot \vct R_j \right) \, c_j = \lambda,
  %\label{eq:cj_DIIS}
\end{equation*}
for all $i$.
%
If Eq. \eqref{eq:R_f} is nearly linear
around the solutions,
%
$\vct{\hat R}$
is roughly the residual vector of
$\vct{\hat f} = \sum_i c_i \, \vct f_i$.
%
This means,
among all linear combinations of
$\{ \vct f_i \}$,
$\vct{\hat f}$
is roughly the least erroneous
and closest to the true solution,
$\vct f^*$.
%
Thus,
an iteration based on
$\vct{\hat f}$
would be most efficient.



We now construct a new trial vector $\vct f^{(n)}$ as
%
\begin{equation}
\vct f^{(n)}
=
\vct{\hat f}
+
\alpha \, \vct{\hat R}( \vct{\hat f} ),
\end{equation}
%
where the factor $\alpha$ is $1.0$ in this study
(although a smaller value is recommended
for other applications\cite{kovalenko1999, howard2011}).
%
The new vector $\vct f^{(n)}$
is used to update the basis as follows.





\tikzstyle{emptydot}=[inner sep=0pt,minimum size=0.0mm]
\tikzstyle{fRarrow1}=[->, very thick, draw={rgb:red,4;white,1;gray,1}]
\tikzstyle{fRarrow2}=[->, very thick, draw={rgb:blue,4;white,1;gray,1}]
\tikzstyle{fRarrowx}=[->, very thick]
\tikzstyle{fRarr}=[->, thin]
\tikzstyle{fRlabel}=[inner sep=0pt, text=black!80!white]

\begin{figure}
  \begin{tikzpicture}
    %
    %
    % The potential function to minimize is
    %
    % F = 3/8 (x - 3/5)^2 + 5/8 (y - 4/5)^2 + 1/4 (x - 3/5) (y - 4/5)
    %   = 7/2 R^2
    %
    % x = 3/5
    %   + (sqrt(2) - 1) sqrt(3 + sqrt(2)) cos t
    %   + (sqrt(2) + 1) sqrt(3 - sqrt(2)) sin t.
    %
    % y = 4/5
    %   + sqrt(3 + sqrt(2)) cos t
    %   - sqrt(3 - sqrt(2)) sin t.
    %
    % The long axis is achieved at t = Pi/2
    % its length is sqrt(8 + 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) + 1, -1),
    % which has an angle of -22.5 degrees
    % with the x axis.
    %
    % The short axis is achieved at t = 0,
    % its length is sqrt(8 - 2 sqrt(2)) R,
    % and it is along the direction of
    %   (sqrt(2) - 1, 1),
    % which has an angle of 67.5 degrees
    % with the x axis.
    %
    %
    \newcommand{\sz}{4cm}
    \node (label-A) at (-0.10*\sz, 1.28*\sz) [label=below:{(a)}] {};
    \node (label-B) at ( 1.20*\sz, 1.28*\sz) [label=below:{(b)}] {};
    \begin{scope}
      %\clip ({-0.08*\sz}, {-0.08*\sz}) rectangle ({1.3*\sz}, {1.15*\sz});
      %
      % Draw ellipses
      %
      \foreach \i in {0,...,3}
      {
        \pgfmathsetmacro{\Fval}{0.13 - \i * 0.04};
        \pgfmathsetmacro{\Rval}{sqrt(\Fval*2/7)};
        \pgfmathsetmacro{\aval}{sqrt(8 + sqrt(8))*\Rval};
        \pgfmathsetmacro{\bval}{sqrt(8 - sqrt(8))*\Rval};
        \pgfmathsetmacro{\colora}{12 + 8 * \i};
        \draw[fill,
              gray!\colora!white,
              rotate around={67.5:(0.6*\sz, 0.8*\sz)}]
          (0.6*\sz, 0.8*\sz)
          ellipse ({\bval*\sz} and {\aval*\sz});
      }
      %
      %
      %
      \foreach \i in {1,...,19}
      {
        \pgfmathsetmacro\lambda{\i * 0.05};
        \pgfmathsetmacro\x{1 - \lambda};
        \pgfmathsetmacro\y{\lambda};
        \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda};
        \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda};
        \pgfmathsetmacro\xx{\x + \dx};
        \pgfmathsetmacro\yy{\y + \dy};
        \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
          (0, 0) -- (\x*\sz, \y*\sz) -- (\xx*\sz, \yy*\sz);
      }
      %
      %
      %
      % the minimum
      %
      %
      \draw
          (0.6*\sz, 0.8*\sz)
          node
            [ fill, circle, inner sep=0pt, minimum size=1.5mm,
              label={
                [inner sep=0.01*\sz]
                -45:{$\vct f^*$}
              }
            ] {};
      %
      % vector 1
      %
      \node (f1R1) at (0.9*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.05*\sz},
                  rotate=-83.7]
               20:{$\vct R_1$}}
        ] {};
      \node (f1) at (1.0*\sz, 0)
        [emptydot, label=below:{$\vct f_1$}]{}
        edge[fRarrow1] (f1R1);
      %
      % vector 2
      %
      \node (f2R2) at (0.4*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.03*\sz},
                  rotate=-14.0]
               93:{$\vct R_2$}}
        ] {};
      \node (f2) at (0, 1.0*\sz)
        [emptydot, label=left:{$\vct f_2$}]{}
        edge[fRarrow2] (f2R2);
      %
      %
      %
      \draw [gray, dashed, thin]
        (-0.1*\sz, 1.1*\sz) -- (1.1*\sz, -0.1*\sz);
      %
      %
      %
      % optimal vector
      %
      %
      \node (fhatRhat) at (0.52*\sz, 0.9*\sz)
        [ emptydot,
          label={[fRlabel,
                  label distance={0.02*\sz},
                  rotate=26.6]
                -150:{$\vct {\hat R}$}}
        ] {};
      \node (fhat) at (0.24*\sz, 0.76*\sz)
        [ emptydot, inner sep=0,
          label={
            [label distance={0.03*\sz}]
            180:{$\vct {\hat f}$}
          }
        ] {}
        edge[fRarrowx] (fhatRhat);
      %
      %
      %
      \node (origin) at (0, 0) [emptydot]{}
        edge[fRarrow1] (f1)
        edge[fRarrow2] (f2)
        edge[fRarrowx] (fhat);
    \end{scope}
    %
    %
    %
    %
    % origin of the residual vectors
    %
    %
    %
    \newcommand{\Rox}{1.45*\sz}
    \newcommand{\Roy}{0.13*\sz}
    %
    %
    %
    \foreach \i in {1,...,19}
    {
      \pgfmathsetmacro\lambda{\i * 0.05}
      \pgfmathsetmacro\x{1 - \lambda};
      \pgfmathsetmacro\y{\lambda};
      \pgfmathsetmacro\dx{-0.1 + 0.5 * \lambda}
      \pgfmathsetmacro\dy{0.9 - 1.0 * \lambda}
      \draw [fRarr, draw={rgb:red,\x;blue,\y;white,1.5;gray,0.5}]
        (\Rox, \Roy) -- (\Rox + \dx*\sz, \Roy + \dy*\sz);
    }
    %
    %
    %
    \node (R1) at ({\Rox - 0.1 * \sz}, {\Roy + 0.9 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_1$}}
      ] {};
    %
    %
    %
    \node (R2) at ({\Rox + 0.4 * \sz}, {\Roy - 0.1 * \sz})
      [ emptydot,
        label={[fRlabel, label distance=1.5mm]
               0:{$\vct R_2$}}
      ] {};
    %
    %
    %
    \draw [gray, dashed, thin]
         ({\Rox - 0.15 * \sz}, {\Roy + 1.0 * \sz})
      -- ({\Rox + 0.45 * \sz}, {\Roy - 0.2 * \sz});
    %
    %
    %
    % Draw the perpendicular sign
    %
    %
    %
    \draw [thick]
          ({\Rox + 0.24*\sz}, {\Roy + 0.12*\sz})
       -- ({\Rox + 0.22*\sz}, {\Roy + 0.16*\sz})
       -- ({\Rox + 0.26*\sz}, {\Roy + 0.18*\sz});
    %
    %
    %
    \node (Rhat) at ({\Rox+0.28*\sz}, {\Roy+0.14*\sz})
      [ emptydot,
        label={[fRlabel, label distance=0.1mm]
               10:{$\vct {\hat R}$}}
      ] {};
    %
    %
    %
    \node (Rorigin) at (\Rox, \Roy) [emptydot] {}
      edge[fRarrow1] (R1)
      edge[fRarrow2] (R2)
      edge[fRarrowx] (Rhat);
    %
    %
    %
  \end{tikzpicture}
  %
  %
  %
  \caption{\label{fig:scheme}
    Schematic illustration of the method of
    direct inversion of the iterative subspace (DIIS)
    for solving a set of equations.
    %
    Each trial vector $\vct f_i$
    represents an approximate solution,
    and the residual vector $\vct R_i$
    represents the correction.
    %
    Given a basis of a few (two here) trial vectors,
    DIIS seeks the combination
    $\vct {\hat R} = \sum_i c_i \, \vct R_i$
    that minimizes the magnitude
    $\| \vct{\hat R} \|$
    under the constraint
    $\sum_i c_i = 1$ [panel (b)].
    %
    The corresponding combination
    of the trial vectors,
    $\vct {\hat f} = \sum_i c_i \, \vct f_i$,
    is used to construct
    the new trial vector as
    $\vct f^{(n)} = \vct {\hat f} + \vct {\hat R}$.
    %
    Then, $\vct f^{(n)}$ is used to
    update the basis
    for the next round of iteration.
  }
\end{figure}




\subsection{Basis updating}



In each iteration of DIIS,
the basis is updated
by the new trial vector $\vct f^{(n)}$
from the above iteration step.
%
Initially,
the basis contains a single vector.
%
As we add more vectors into the basis,
some old vectors are removed
to maintain a maximal size of $M$.



In a popular updating scheme\cite{kovalenko1999},
the basis is treated as a queue:
%
we add $\vct f^{(n)}$ to the basis,
if the latter contains fewer than $M$ vectors,
%
or substitute $\vct f^{(n)}$ for the earliest vector in the basis.
%
If, however, $\vct f^{(n)}$
produces an error greater than
$K_r$ times the error of
$\vct f_\mathrm{min}$,
the least erroneous vector in the basis,
%
we rebuild the basis
from $\vct f_\mathrm{min}$.
%
Here, the error of a vector $\vct f$ is defined as
$\| \vct R(\vct f) \|$,
and
$K_r = 10.0$ as recommended\cite{
kovalenko1999}.



We used the following modification
in this study.
%
% Howard-Pettitt can be problematic for villin-headpiece NVT
%
%%
%When the basis contains $M$ vectors,
%we find the most erroneous vector,
%$\vct f_\mathrm{max}$, from the basis,
%and replace it by $\vct f^{(n)}$.
%%
%If, however,
%$\vct f_\mathrm{max}$
%is the updated vector in the previous step,
%the process is stagnant.
%%
%In this case,
%we rebuild the basis from $\vct f^{(n)}$.
%
% Below is another variant
%
First, we find the most erroneous vector,
$\vct f_\mathrm{max}$, from the basis.
%
If the new vector, $\vct f^{(n)}$,
produces an error less than $\vct f_\mathrm{max}$,
we add $\vct f^{(n)}$ into the basis
or, if the basis is full,
substitute $\vct f^{(n)}$ for $\vct f_\mathrm{max}$.
%
Otherwise,
we remove $\vct f_\mathrm{max}$ from the basis,
and if this empties the basis,
we rebuild the basis from $\vct f^{(n)}$.



The DIIS process is reduced the direct one
if $M = 1$.
%
Thus,
the method is effective
only if multiple bases are used.





\section{Results}





In testing the direct and DIIS WHAM,
the initial free energy was obtained from
the single histogram method:
%
\begin{equation*}
f_i - f_{i-1}
=
\log
\left\langle
  \exp\left[
    (\beta_j - \beta_{j-1}) \, E
  \right]
\right\rangle_j.
\end{equation*}
%
Iterations are continued
until all $|R_i|$ are reduced under a certain tolerance level.





\subsection{Ising model}





The first system is
the two-dimensional $64\times64$ Ising model.
%
We used parallel tempering\cite{
swendsen1986, *geyer1991, *hukushima1996, *hansmann1997,
*earl2005}
Monte Carlo (MC)
for
eighty temperatures: $T = 1.5$, $1.52$, \dots, $3.08$,
with $10^7$ MC steps on each temperature.



In Fig. \ref{fig:is2ref},
we show that
the dimensionless free energies computed from WHAM
agreed well with the exact result\cite{
ferdinand1969},
and DIIS WHAM produced the same results
as the direct WHAM.
%
On the other hand,
ST-WHAM produced slightly different results
(note the accumulative difference
in the density of states
from integrating the WHAM and ST-WHAM
statistical temperatures).



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2ref.pdf}
  }
  \caption{
    \label{fig:is2ref}
    (a) Dimensionless free energy and
    (b) the logarithm of the density of states
    for the $N = 64\times64$ two-dimensional Ising model.
    %
    For the insets,
    $\varepsilon(a) \equiv a - a^\mathrm{(ref)}$.
    %
    For ST-WHAM,
    the $n_j'(E)$ in Eq. \eqref{eq:beta_STWHAM}
    is estimated from
    $[n_j(E + \Delta E) - n_j(E - \Delta E)]/(2 \Delta E)$,
    with $\Delta E = 4$.
  }
\end{figure}




From Fig. \ref{fig:is2trace},
it is clear that the DIIS WHAM more rapidly
reduced the error than the direct WHAM.
%
As shown in Fig. \ref{fig:nsnt},
direct WHAM
required thousands of iterations for convergence,
while the DIIS version optimally
delivered a speedup of two orders of magnitude.
%
The real run time was similar,
showing that the overhead of DIIS was negligible
compared to the expense of computing
the right-hand side of
Eq. \eqref{eq:f_WHAM} or \eqref{eq:f_MBAR}.






\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2trace.pdf}
  }
  \caption{
    \label{fig:is2trace}
    Convergence error, $\max_i \{ |R_i| \}$,
    versus the number of iterations
    for the $64\times64$ two-dimensional Ising model.
    %
    Results were averaged over independent samples.
    %
    The lines are to guide the eyes.
  }
\end{figure}




%The second system is the 108-particle Lennard-Jones system.
%%
%Molecular dynamics (MD)
%was used with a time step of $0.002$.
%The velocity rescaling thermostat\cite{bussi2007}
%was used with a time step of $0.02$.
%%
%The potential was cutoff at $r = 2.5$.
%%
%We simulated the system under
%the canonical ($NVT$), and isothermal-isobaric ($NPT$) ensembles,
%both using parallel tempering.
%%
%For the $NVT$ ensemble,
%we used ten temperatures: $T = 0.7, 0.8, \dots, 1.6$,
%and fixed the density at $\rho = 0.3$.
%%
%For the $NpT$ ensemble,
%we simulated the system
%under $N_T \times N_p = 5\times 10$ conditions:
%the temperatures were $T = 1.0, 1.1, \dots, 1.4$,
%the pressures were $p = 0.2, 0.4, \dots, 1.8$.
%%
%The position Langevin barostat based on $\log V$
%was used with a step size of $10^{-5}$.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/nsnt.pdf}
  }
  \caption{
    \label{fig:nsnt}
    Number of iterations (empty symbols)
    and
    the real run time (filled symbols)
    to achieve $|R_i| < 10^{-8}$
    versus the number of bases, $M$,
    in DIIS WHAM.
    %
    The $M = 0$ points represent direct WHAM.
    %
    For the Ising model,
    the results have been averaged over many independent samples.
    %
    For the villin headpiece,
    roughly 10\% of the trajectory frames
    were randomly selected to build a sample each time.
    %
    The lines are to guide the eyes.
  }
\end{figure}




\subsection{Villin headpiece}



We tested the methods on a mini-protein:
villin headpiece\cite{duan1998}
(PDB ID: 1VII).
%
The protein was immersed in
a dodecahedron box with 1898 TIP3P water molecules and two chloride ions.
%
MD simulations were performed
using GROMACS\cite{
berendsen1995, *lindahl2001, *vanderspoel2005, *hess2008},
with a time step of 2 fs.
%
Velocity rescaling\cite{bussi2007}
was used as the thermostat with
the time constant being 0.1 ps.
%
The electronic interaction was
handled by the particle mesh Ewald method\cite{
essmann1995}.
%
The constraints were handled by the LINCS method\cite{
hess1997}
for hydrogen-related chemical bonds on the protein
and by the SETTLE method\cite{
miyamoto1992}
for water molecules.
%
Energy frames were registered every 0.1 ps.



We simulated the system under 12 temperatures
$T$ = 300 K, 310 K, \dots, 410 K,
each for {\color{red} 100} ns.
%
As shown in Fig. \ref{fig:nsnt},
despite the few number of temperature,
direct WHAM suffers from a slow convergence,
while the DIIS version again
delivered a speedup of two orders of magnitude,
both in terms of the number of iterations
and the real run time.


%
% In the NPT case,
% the influence of pressure is minimal
% so we do not need this example
%
%In the $NpT$ case,
%the temperatures were the same,
%and the pressures were
%1.0 bar, 1.1 bar, \dots, 2.1 bar,
%respectively.





\section{Acknowledgments}





It is a pleasure to thank Dr. Y. Zhao
for many helpful discussions.
%
Computer time on the Lonestar supercomputer
at the Texas Advanced Computing Center
at the University of Texas at Austin
is gratefully acknowledged.



\bibliography{simul}
\end{document}
