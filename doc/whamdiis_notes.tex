%\documentclass[reprint,superscriptaddress]{revtex4-1}
\documentclass[aip,jcp,preprint,notitlepage, superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}

\hypersetup{
    colorlinks,
    linkcolor={red!30!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\begin{document}




\renewcommand{\theequation}{N.\arabic{equation}}

\newcommand{\vct}[1]{\bm{\mathrm{#1}}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\var}{\mathrm{var}}

% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{OliveGreen}\small [\textbf{Comment.} #1]}}




\title{Notes on WHAM, MBAR and DIIS}

\maketitle


\tableofcontents



\section{Methods: WHAM}



\subsection{Mnemonic derivation of WHAM}



We give a mnemonic derivation of the expression
for the density of states
%
\begin{equation}
g(E)
=
\frac{
  \sum_{k = 1}^K n_k(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
}
=
\frac{
  \sum_{k = 1}^K n_k(E)
}
{
  \sum_{k = 1}^K d_k(E)
},
\label{eq:gE_WHAM}
\end{equation}
%
where
$d_k(E) \equiv N_k \exp(-\beta_k E) / Z_k$.



In ensemble $k$,
the normalized energy distribution,
$n_k(E) / N_k$,
is related to the density of states as
%
\begin{equation}
\frac{ n_k(E) } { N_k }
=
g(E) \,
\frac{ \exp(-\beta_k E) } { Z_k }.
\end{equation}
%
Here, the right-hand side
is the exact distribution.
%
Thus,
we have
%
\begin{equation}
g(E)
=
\frac{ n_k(E) }
     { d_k(E) }.
\label{eq:gnk}
\end{equation}
%
Separately averaging the numerators and denominators of different $k$'s
yields Eq. \eqref{eq:gE_WHAM}.



Another way to remember Eq. \eqref{eq:gE_WHAM}
is to consider the special case of
$\beta_1 = \dots = \beta_K$,
which allows
the $K$ unnormalized distributions
to be aggregated as
$\sum_{k=1}^K n_k(E)$.
%
Using Eq. \eqref{eq:gnk}
on the aggregated distribution
yields Eq. \eqref{eq:gE_WHAM}.



\subsection{Optimal combination of independent data}



In the main text,
we mentioned the result that
in an optimal combination,
the weight is inversely proportional to the variance.
%
To show this,
consider $K$ independent quantities,
$x_k$.
%
The combination is
\[
  X = c_1 \, x_1 + \cdots + c_K \, x_K,
\]
with
\begin{equation}
  c_1 + \cdots + c_K = 1,
  \label{eq:constraint_normalization}
\end{equation}
%
The variance of $X$ is
%
\begin{align*}
  \var(X)
=
\sum_{k = 1}^K c_k^2 \, \var(x_k)
\end{align*}
%
To minimize $\var(X)$,
we seek the minimum
under the Lagrange multiplier,
$\lambda$ of Eq. \eqref{eq:constraint_normalization}.
%
This yields,
%
\begin{align*}
  2 c_k \, \var(x_k) - \lambda = 0
\end{align*}
%
which shows that
$c_k \propto 1/\var(x_k)$,
in other words,
the optimal weight
is inversely proportional to the variance.



\subsection{Variance of the histogram}



The classical derivation of WHAM from the variance analysis
suffers from the following drawback.
%
In the literature,
the variance of the histogram height $H(E) = n(E) \, \Delta E$,
is usually assumed to satisfies a Poisson distribution,
such that $\var \, H(E) = H(E)$.
%
Strictly speaking, however,
this is incompatible with the setting here;
and we shall show below that
the right formula is
$\var \, H(E) = H(E) [ 1 - H(E)/N]$,
with $N = \sum_E H(E)$ being the sample size.
%



The conceptual difference is the following.
%
In the correct setting,
for each simulation $k$,
we have a fixed total sample size $N_k$.
%
The energy $E_i$ of each data point $\vx_i$
satisfies the canonical distribution.



If we assume a Poisson distribution,
we essentially allow the total number of data points
to fluctuate (as if the data points come from a grand-canonical ensemble).
%
In this way,
each energy bin can \emph{independently} grow
a random number of data points.
%
The total number of data points
from all bins is not fixed, however.



\subsubsection{Derivation of the variance}



The variance of histogram
of the correct setting
can be computed as follows.
%
We start by defining an indicator function $\chi(x)$,
which is $1$ if $x \in (E - \Delta E/2, E + \Delta E/2)$,
or $0$ otherwise.
%
Then, for a sample of $N$ points,
$E_1$, $E_2$, \dots, $E_N$,
we have
%
\[
  H = \sum_{j = 1}^N \chi( E_j ),
\]
%
Note that,
so far $H$ is a random variable,
instead of its average value, $\langle H \rangle$.
%
To compute the variance of $H$,
we compute the generating function\cite{vankampen}.
\begin{align*}
G(k)
&\equiv
\langle
  \exp( i k H )
\rangle
\\
&=
\left\langle
\exp\left[ i k \sum_{j = 1}^N \chi( E_j) \right]
\right\rangle
\\
&=
\prod_{j = 1}^N
\left\langle
\exp[ i k \chi( E_j) ]
\right\rangle.
\end{align*}
%
In the last step,
we have assumed that the data points are independent.
%
Since the data points
satisfy the same distribution,
we further have
\begin{align*}
G(k)
&=
  \left\langle
    \exp[ i k \chi( E_1) ]
  \right\rangle^N.
\end{align*}
%
Or
\begin{align*}
\log G(k)
&=
N \log
  \left\langle
    \exp[ i k \chi( E_1) ]
  \right\rangle
.
\end{align*}


Now
\begin{align*}
  \left\langle
    \exp[ i k \chi( E_1) ]
  \right\rangle
&=
  1 +
  \sum_{p = 1}^\infty
    \frac{ (i k)^p }{ p! }
    \langle
    \chi^p( E_1 )
    \rangle
\\
&=
  1  +
  \sum_{p = 1}^\infty
    \frac{ (i k)^p }{ p! }
    \langle
    \chi( E_1 )
    \rangle
\\
&=
  1
  +
  \langle
  \chi( E_1 )
  \rangle
  [\exp( i k ) - 1]
\\
&=
  1
  +
  \langle
  \chi( E_1 )
  \rangle
  \left[
  i k
  +
  \frac{ (i k)^2 } { 2! }
  + \dots
  \right].
\end{align*}
%
Here,
we have used the fact that
$\chi^p(E) = \chi(E)$.



With the definition $\bar\chi \equiv \langle \chi( E_1 ) \rangle$,
we have
%
\begin{align*}
\log G(k)
&=
N
\left[
  \bar\chi \cdot (i k)
+
  \bar\chi
  \left(
    1 - \bar\chi
  \right)
  \frac{ (i k)^2 } { 2! }
+ \dots
\right]
.
\end{align*}
%
By definition,
the power expansion of $\log G(k)$
gives all cumulants\cite{vankampen}:
\begin{align*}
\log G(k)
&=
 \langle H \rangle \cdot (i k)
+
\var(H)
\frac{ (i k)^2 } { 2! }
+ \dots
.
\end{align*}
%
This means
%
\begin{align}
\langle H \rangle
&=
N \cdot \bar\chi,
\label{eq:hist_mean} \\
\var( H )
&=
N \cdot \bar\chi (1 - \bar\chi)
=
\langle H \rangle \cdot ( 1 - \langle H \rangle /N).
\label{eq:hist_var}
\end{align}
%
This shows the variance of
the histogram height
is $\langle H \rangle (1 - \langle H \rangle/N)$,
which is not equal to $\langle H \rangle$,
unless $\langle H \rangle \ll N$.



The above result can be alternatively
understood from the results of the binomial distribution.
%
If a random event can only two outcomes, $1$ and $0$
with probabilities $p$ and $q = 1 - p$,
then the number of $1$'s after $N$ events, $n$
satisfies the binomial distribution,
which means the mean of $n$ is given by $N \, p$,
and the variance is given by
$N \, p \, q = N \, p \, (1 - p)$.
%
To adapt this result to our problem,
let the outcome being $1$ mean
$x$ falling in $(E - \Delta E/2, E + \Delta E/2)$,
or $\chi(x)$ being 1.
%
Then we set $p = \bar \chi$,
and the mean and variance of the binomial distribution
lead to Eqs. \eqref{eq:hist_mean} and \eqref{eq:hist_var}
respectively.





\subsubsection{Possible resolution}



The above analysis clear shows
that for the classical derivation of WHAM
does not strictly minimize the variance of
the density of states or the distributions.
%
However,
it does not means that
the free energies obtained from WHAM are not optimized.
%
Actually, from the more rigorous derivation of BAR/MBAR,
which yields the same result as WHAM,
we know that the estimates of the free energies
are in fact optimized.
%
Besides the composite ensemble argument,
clearly supports that.


So, we can say that the WHAM result
optimizes the free energy,
but it \emph{might} not always optimize
the histograms of finite width.
%
This difference may be
similar to the difference between canonical and microcanonical ensembles.


We should also be aware of
that the formula for optimal combination of histograms
assumes that histogram at different energies
to be independent.
%
This is clearly untrue.
%
But if this were true,
then each histogram would
satisfy a Poisson distribution.



\subsection{Derivation of Eq. (6) from Eqs. (4) and (5)}



From Eq. (4)
\begin{align*}
f_i
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K n_j(E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K \sum_{\vct x}^{(j)}
      \delta(\E(\vx) - E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
  \int
    \frac{
      \exp(-\beta_j E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    \delta(\E(\vx) - E) \, dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
    \frac{
      \exp(-\beta_i \, \E(\vx))
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k \, \E(\vx) + f_k)
    },
\end{align*}
which is Eq. (6).



\subsection{Derivation of Eq. (7) from Eq. (6)}



From Eq. (6)
\begin{align*}
f_i
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[-\Ham_0(\vx) - \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[-\Ham_0(\vx) - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\delta(\W(\vx) - W) \, dW
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W)
  \exp f_k
}
\delta(\W(\vx) - W)
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  \left[
    \sum_{\vx}^{(j)}
    \delta(\W(\vx) - W)
  \right]
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  n_j(W)
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW,
\end{align*}
which is Eq. (7).




\section{Methods: ST-WHAM and related methods}



\subsection{Statistical-temperature WHAM}



Taking the logarithm of Eq. \eqref{eq:gE_WHAM} and differentiating,
we get
\begin{equation}
(\log g)'(E)
=
\frac{ \sum_{k = 1}^K n'_k(E) }
     { \sum_{k = 1}^K n_k(E) }
+
\frac{ \sum_{k = 1}^K d_k(E) \, \beta_k }
     { \sum_{k = 1}^K d_k(E) }.
\label{eq:beta0_STWHAM}
\end{equation}
%
By Eq. \eqref{eq:gnk},
we have
$n_k(E) = d_k(E) \, g(E)$,
which shows that
$d_k(E)$
is proportional to $n_k(E)$
for a fixed $g(E)$.
%
So we can rewrite Eq. \eqref{eq:beta0_STWHAM} as
%
\begin{equation}
(\log g)'(E)
=
\sum_{k = 1}^K c_k(E) \, \left[ (\log n_k)'(E)  + \beta_k \right],
\label{eq:beta_STWHAM}
\end{equation}
%
where
\begin{align}
  c_k(E) \equiv \frac{ n_k(E)  }{ \sum_{l = 1}^{K} n_l(E) },
  \label{eq:ck}
\end{align}
%
This is the main result of ST-WHAM
from the original paper\cite{
kim2011}.


It is possible to get rid of
the derivative $(\log n_k)'(E)$.
%
To do so, we only need to apply the differentiation
to the logarithm of the denominator of Eq. \eqref{eq:gE_WHAM}.
%
\begin{align}
\frac{d}{dE}
\log
  \sum_{k = 1}^K d_k(E)
&=
-
\frac{
  \sum_{k = 1}^K d_k(E) \, \beta_k
}
{
  \sum_{k = 1}^K d_k(E)
}
%\notag\\
%&
=
-
\frac{
  \sum_{k = 1}^K n_k(E) \, \beta_k
}
{
  \sum_{k = 1}^K n_k(E)
}.
%\notag
\label{eq:ddenom_STWHAM}
\end{align}
%
Then,
%
\begin{align}
\log \sum_{k = 1}^K d_k(E)
&=
-
\int^E
\frac{
  \sum_{k = 1}^K n_k(E') \, \beta_k
}
{
  \sum_{k = 1}^K n_k(E')
} dE'.
%\notag
\label{eq:denom_STWHAM}
\end{align}
%
Using Eq. \eqref{eq:denom_STWHAM} in Eq. \eqref{eq:gE_WHAM} yields
%
\begin{align}
g(E)
=
\left[
  \sum_{k = 1}^K n_k(E)
\right]
\,
\exp
\left[
\int^E
    \frac{ \sum_{k = 1}^K n_k(E') \, \beta_k }
         { \sum_{k = 1}^K n_k(E') }
  dE'
\right].
%\notag
\label{eq:g_STWHAM}
\end{align}



\subsection{W-DF method}



The W-DF method by Shen and McCammon\cite{
shen1991, roux1995}
used a similar weighting strategy.
%
\begin{align}
g(E) = \sum_{k = 1}^K c_k(E) \, \frac{ n_k(E) } { d_k(E) },
\end{align}
%
where $c_k(E)$ is given by Eq. \eqref{eq:ck}.
%
Note that
if $c_k(E)$
is replaced by $ d_k(E) / \sum_{l = 1}^K d_l(E)$,
we recover WHAM.



\subsection{W-PMF method}



Another variant is the W-DF method by Woolf and Roux\cite{
woolf1994, crouzy1994, roux1995}.
%
Here,
%
\begin{align}
\log g(E) = \sum_{k = 1}^K c_k(E) \, \log \frac{ n_k(E) } { d_k(E) },
\end{align}
%
where $c_k(E)$ is given by Eq. \eqref{eq:ck}.
%







\section{Methods: DIIS}


\subsection{Determination of coefficients in DIIS}



The coefficients $c_i$ are determined as follows.
%
We wish to minimize
%
\[
S
=
\vct{\hat R}^2 / 2
=
\frac 1 2 \left( \sum_i c_i \, \vct R_i \right)^2.
\]
under the constraint
%
\begin{equation}
\sum_i c_i = 1
\label{eq:c_normalize}
\end{equation}



Thus we shall minimize the function
\[
  S - \lambda \sum_i c_i
\]
with respect all $c_i$.
%
Thus, we get
\begin{equation}
  \sum_j c_j \, (\vct R_j \cdot \vct R_i) - \lambda = 0,
  \label{eq:cj_DIIS}
\end{equation}


Equations \eqref{eq:cj_DIIS} and \eqref{eq:c_normalize}
can be written in matrix form
%
\begin{equation}
\left(
  \begin{array}{ccccc}
    \vct R_1 \cdot \vct R_1 &
    \vct R_1 \cdot \vct R_2 &
    \dots &
    \vct R_1 \cdot \vct R_M &
    -1 \\
    \vct R_2 \cdot \vct R_1 &
    \vct R_2 \cdot \vct R_2 &
    \dots &
    \vct R_2 \cdot \vct R_M &
    -1 \\
    \vdots &
    \vdots &
    &
    \vdots &
    -1 \\
    \vct R_M \cdot \vct R_1 &
    \vct R_M \cdot \vct R_2 &
    \dots &
    \vct R_M \cdot \vct R_M &
    -1 \\
    1 &
    1 &
    \dots &
    1 &
    0
  \end{array}
\right)
\left(
  \begin{array}{c}
    c_1 \\
    c_2 \\
    \vdots \\
    c_M \\
    \lambda
  \end{array}
\right)
=
\left(
  \begin{array}{c}
    0 \\
    0 \\
    \vdots \\
    0 \\
    1
  \end{array}
\right).
\end{equation}





\subsection{Parameters for Figure 1}



The illustration was produced according to the following potential
%
\begin{equation}
F
=
\frac{1}{2} A \, ( x - x_0 )^2
+
\frac{1}{2} B \, ( y - y_0 )^2
+
C \, ( x - x_0 ) ( y - y_0 ),
\label{eq:fig1_F}
\end{equation}
%
where
$x_0 = 3/5$,
$y_0 = 4/5$,
$A = 3/4$,
$B = 5/4$,
and
$C = 1/4$.



The two vectors are
$\vct f_1 = (1, 0)$
and
$\vct f_2 = (0, 1)$.
%
The residual vectors are
$\vct R_1 = (-1/10, 9/10)$
and
$\vct R_2 = (4/10, -1/10)$,
respectively.



We now determine
$\vct{\hat R} = \vct R_1 + \lambda \, (\vct R_2 - \vct R_1)$
with the minimal magnitude
$\| \vct{\hat R} \|$.
%
It is readily shown that
$\lambda$ satisfies
%
\[
\lambda
=
-\frac{
  \Delta \vct R \cdot \vct R_1
}
{
  \Delta \vct R \cdot \Delta \vct R
}
=
-\frac{-19/20}{5/4}
=
\frac{19}{25},
\]
%
where
$\Delta \vct R = \vct R_2 - \vct R_1 = (1/2, -1)$.
%
Thus,
\[
\vct{\hat R}
=
(1 - \lambda) \, \vct R_1
+ \lambda \, \vct R_2
=
\left(
  \frac{7}{25},
  \frac{7}{50}
\right),
\]
\[
\vct{\hat f}
=
(1 - \lambda) \, \vct f_1
+ \lambda \, \vct f_2
=
\left(
  \frac{6}{25},
  \frac{19}{25}
\right),
\]
and
\[
\vct{\hat f}
+
\vct{\hat R}
=
\left(
  \frac{13}{25},
  \frac{9}{10}
\right).
\]



We can complete squares of $F$ as
%
\begin{align*}
F
&=
\frac{1}{16}
\left\{
  \left( 5 + 3 \sqrt 2 \right)
  \left[
    \left( \sqrt 2 - 1 \right) \, \bar x
  +
    \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{1}{16}
\left\{
  \left( 3 - \sqrt 2 \right)
  \left[
    \bar x
  +
    \left( \sqrt 2 + 1 \right) \, \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{7}{2} R^2,
\end{align*}
%
where
%
\begin{align*}
\bar x
&= x - x_0
\\
&=
\left(
  \sqrt{ 5 - 3 \sqrt{2} } \, \cos \theta
  +
  \sqrt{ 5 + 3 \sqrt{2} } \, \sin \theta
\right) R
\\
&=
\left[
  \left( \sqrt 2 - 1 \right) \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  +
  \left( \sqrt 2 + 1 \right) \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right] R,
\end{align*}
%
and
%
\begin{align*}
\bar y
&= y - y_0
\\
&=
\left(
  \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  -
  \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right) R,
\end{align*}



The short axis is achieved at $\theta = 0$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 - \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(\sqrt 2 - 1\right) = \pi/8$.



The long axis is achieved at $\theta = \pi/2$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 + \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(-\sqrt 2 - 1\right) = -3 \, \pi/8$.




\section{Appendix A: Probabilistic derivation of the MBAR equation}



\subsection{Maximum-likelihood argument}



From the unnumbered equation after Eq. (A1),
%we get the posterior probability
%\begin{align*}
%p\left( g | \{ \vct x \} \right)
%&=
%\frac{
%  p\left( \{ \vct x \} | g \right) \, p(g)
%}
%{
%  p\left( \{ \vct x \} \right)
%}
%\\
%&\propto
%p\left( \{ \vct x \} | g \right).
%\end{align*}
%
by taking the logarithm, we get
%
\begin{align*}
\log p\left( \{ \vct x \} | g \right)
&=
\sum_{k = 1}^K
  \sum_{\vct x}^{(k)}
    \left(
      \log g(\vx) + \log q_k(\vx) - \log Z_k[g]
    \right)
+ C
\\
&=
\sum_{k = 1}^K
\left(
  \sum_{\vx}^{(k)}
    \log g(\vx) - N_k \log Z_k[g]
\right) + C',
\end{align*}
%
where $C$ and $C'$
are constants independent of $g$.
%
We now differentiate it with respect to $g(\vy)$,
%
\begin{align}
\frac{
  \delta \log p\left( \{ \vct x \} | g \right)
}
{
  \delta g(\vy)
}
&=
\sum_{k = 1}^K
\left(
  \sum_{\vct x}^{(k)}
  \frac{ \delta( \vy - \vx ) }
  { g(\vy) }
  -
  \frac {N_k} { Z_k[g] }
  \frac{ \delta Z_k[g] } { \delta g(\vy) }
\right)
= 0.
\label{eq:dpdg}
\end{align}
%
Now since
\begin{equation}
Z_k[g]
=
\int g(\vy) \, q_k(\vy) \, d\vy,
\tag{A1}
\label{eq:Zg}
\end{equation}
we have
\begin{equation*}
\frac{ \delta Z_k[g] } { \delta g(\vy) }
= q_k(\vy).
\end{equation*}
%
Using this in Eq. \eqref{eq:dpdg},
we get
\begin{equation}
g(\vy)
=
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
}.
\tag{A2}
\label{eq:gy}
\end{equation}
%
Finally,
we use Eq. \eqref{eq:gy} in Eq. \eqref{eq:Zg},
%
\begin{align}
Z_i[g]
&=
\int
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
}
q_i(\vy) d\vy
\notag \\
&=
\sum_{j = 1}^K \sum_{\vct x}^{(j)}
\int
\frac{
  \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
}
q_i(\vy) d\vy
\notag \\
&=
\sum_{j = 1}^K \sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct x) / Z_k[g]
}.
\end{align}




\subsection{Expectation-maximization argument}



\paragraph{Function to maximize}



In the manuscript,
we show that the likelihood function is
%
\begin{align}
  L(\vct \nu)
  = p\left( \{ \vx \} | \vct \nu \right)
  =
  \prod_{\vx}
    \sum_{ k = 1 }^K
      \nu_k \, w_k(\vx).
\end{align}
%
Taking the logarithm,
we get
%
\begin{align}
  \log L(\vct \nu)
  =
  \sum_{\vx}
  \log\left(
  \sum_{ k = 1 }^K
    \nu_k \, w_k(\vx)
  \right).
\end{align}
%
Our aim is to maximize this likelihood function.
%
Since $\nu_i$ represents the fractions of mixing,
we have a constraint
%
\begin{equation}
  \sum_{i = 1}^K \nu_i = 1.
  \label{eq:constraint_nu}
\end{equation}
%
Thus the actual function to maximize is
%
\begin{align*}
  S(\vct \nu)
  &=
  \log L(\vct \nu) - \lambda \,
    \left( \sum_{i = 1}^K \nu_i - 1 \right)
  \notag \\
  &=
  \sum_{\vx}
  \log\left(
  \sum_{ k = 1 }^K
    \nu_k \, w_k(\vx)
  \right)
  -
  \lambda \,
  \left(
    \sum_{i = 1}^K \nu_i - 1
  \right),
\end{align*}
where $\lambda$ is the Lagrange multiplier.




\paragraph{Location of the maximum}



The maximum of $S(\vct \nu)$
should be achieved at
\[
  \frac{ \partial S(\vct \nu) }
  { \partial \nu_i } = 0.
\]
%
This gives
%
\begin{align}
  \sum_{\vx}
    \frac{ w_i(\vx) }
    { \sum_{k = 1}^K \nu_k \, w_k(\vx) }
  =
  \lambda.
  \label{eq:sumx_lambda}
\end{align}



\paragraph{Determination of the Lagrange multiplier}



We still need to determine $\lambda$.
%
To do this, we multiply Eq. \eqref{eq:sumx_lambda}
by $\nu_i$ and sum over $i$.
For the left-hand side,
we have
%
\begin{align*}
  \sum_{i = 1}^K \nu_i \, \sum_{\vx}
    \frac{ w_i(\vx) }
    { \sum_{k = 1}^K \nu_k \, w_k(\vx) }
  &=
  \sum_{\vx}
    \frac
    { \sum_{i = 1}^K \nu_i \, w_i(\vx) }
    { \sum_{k = 1}^K \nu_k \, w_k(\vx) }
  = \sum_{\vx} 1 = N_\mathrm{tot}.
\end{align*}
%
For the right-hand side,
we have
%
\begin{align*}
  \lambda \, \sum_{i = 1}^K \nu_i
  &=
  \lambda.
\end{align*}
%
This means that the Lagrange multiplier
\[
  \lambda = N_\mathrm{tot}.
\]


\paragraph{Finishing up}


Using this result in Eq. \eqref{eq:sumx_lambda},
we get
%
\begin{equation*}
  \sum_{ \vx}
    \frac{
      w_i(\vx)
    }
    {
      \sum_{k = 1}^K \nu_k \, w_k(\vx)
    }
    = N_\mathrm{tot}.
\end{equation*}
%
Finally by using
\begin{align*}
  w_i(\vx) &= q_i(\vx) / Z_i, \\
  \nu_k    &= N_k / N_\mathrm{tot}
\end{align*}
we get
\begin{equation*}
  \sum_{ \vx }
  \frac{ q_i(\vx) / Z_i }
  { \sum_{k = 1}^K N_k \, q_k(\vx) / Z_k }
  = 1,
\end{equation*}
which becomes the MBAR result,
upon multiplying both sides by $Z_i$.





\section{Appendix B: Models for the convergence}



%We shall use the normalization
%\begin{equation}
%\int \rho_i(E) \, dE
%= 1.
%\label{eq:rho_normalize}
%\end{equation}



\subsection{Linearized WHAM equation}


\subsubsection{Derivation of the linearized equation, Eq. (B2)}



The WHAM equation is
%
\begin{align}
f_i^\mathrm{(new)}
&=
-\log
\int
\frac
{
  \sum_{k=1}^K
    N_k \, \rho_k(E) \, \exp(-\beta_i \, E)
}
{
  \sum_{k=1}^K
    N_k \, \exp( -\beta_k \, E + f_k^\mathrm{(old)} )
}
dE
\notag \\
&=
-\log
\int
\frac
{
  \sum_{k=1}^K N_k \, \rho_k(E) \, \exp(-\beta_i \, E)
}
{
  D(E; \vct f^\mathrm{(old)})
}
dE,
\label{eq:f_WHAM1}
\end{align}
%
where
%
\begin{align}
D(E; \vct f)
=
\sum_{k=1}^K N_k \, \exp(-\beta_k \, E + f_k).
\end{align}
%
If $N_i = \mathrm{const}$, then it is reduced to Eq. (B1) in the main text.



The linearized equation is
\begin{align}
\delta f_i^\mathrm{(new)}
&=
\left.
  \int
  \frac
  {
    \sum_{k=1}^K N_k \, \rho_k(E) \, e^{-\beta_i \, E}
  }
  {
    D(E; \vct f^*)^2
  }
  \delta D(E; \vct f^\mathrm{(old)}) \,
  dE
\middle/
  \int
  \frac
  {
    \sum_{k=1}^K N_k \, \rho_k(E) \, e^{-\beta_i \, E}
  }
  {
    D(E; \vct f^*)
  }
  dE
\right.
%
\notag \\
&=
\sum_{j = 1}^K
\left(
\int
\frac
{
  \sum_{k=1}^K N_k \, \rho_k(E) \, e^{-\beta_i \, E + f_i^*}
}
{
  D(E; \vct f^*)^2
}
N_j \, e^{-\beta_j \, E + f_j^*}
dE
\right)
\, \delta f_j^\mathrm{(old)}
\notag \\
&=
\sum_{j = 1}^K
A_{ij} \, \delta f_j^\mathrm{(old)},
%\tag{B2}
\label{eq:f_lin}
\end{align}
where we have used
Eq. \eqref{eq:f_WHAM1}
in the second step
for the denominator.




\subsubsection{On the normalization of $A_{ij}$, Eq. (B3)}


From \eqref{eq:f_lin}, we have
%
\begin{align}
  \sum_{j = 1}^K A_{ij}         &= 1,
\label{eq:Aij_normalization_j} \\
  \sum_{i = 1}^K N_i \, A_{ij}  &= N_j.
\label{eq:Aij_normalization_i}
\end{align}



For the first condition, we have
\begin{align}
\sum_{j = 1}^K
A_{ij}
&=
\int
\frac
{
  \sum_{k=1}^K N_k \, \rho_k(E) \,
  \sum_{i = 1}^K e^{-\beta_i \, E + f_i^*}
}
{
  D(E; \vct f^*)^2
}
N_j \, e^{-\beta_j \, E + f_j^*}
dE
\notag \\
&=
\int
\frac
{
  \sum_{k=1}^K N_k \rho_k(E)
}
{
  D(E; \vct f^*)
}
e^{-\beta_i \, E + f_i^*}
dE
=
e^{-f_i^* + f_i^*} = 1.
\notag
\end{align}
%
where,
we have used Eq. \eqref{eq:f_WHAM1}
in the last step
(with
$f_i^\mathrm{(old)} \rightarrow f_i^*$
and
$f_i^\mathrm{(new)} \rightarrow f_i^*$
).



Similarly,
\begin{align}
\sum_{i = 1}^K
N_i \, A_{ij}
&=
\int
\frac
{
  \sum_{k=1}^K N_k \, \rho_k(E) \,
  \sum_{i = 1}^K N_i \, e^{-\beta_i \, E + f_i^*}
}
{
  D(E; \vct f^*)^2
}
N_j \, e^{-\beta_j \, E + f_j^*}
dE
\notag \\
&=
\int
\frac
{
  \sum_{k=1}^K N_k \, \rho_k(E)
}
{
  D(E; \vct f^*)
}
N_j \, e^{-\beta_j \, E + f_j^*}
dE
=
N_j \, e^{-f_j^* + f_j^*} = N_j.
\notag
\end{align}
%


Thus, from Eq. \eqref{eq:f_lin}, we get
%
\begin{align}
  \sum_{i = 1}^K
    N_i \, \delta^\mathrm{(new)}
&=
  \sum_{j = 1}^K
    \sum_{i = 1}^K
      N_i \, A_{ij}
    \, \delta f_j^\mathrm{(old)} \notag \\
&=
  \sum_{j = 1}^K
    N_j \, \delta f_j^\mathrm{(old)},
  \notag
\end{align}
%
where we have used Eq. \eqref{eq:Aij_normalization_i}.




\subsection{Two-temperature case}

\subsubsection{Evaluation of $A_{12}$}



For convenience,
we set
\begin{align}
  E_c = (\beta_1 + \beta_2)/(2a).
  \label{eq:Ec_twoT}
\end{align}
%
Then the density of states is
%
\begin{align}
g(E)
&=
\sqrt{ \frac a { 2 \, \pi } }
\exp
\left[
  -\frac 1 2 a
  (E - E_c)^2
\right]
\notag \\
&=
\sqrt{ \frac a { 2 \, \pi } }
\exp
\left[
  -\frac 1 2 a \, E^2
  + \frac{
    (\beta_1 + \beta_2) \, E
  }{2}
  - \frac{
    (\beta_1 + \beta_2)^2
  }{ 8 \, a }
\right].
\end{align}


The dimensionless free energy is
%
\begin{align}
f_1
&=
\beta_1 \, E_c
- \frac{ \beta_1^2 } { 2 \, a }
=
\frac{\beta_1 \, \beta_2 } { 2 \, a }
= f_2.
\end{align}



Now for $A_{12}$
\begin{align}
A_{12}
&=
\int
\frac
{
  g(E) \, e^{ -(\beta_1 + \beta_2) \, E + f_1^* + f_2^*}
}
{
  D(E; \vct f^*)
}
dE
\notag \\
&=
\sqrt{ \frac a { 2 \, \pi } }
\int
\frac
{
  \exp\left[
    -\frac 1 2 a E^2
    - \frac 1 2 (\beta_1 + \beta_2) \, E
    - \frac{
      (\beta_1 + \beta_2)^2
    }{ 8 \, a }
    +
    \frac{ \beta_1 \, \beta_2 } { a }
  \right]
}
{
  \exp\left(
    -\beta_1 \, E
    + \frac{\beta_1 \, \beta_2}{2 \, a}
  \right)
  +
  \exp\left(
    -\beta_2 \, E
    + \frac{\beta_1 \, \beta_2}{2 \, a}
  \right)
}
dE
\notag \\
&=
\sqrt{ \frac a { 2 \, \pi } }
\int
\frac
{
  \exp\left[
    -\frac 1 2 a E^2
    - \frac{
      (\beta_1 - \beta_2)^2
    }{ 8 \, a }
  \right]
}
{
  \exp\left(
    \frac{ \beta_2 - \beta_1 } {2 } \, E
  \right)
  +
  \exp\left(
    \frac{ \beta_1 - \beta_2 } {2 } \, E
  \right)
}
dE
\notag \\
&=
\frac 1 2
\sqrt{ \frac a { 2 \, \pi } }
\int
\frac
{
  \exp\left[
    -\frac 1 2 a E^2
    - \frac{
      \Delta \beta^2
    }{ 8 \, a }
  \right]
}
{
  \cosh\left(
    \Delta \beta \, E / 2
  \right)
}
dE.
\label{eq:A12}
\end{align}



To evaluate the integral,
we use the expansion
\begin{equation}
1/\cosh(x)
=
\exp(-x^2/2) \,
(1 + x^4/12 + \dots),
\label{eq:invcoshexp}
\end{equation}
%
Better results are obtained
when Eq. \eqref{eq:invcoshexp}
is truncated at an even number of terms,
as is done here.
%
Thus,
\begin{align}
A_{12}
&=
\frac 1 2
\sqrt{ \frac a { 2 \, \pi } }
\int
  \exp\left[
    -\frac 1 2
      \left(
          a + \frac{\Delta \beta^2}{4}
      \right) E^2
    - \frac{
      \Delta \beta^2
    }{ 8 \, a }
  \right]
  \left(
    1 + \frac{ (\Delta \beta \, E)^4 } { 12 \cdot 16 }
  \right)
dE
\notag \\
&=
\frac 1 2
\sqrt{
  \frac{ a  }
  { a +  \Delta \beta^2/4 }
}
\exp\left(
  - \frac{
    \Delta \beta^2
  }{ 8 \, a }
\right)
\left(
  1 + \frac{ \Delta \beta^4 \langle E^4 \rangle}
  { 12 \cdot 16 }
\right)
\notag \\
&=
\frac 1 2
\frac
{
  \exp\left[
    - \Delta \beta^2 / (8 \, a)
  \right]
}
{
  \sqrt{1 +  \Delta \beta^2/(4 \, a) }
}
\left(
  1 + \frac{ \Delta \beta^4 / 16 }
  { 4 \, (a + \Delta \beta^2 / 4)^2}
\right).
\end{align}




\subsection{Three-temperature case}



\subsubsection{Eigenvalues of the $3\times 3$ matrix}


The transition matrix $\vct A$
for three temperatures is
\begin{align*}
\vct A
=
\left(
  \begin{array}{ccc}
    1 - A_{12} - A_{13}   &     A_{12}    &  A_{13}             \\
    A_{12}                &  1 - 2 A_{12} &  A_{12}             \\
    A_{13}                &     A_{12}    &  1 - A_{12} - A_{13}
  \end{array}
\right).
\end{align*}
%
There are three eigenvalues.
%
The first is $\lambda_0 = 1$,
with the eigenvector being $\vct c_0 = (1, 1, 1)$.
%
The second is $\lambda_1 = 1 - A_{12} - 2 \, A_{13}$,
and
\begin{align*}
\vct A - \lambda_1 \vct I
=
\left(
  \begin{array}{ccc}
    A_{13}   &     A_{12}           &  A_{13} \\
    A_{12}   &  2 A_{13} - A_{12}   &  A_{12} \\
    A_{13}   &     A_{12}           &  A_{13}
  \end{array}
\right).
\end{align*}
%
So, the eigenvector $\vct c_1 = (1, 0, -1)$.
%
The third is $\lambda_2 = 1 - 3 \, A_{12}$,
%
\begin{align*}
\vct A - \lambda_1 \vct I
=
\left(
  \begin{array}{ccc}
    2 \, A_{12} - A_{13}   &     A_{12}         &  A_{13} \\
    A_{12}                 &  A_{13} + A_{12}   &  A_{12} \\
    A_{13}                 &     A_{12}         &  2 \, A_{12} - A_{13}
  \end{array}
\right).
\end{align*}
%
So the eigenvector $\vct c_2 = (1, -2, 1)$.
%
Usually,
$A_{12} > A_{13}$,
so $\lambda_1 > \lambda_2$.



\subsubsection{Temperature setup}



For the Gaussian model,
the origin of the inverse temperature, $\beta$,
is arbitrary,
because the center of
energy distribution is shifted linearly with $\beta$.
%
For the three-temperature case,
we can assume that the three temperatures are
$\beta_1 = -\beta$,
$\beta_2 = 0$,
and
$\beta_3 = \beta$.



\subsubsection{Evaluation of $A_{12}$}



For $A_{12}$,
we first transform
\begin{align}
D(E)
&=
\exp\left(
  \beta E - \tfrac{ \beta^2 }{ 2 \, a }
\right)
+
1
+
\exp\left(
  -\beta E - \tfrac{ \beta^2 }{ 2 \, a }
\right)
\notag \\
&=
\exp\left(
  \beta \Delta E
\right)
+
1
+
\exp\left(
  -\beta \Delta E - \tfrac{ \beta^2 }{ a }
\right)
\notag \\
&=
\exp\left(
  \tfrac 1 2 \beta \Delta E
\right)
\left[
\exp\left(
  \tfrac 1 2 \beta \Delta E
\right)
+
\exp\left(
  -\tfrac 1 2 \beta \Delta E
\right)
+
\exp\left(
  -\tfrac 3 2 \beta \Delta E
  - \tfrac{ \beta^2 }{ a }
\right)
\right]
\notag \\
&=
\exp\left(
  \tfrac 1 2 \beta \Delta E
\right)
\left[
2 \cosh\left(
  \tfrac 1 2 \beta \Delta E
\right)
+
\exp\left(
  -\tfrac 3 2 \beta \Delta E
  - \tfrac{ \beta^2 }{ a }
\right)
\right]
\notag \\
&=
\exp\left(
  b x
\right)
\left[
2 \cosh\left(
  b x
\right)
+
\exp\left(
  -3 \, b \, x
  -4 \, b^2
\right)
\right],
\label{eq:DE_threeT_exact}
\end{align}
%
where $\Delta E = E - \beta/(2 \, a)$.
%
On the next line,
we have introduced
the dimensionless quantities
$x = \sqrt{a} \, \Delta E$,
and
$b = \beta / \sqrt{4 \, a}$.


We now make an approximation
\begin{align}
D(E)
&\approx
\exp\left(
  b \, x
\right)
\left[
2 \cosh\left(
  b \, x
\right)
+
\exp\left(
  - u \, b^2
\right)
\right]
\label{eq:DE_threeT_approx1}
\\
&=
2 \exp\left(
  b \, x
\right)
\left[
\cosh\left(
  b \, x
\right)
+
\theta - 1
\right]
\notag \\
&\approx
2 \, \theta
\left.
  \exp\left(
    b \, x
    +
    \frac{ b^2 \, x^2 }
    {2 \, \theta}
  \right)
\middle/
\left[
  1
  +
  \frac{ (3 - \theta) \, b^4 x^4 }
  { 24 \, \theta^2 }
\right]
\right.
,
\label{eq:DE_threeT_approx}
\end{align}
%
where
$\theta \equiv 1 + e^{-u \, b^2}/2$,
%
and
$u$ is a parameter to be determined.



$A_{12}$ is computed as follows
%
\begin{align}
  A_{12}
&=
\sqrt{ \frac a {2 \, \pi} }
\int
  \frac{ \exp[-\frac 1 2 a E^2 + \beta E - \beta^2/(2a) ] }
  { D(E) } \, dE
\notag \\
&\approx
\sqrt{ \frac 1 {2 \, \pi} }
\int
  \frac{ \exp[-\frac 1 2 x^2 + b \, x - b^2 / 2 ] }
  { 2 \, \theta \,
    \exp\left(
      b \, x
      +
      \frac{ b^2 \, x^2 }
      { 2 \, \theta }
    \right)
  }
  \left[
    1
    +
    \frac{ (3 - \theta) \, b^4 x^4 }
    { 24 \, \theta^2 }
  \right]
 \, d x
\notag \\
&=
\frac { \exp(-b^2/2) }
{ 2 \, \theta \, \sqrt{2 \, \pi} }
\int
  \exp\left[
    -\frac 1 2
    \left(
      1 + \frac{ b^2 } { \theta }
    \right)
    x^2
  \right]
  \left[
    1
    +
    \frac{ (3 - \theta) \, b^4 x^4 }
    { 24 \, \theta^2 }
  \right]
 \, d x
\notag \\
&=
\frac { \exp(-b^2/2) }
{ 2 \, \theta \, \sqrt{1 + b^2/\theta} }
  \left[
    1
    +
    \frac{ (3 - \theta) \, b^4 }
    { 8 \, (\theta + b^2)^2}
  \right]
 \, d x.
\end{align}



To determine $u$,
we compare the small $b$ expansions.
%
For Eq. \eqref{eq:DE_threeT_exact},
we have
%
\begin{align*}
\frac{ 3 \exp(b \, x) }
{ D(E) }
&\approx
\left[
  1 -
  x \, b
  +
  \left(
    \frac{ 11 x^2 } 6
    - \frac 4 3
  \right)
  b^2
\right]^{-1}
\notag \\
&\approx
  1 +
  x \, b
  +
  \left(
    \frac 4 3
    - \frac{ 5 \, x^2 } 6
  \right)
  b^2
\end{align*}
%
For Eq. \eqref{eq:DE_threeT_approx},
we have
%
\begin{align*}
\frac{ 3 \exp(b \, x) }
{ D(E) }
&\approx
\left(
  1 +
  \frac{ x^2 - u } { 3 }
  b^2
\right)^{-1}
\notag \\
&\approx
  1 +
  \frac{ u - x^2 } { 3 }
  b^2
\end{align*}
%
This quantity is to be integrated with
a distribution-like quantity:
$\exp(-\frac1 2 x^2)/\sqrt{2 \pi}$,
and $x$ and $x^2$ can be replaced by their
averages, $0$ and $1$, respectively.
%
Thus,
to get the consistent small $b$ behavior,
we have
$4/3 - 5/6 = (u - 1)/3$,
and $u = 5/2$.




\subsubsection{Improving the accuracy of $A_{12}$}



The accuracy may be improved
by a more complex function form, e.g.
%
\begin{align*}
u = \frac 5 2
\times
\frac{1+5.8 \, b^2}{1 + 7 \, b^2}.
\end{align*}
%
This formula gives more consistent
small $b$ behavior with the exact result.





\subsubsection{Evaluation of $A_{13}$}



For $A_{13}$,
we transform
\begin{align}
D(E)
&=
\exp\left(
  \beta E - \tfrac{ \beta^2 }{ 2 \, a }
\right)
+
1
+
\exp\left(
  -\beta E - \tfrac{ \beta^2 }{ 2 \, a }
\right)
\notag \\
&=
\exp\left(
  -\tfrac{ \beta^2 } { 2 \, a }
\right)
\left[
2 \cosh\left(
  \beta E
\right)
+
\exp\left(
  \tfrac{ \beta^2 } { 2 \, a }
\right)
\right]
\notag \\
&=
\exp\left(
  -\tfrac{ B^2 } { 2 }
\right)
\left[
2 \cosh\left(
  B x
\right)
+
\exp\left(
  \tfrac{ B^2 } { 2 }
\right)
\right]
\notag \\
&=
2 \, \exp\left(
  -\tfrac{ B^2 } { 2 }
\right)
\left[
\cosh\left(
  B x
\right)
-1
+
\Theta
\right]
\notag \\
&=
\left.
2 \, \Theta \, \exp\left(
  -\frac{ B^2 } { 2 }
  +\frac{ B^2 x^2 } { 2 \, \Theta }
\right)
\middle/
\left[
  1 +
  \frac{(3 - \Theta) \, B^4 x^4 }
  { 24 \, \Theta^2 }
\right]
\right.,
\label{eq:DE_threeT_exact13}
\end{align}
%
where,
we have introduced
the dimensionless quantities
$x = \sqrt{a} \, E$,
and
$B = \beta / \sqrt{a}$.
And
$\Theta \equiv 1 + \frac{1}{2} \exp(B^2/2)$.



We now see that
Eq. \eqref{eq:DE_threeT_exact13}
is very similar to
Eq. \eqref{eq:DE_threeT_approx}.
%
$A_{13}$ is computed as follows
%
\begin{align}
A_{13}
&=
\sqrt{ \frac a {2 \, \pi} }
\int
  \frac{ \exp[-\frac 1 2 a E^2 - \beta^2/a ] }
  { D(E) } \, dE
\notag \\
&\approx
\frac {1 }{ \sqrt{2 \, \pi} }
\int
  \frac{ \exp[-\frac 1 2 x^2 - B^2 ] }
  {
    2 \, \Theta \, \exp\left(
      -\frac{ B^2 } { 2 }
      +\frac{ B^2 x^2 } { 2 \, \Theta }
    \right)
  }
  \left[
    1
    +
    \frac{ (3 - \Theta) \, B^4 x^4 }
    { 24 \, \Theta^2 }
  \right]
 \, d x
\notag \\
&=
\frac { \exp(-B^2/2) }
{ 2 \Theta \, \sqrt{2 \, \pi} }
\int
  \exp\left[
    -\frac 1 2
    \left(
      1 + \frac{ B^2 } { \Theta }
    \right)
    x^2
  \right]
  \left[
    1
    +
    \frac{ (3 - \Theta) \, B^4 x^4 }
    { 24 \, \Theta^2 }
  \right]
 \, d x
\notag \\
&=
\frac { \exp(-B^2/2) }
{ 2 \, \Theta \, \sqrt{1 + B^2/\Theta} }
  \left[
    1
    +
    \frac{ (3 - \Theta) \, B^4 }
    { 8 \, (\Theta + B^2)^2}
  \right].
\end{align}



\subsection{Continuous-temperature case}

\subsubsection{Characteristic equation}


The characteristic equation
in the continuous case is
%
\begin{equation}
\sum_{j = 1}^K
  c_j \, A_{ij}
= \lambda_i \, c_i.
\end{equation}
%
Using the recipe of
\[
\sum_{k = 1}^K
\rightarrow
K \int d\beta \, w(\beta),
\]
we get
\begin{equation}
K \int w(\beta') \, c_l(\beta') \, A(\beta, \beta') \, d\beta'
= \lambda_l \, c_l(\beta),
\tag{B9}
\label{eq:eig_continuous}
\end{equation}



\subsubsection{Continuous Gaussian temperature distribution}



For the Gaussian distribution
\begin{align}
w(\beta)
=
\frac{1}{\sqrt{2 \, \pi \, a'}}
\exp\left[
  -\frac{ (\beta - \beta_c)^2 }{ 2 \, a' }
\right],
\notag
%\label{eq:wbeta_Gaussian}
\end{align}
%
we have
%
\begin{align*}
D(E; \vct f^*)
&=
K \int d\beta \, w(\beta)
\exp\left[
  -\beta \Delta E -\frac{ \beta^2 }{ 2 \, a}
\right]
\\
&=
K \sqrt{\frac{a}{a+a'}}
\exp\left[
\frac{
  a \, a' \, \Delta E^2
  - 2 \, a \, \beta_c \, \Delta E
  - \beta_c^2
}
{
  2 (a + a')
}
\right],
\end{align*}
where $\Delta E = E - E_c$.
%
In the same spirit of Eq. \eqref{eq:Ec_twoT},
let us set
\begin{align}
E_c = \beta_c / a,
\label{eq:Ec_cont}
\end{align}
%
Then,
\begin{align*}
D(E; \vct f^*)
&=
K \sqrt{\frac{a}{a+a'}}
\exp\left[
\frac{
  a \, a' \, E^2
}
{
  2 (a + a')
}
-\beta_c \, E
+
\frac{\beta_c^2}{2 \, a}
\right],
\end{align*}


The $A_{ij}$ becomes,
%
%
%
\begin{align*}
A(\beta, \beta')
&=
\frac 1 K
\int
\sqrt \frac { a + a' } { 2 \, \pi }
\exp\left[
-\frac{ a \, \Delta E^2 } { 2 }
-(\beta + \beta') \, \Delta E
-
\frac{
  \beta^2 + \beta'^2
} { 2 \, a}
-
\frac{
  a \, a' \, E^2
}
{
  2 (a + a')
}
+\beta_c \, E
-
\frac{\beta_c^2}{2 \, a}
\right]
\, dE
\notag \\
%
%
%
&=
\frac 1 K
\int
\sqrt \frac { a + a' } { 2 \, \pi }
\exp\left[
-\frac{
  a \, E^2
}
{
  2
}
-(\beta + \beta') \, \Delta E
-
\frac{
  \beta^2 + \beta'^2
} { 2 \, a}
-
\frac{
  a \, a' \, E^2
}
{
  2 (a + a')
}
+2\beta_c \, E
-
\frac{\beta_c^2}{a}
\right]
\, dE
\notag \\
%
%
%
&=
\frac 1 K
\int
\sqrt \frac { a + a' } { 2 \, \pi }
\exp\left[
-\frac{
  a \, (a + 2a') \, E^2
}
{
  2 (a + a')
}
-(\Delta \beta + \Delta \beta') \, E
-
\frac{
  \Delta \beta^2 + \Delta \beta'^2
} { 2 \, a}
\right]
\, dE
\notag \\
%
%
%
&=
\frac{ a + a' } { K \, \sqrt{ a \, (a + 2 a')} }
\exp\left[
  \frac{
    (a+a') (\Delta \beta + \Delta \beta')^2
  }
  {
    2 a (a + 2 a')
  }
  -
  \frac{
    \Delta \beta^2 + \Delta \beta'^2
  } { 2 \, a}
\right]
\notag\\
%
%
%
&=
\frac{ a + a' } { K \, \sqrt{a \, ( a + 2 a') } }
\exp\left[
  \frac{
    -a' (\Delta \beta^2 + \Delta {\beta'}^2)
    +2(a+a') \Delta \beta \, \Delta \beta'
  }
  {
    2 a (a + 2 a')
  }
\right],
\end{align*}
where
$\Delta \beta = \beta - \beta_c$,
and
$\Delta \beta' = \beta' - \beta_c$.



Now characteristic equation,
Eq. \eqref{eq:eig_continuous},
can be written as
%
\begin{align}
\lambda_l \, c_l(\beta)
&=
\frac{ a + a' } { \sqrt{2 \, \pi \, a \, a' \, (a + 2 a')} }
\int
\exp\left[
  \frac{
    -a' (\Delta \beta^2 + \Delta \beta'^2)
    +2(a+a') \Delta \beta \, \Delta \beta'
  }
  {
    2 a (a + 2 a')
  }
  -\frac{
    \Delta \beta'^2
  } { 2 a'}
\right]
c_l(\beta') \, d\beta'
\notag
\\
&=
\frac{ a + a' } { \sqrt{2 \, \pi \, a \, a' \, (a + 2 a')} }
\int
\exp\left\{
  \frac{
    -[(a + a') \Delta \beta' - a' \Delta \beta]^2
  }
  {
    2 a \, a' (a + 2 a')
  }
\right\}
c_l(\beta') \, d\beta'.
%\notag
\label{eq:eig_Gaussian1}
\end{align}
%



\subsubsection{Solution of the characteristic equation}



We wish to show the solution of
Eq. \eqref{eq:eig_Gaussian1}
is
\begin{equation}
c_l(\beta) = H_l(\Delta\beta/\sqrt{2\,a'}).
\label{eq:cl_solution}
\end{equation}
%
We need the following result.
%
\begin{align}
\int
e^{
  -(A x - B y)^2
}
e^{
  -(A t)^2 + 2 (C x) (A t)
}
d x
=
\frac{ \sqrt \pi } { A }
e^{
  -(B t)^2 + 2 (C y) (B t)
},
\label{eq:convint1}
\end{align}
where,
$C = \pm\sqrt{A^2 - B^2}$.
%
To show this,
%
\begin{align*}
\mathrm{l.h.s.}
&=
\int
\exp\left[
  -(A x)^2 + 2 (A x) (B y + C t)
  -(B y)^2 -(A t)^2
\right]
d x
\notag \\
&=
\frac{ \sqrt \pi } { A }
\exp\left[
  (B y + C t)^2 - (B y)^2 - (A t)^2
\right]
\notag \\
&=
\frac{ \sqrt \pi }{ A }
\exp\left[
  2 B \, y \, C \, t - (B t)^2
\right]
=
\mathrm{r.h.s.}
.
\end{align*}



Now expand both sides of Eq. \eqref{eq:convint1}
as Hermite polynomials:
%
\begin{align*}
\exp\left[
  -(A t)^2 + 2 (C x) (A t)
\right]
&=
\sum_{l = 0}^\infty
  H_l(C x) \frac{ (A t)^l }{ l! }.
\\
\exp\left[
  -(B t)^2 + 2 (C y) (B t)
\right]
&=
\sum_{l = 0}^\infty
  H_l(C y) \frac{ (B t)^l }{ l! }.
\end{align*}
%
Thus
%
\begin{align}
\int
e^{
  -(A x - B y)^2
}
H_l(C x) \, d x
=
\frac{ \sqrt \pi } { A }
\left(
  \frac B A
\right)^l
H_l(C y),
\label{eq:convint1_Hermite}
\end{align}



In our case,
$A = (a + a')/\sqrt{2 a a' (a + 2 a')}$,
$B = a'/\sqrt{2 a a' (a + 2 a')}$,
$C = 1/\sqrt{2 a'}$,
$x = \beta$,
$y = \beta'$.
%
This shows that
Eq. \eqref{eq:cl_solution}
is indeed an solution,
and the eigenvalue is
\begin{equation}
\lambda_l
=
\left( \frac{ B }{ A } \right)^l
=
\left( \frac{ a' }{ a + a' } \right)^l.
\end{equation}





\subsubsection{Alternative solution of the characteristic equation}



There is at least another solution of
Eq. \eqref{eq:eig_Gaussian1}
is
\begin{equation}
c_l(\beta) =
H_l(i \, \Delta\beta/\sqrt{2\,a'})
\exp[
  \Delta \beta^2 / (2 \, a')
].
\label{eq:cl_solution2}
\end{equation}
%
We need the following result.
%
\begin{align}
\int
e^{
  -(A x - B y)^2
}
e^{
  (B t)^2 - 2 (C x) (B t) + (C x)^2
}
d x
=
\frac{ \sqrt \pi } { B }
e^{
  (A t)^2 - 2 (C y) (A t) + (C y)^2
},
\label{eq:convint2}
\end{align}
where,
$C = \pm\sqrt{A^2 - B^2}$.
%
To show this,
%
\begin{align*}
\mathrm{l.h.s.}
&=
\int
\exp\left[
  -(B x)^2 + 2 (B x) (A y + C t)
  -(B y)^2 + (B t)^2
\right]
d x
\notag \\
&=
\frac{ \sqrt \pi } { B }
\exp\left[
  (A y + C t)^2 - (B y)^2 + (B t)^2
\right]
\notag \\
&=
\frac{ \sqrt \pi }{ B }
\exp\left[
  (C y)^2 + 2 A \, y \, C \, t + (A t)^2
\right]
=
\mathrm{r.h.s.}
.
\end{align*}



Now expand both sides of Eq. \eqref{eq:convint1}
as Hermite polynomials:
%
\begin{align*}
\exp\left[
  (B t)^2 - 2 (C x) (B t)
\right]
&=
\sum_{l = 0}^\infty
  H_l(i \, C x) \frac{ (i B t)^l }{ l! }.
\\
\exp\left[
  (A t)^2 - 2 (C y) (A t)
\right]
&=
\sum_{l = 0}^\infty
  H_l(i \, C y) \frac{ (i A t)^l }{ l! }.
\end{align*}
%
Thus
%
\begin{align}
\int
e^{
  -(A x - B y)^2
}
H_l(i \, C x) \,
\exp[ (C x)^2 ] \, d x
=
\frac{ \sqrt \pi } { A }
\left(
  \frac A B
\right)^{l + 1}
H_l(C y) \,
\exp[ (C y)^2 ] \, d y,
\label{eq:convint2_Hermite}
\end{align}



This shows that
Eq. \eqref{eq:cl_solution2}
is also an solution,
and the eigenvalue is
\begin{equation}
\lambda_l
=
\left( \frac{ A }{ B } \right)^{l + 1}
=
\left( \frac{ a + a' }{ a' } \right)^{l + 1}.
\end{equation}
%
However,
since the eigenvalue is greater than $1.0$,
it is unlikely a physical solution.




\section{Appendix C: Approximate formulae}


\subsection{Summary of approximate formulae}

Here, we listed a few approximate formulae.
%
Since WHAM and related methods
are relatively expensive,
here we explore a few cheaper,
albeit less accurate, alternatives
for on-the-fly estimations\cite{park2007}.
%
Below is a summary of the results,
and detailed derivations are given in latter sections.








\subsubsection{Two-point average (TPA)}



For a pair of temperatures,
$\beta_i$ and $\beta_{i+1}$,
the free energy difference
can be approximated as\cite{park2007}
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i,
%\tag{C1}
\label{eq:df_eav}
\end{equation}
%
where
$\Delta A_i \equiv A_{i+1} - A_i$,
$\overline{ A }_i \equiv (A_{i+1} + A_i)/2$
for any quantity $A$,
and
$\langle\dots\rangle_i$
denotes an average in trajectory $i$.
%


\subsubsection{Euler-Maclaurin (EM) formula}



Equation \eqref{eq:df_eav} can be improved
by the Euler-Maclaurin formula\cite{
arfken, whittaker, wang_specfunc}
as
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
+
\Delta \langle \delta E^2 \rangle_i
\frac{ \Delta \beta_i^2 }{ 12 },
%\tag{C2}
\label{eq:df_eavb}
\end{equation}
where
$\langle \delta E^m \rangle_k
\equiv \langle (E - \langle E \rangle_k)^m \rangle_k$
for $k = i$ and $i + 1$,
and
$\Delta \langle \delta E^m \rangle_i
= \langle \delta E^m \rangle_{i + 1}
- \langle \delta E^m \rangle_{i}$.
%
More generally,
%
\begin{align*}
\Delta f_i
%&= \int_{\beta_i}^{\beta_{i+1}} f'(\beta) \, d\beta \\
&\approx
\frac{\Delta \beta_i}{2}
\left[
  f'(\beta_{i+1}) + f'(\beta_i)
\right]
\\
&\phantom{=}
-
\sum_{s = 1}^{S}
  B_{2s}
  \frac{ (\Delta \beta_i)^{2s} } { (2 s)! }
  \left[
    f^{(2s)}(\beta_{i+1})
    -
    f^{(2s)}(\beta_i)
  \right],
\end{align*}
%
where $B_{2s}$ are the Bernoulli numbers,
$B_2 = 1/6$, $B_4 = -1/30$, \ldots,
and $S = 1$.



\subsubsection{A variant of the Euler-Maclaurin (VEM) formula}



An alternative expansion is
%
\begin{align}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
-
\overline{ \langle \delta E^3 \rangle }_i
\, \frac{ \Delta \beta_i^3 } { 12 }
+ \cdots.
%\tag{C3}
\label{eq:df_eavc}
\end{align}
%
The general form is
\begin{align*}
\Delta f_i
&\approx
-\sum_{s = 1}^{S'}
  a_s
  \frac{ (\Delta \beta_i)^{2s - 1} } { (2 s - 1)! }
  \left[
    f^{(2s-1)}(\beta_{i+1})
    +
    f^{(2s-1)}(\beta_i)
  \right],
\end{align*}
where
$a_s = (1 - 4^s) B_{2s} /s$,
%
and
$a_1 = a_3 = -1/2$,
$a_2 = 1/4$,
\dots.
%
This variant can be similarly derived
by repeated partial integrations
(or by Darboux's formula\cite{
whittaker, wang_specfunc})
using Euler polynomials\cite{
wang_specfunc, abramowitz}
in place of Bernoulli ones.



\subsubsection{Euler-Maclaurin expansion in energy}


A variant of Eq. \eqref{eq:df_eavb},
using an expansion in energy $E$
instead of inverse temperature $\beta$,
is
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
-
\left(
  \frac 1 { \langle \delta E^2 \rangle }_{i+1}
  -
  \frac 1 { \langle \delta E^2 \rangle }_{i}
\right)
\frac{ \left( \Delta \langle E \rangle_i \right)^2 }{ 12 }.
%\tag{C2}
\label{eq:df_tgaus}
\end{equation}



\subsubsection{Gaussian partition (GP)}



In this method,
we first seek $\hat E$ such that
\begin{align}
  \frac{ (\hat E - \langle E \rangle_i)^2 }
  { 2 \, \langle \delta E^2 \rangle_i }
  +\frac 1 2
  \log
  \langle \delta E^2 \rangle_i
=
  \frac{ (\hat E - \langle E \rangle_{i+1})^2 }
  { 2 \, \langle \delta E^2 \rangle_{i+1} }
  +\frac 1 2
  \log
  \langle \delta E^2 \rangle_{i+1}.
  \label{eq:Ehat_gpart}
\end{align}
%
The free energy difference is given by
\begin{align}
  \Delta f_i = \Delta \beta_i \, \hat E.
  \label{eq:df_gpart}
\end{align}
%



\subsubsection{Explicit solution of WHAM/BAR equation}



If the WHAM or BAR equation
is applied to the two temperature,
$\Delta f_i$ satisfies
(assuming equal population)
%
\begin{equation*}
\left\langle
\frac{ 1 }
{ 1 + e^{-\Delta \beta_i E + \Delta f_i} }
\right\rangle_i
=
\left\langle
\frac{ 1 }
{ 1 + e^{\Delta \beta_i E - \Delta f_i} }
\right\rangle_{i+1}.
\end{equation*}
%
Expanding the solution
in powers of $\Delta \beta_i$
yields
\begin{align}
\Delta f_i
&
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
%
-\overline{ \langle \delta E^3 \rangle }_i
\, \frac{ \Delta \beta_i^3 } { 12 }
%
-\Delta \langle E \rangle_i
\, \Delta \langle \delta E^2 \rangle_i
\, \frac{ \Delta \beta_i^3 } { 16 }
\notag \\
&
\phantom{\approx}
%
-\overline{ \langle \delta E^2 \rangle }_i
\, \overline{ \langle \delta E^3 \rangle }_i
\, \frac{ \Delta \beta_i^5 } { 48 }
%
+\overline{ \langle \delta E^5 \rangle }_i
\, \frac{ \Delta \beta_i^5 } { 120 }
+ \cdots.
\notag
%\label{eq:df_eavbar}
\end{align}
%
The first two terms, up to $O(\Delta \beta_i^3)$,
are the same as those in Eq. \eqref{eq:df_eavc},
although higher-order terms differ.
%





\subsection{Euler-Maclaurin-like expansions}



\subsubsection{Bernoulli polynomials and numbers}



The Bernoulli numbers are defined as
%
\begin{equation}
  \frac{ t }
  {e^t - 1}
=
\sum_{n = 0}^\infty
  B_n \frac{ t^n } { n! }.
\label{eq:Bernoulli_number}
\end{equation}
%
The first few $B_n$ are\cite{
  whittaker, arfken, abramowitz, wang_specfunc}
$B_0 = 1$,
$B_1 = -1/2$,
$B_2 = 1/6$,
$B_4 = -1/30$,
$B_6 = 1/42$,
$B_8 = -1/30$,
\dots,
$B_3 = B_5 = B_7 = \dots = 0$.



\paragraph{Bernoulli polynomials}



The Bernoulli polynomials are defined as
%
\begin{equation}
  \frac{ t \, e^{x t} }
  {e^t - 1}
=
\sum_{n = 0}^\infty
  B_n(x) \frac{ t^n } { n! }.
\label{eq:Bernoulli_polynomial}
\end{equation}
%
The $x = 0$ case
recovers the Bernoulli numbers,
$B_n \equiv B_n(0)$.
%
The first few polynomials are\cite{
  whittaker, arfken, abramowitz, wang_specfunc}
\begin{align*}
B_0(x) &= 1, \\
B_1(x) &= x - \frac 1 2, \\
B_2(x) &= x^2 - x + \frac 1 6, \\
B_3(x) &= x^3 - \frac 3 2 x^2 + \frac 1 2 x, \\
B_4(x) &= x^4 - 2 \, x^3 + x^2 - \frac{1}{30}, \\
B_5(x) &= x^5 - \frac 5 2 x^4 + \frac 5 3 x^3 - \frac{1}{6} x, \\
B_6(x) &= x^6 - 3 \, x^5 + \frac 5 2 x^4 - \frac 1 2 x^2 + \frac{1}{42}, \\
B_7(x) &= x^7 - \frac 7 2 x^6 + \frac 7 2 \, x^5 - \frac 7 6 x^3 + \frac 1 6 x, \\
B_8(x) &= x^8 - 4 \, x^7 + \frac{14} 3 x^6 - \frac 7 3 \, x^4 + \frac 2 3 x^2 - \frac{1}{30}. \\
\end{align*}




\paragraph{Symmetry}



Bernoulli polynomials have the following symmetry.
%
\begin{equation*}
\sum_{n = 0}^\infty
B_n(1 - x) \, t^n/n!
=
\frac{ 2 \, t \, e^{(1 - x) \, t} }
{ e^t - 1 }
=
\frac{ 2 \, (-t) \, e^{ x \, (-t)} }
{ e^{-t} - 1 }
=
\sum_{n = 0}^\infty
B_n(x) (-t)^n / n!.
\end{equation*}
%
This shows
\[
B_n(1 - x) = (-)^n \, B_n(x).
\]
Particularly,
\[
B_n(1) = (-)^n \, B_n(0).
\]
and $B_1(1) = -B_1(0) = 1/2$, and
\begin{align}
B_{2s}(1) = B_{2s}(0).
\label{eq:Bernoulli1_even}
\end{align}



\paragraph{Derivative}




Differentiating Eq. \eqref{eq:Euler_polynomial}
with respect to $x$ yields
\[
\sum_{n = 0}^\infty \frac{ B'_n(x) \, t^{n} } { n! }.
=
\frac{ t^2 \, e^{x t} }
{ e^t - 1 }
=
\sum_{n = 0}^\infty \frac{ B_n(x) \, t^{n + 1} } { n! }.
\]
Comparing the coefficients of $t^n$ yields
\[
B'_n(x)
=
n \, B_{n-1}(x).
\]
and
\begin{equation}
  B^{(p)}_n(x)
=
\frac{n!}{(n-p)!} \, B_{n-p}(x).
\label{eq:dBernoulli_polynomial}
\end{equation}
Particularly,
$B^{(n)}_n(x) = n!$.



\subsubsection{Euler polynomials}



The Euler polynomials are similarly defined
%
\begin{equation}
  \frac{ 2 \, e^{x t} }
  {e^t + 1}
=
\sum_{n = 0}^\infty
  E_n(x) \frac{ t^n } { n! }.
\label{eq:Euler_polynomial}
\end{equation}
%
Again, we shall first study the $x = 0$ case.
%
\begin{equation}
  \frac{ 2 }
  {e^t + 1}
=
\sum_{n = 0}^\infty
  E_n(0) \frac{ t^n } { n! }.
\label{eq:Euler0}
\end{equation}



\paragraph{Evaluate $E_n(0)$}



Since
\[
\frac{ 2 } {e^t + 1} - 1
=
-\frac{ e^t - 1 } {e^t + 1}
=
-\tanh \frac t 2,
\]
is an odd function,
$E_{n}(0) = 0$
for $n = 2, 4, 6, \dots$.



To evaluate $E_n(0)$,
we multiply Eq. \eqref{eq:Euler0}
by
\[
e^t + 1
=
\sum_{m = 0}^\infty
  (1 + \delta_{m0}) \frac{ t^m }{ m! }.
\]
This yields
\[
E_n(0)
=
-\frac{1}{2}
\sum_{k = 0}^{n - 1}
{n \choose k} E_k(0).
\]
The first values are
$E_0(0) = 1$,
$E_1(0) = -1/2$,
$E_3(0) = 1/4$,
$E_5(0) = -1/2$,
$E_7(0) = 17/8$,
\dots,
%
and
%
\begin{equation}
\frac 2 {e^t + 1}
=
1 - \frac 1 2 t
+ \frac 1 4 \frac{ t^3 }{ 3! }
- \frac 1 2 \frac{ t^5 }{ 5! }
+ \frac{ 17 }{ 8 } \frac{ t^7 }{ 7! }
- \dots.
\label{eq:Euler0_first}
\end{equation}


The numbers can also be derived from the Bernoulli numbers.
%
Expanding both sides of
\[
t \times \frac{ 2 }{ e^t + 1}
=
2 \times \left(
  \frac{ t }{ e^t - 1 }
  -
  \frac{ 2 \, t } { e^{2 \, t} - 1 }
\right)
\]
as series of $t$ yields
\[
E_n(0)
=
\frac{ 2 \, B_{n + 1} } { n + 1 }
\left( 1 - 2^{n + 1} \right).
\]
%
For $n = 2 s - 1$,
\[
E_{2 s - 1}(0)
=
\frac{ B_{2 s} } { s }
\left( 1 - 4^s \right).
\]



\paragraph{First few Euler polynomials}




Multiplying Eq. \eqref{eq:Euler0_first}
by $e^{x t} = \sum_{m = 0}^\infty x^m \, t^m/m!$ yields
the first few Euler polynomials\cite{
abramowitz, wang_specfunc}
\begin{align*}
  E_0(x) &= 1, \\
  E_1(x) &= x -\frac 1 2, \\
  E_2(x) &= x^2 - x, \\
  E_3(x) &= x^3 - \frac 3 2 x^2 + \frac 1 4, \\
  E_4(x) &= x^4 - 2 \, x^3  + x, \\
  E_5(x) &= x^5 - \frac 5 2 x^4 + \frac 5 2 x^2 -\frac 1 2, \\
  E_6(x) &= x^6 - 3 \, x^5 + 5 \, x^3 - 3 \, x, \\
  E_7(x) &= x^7 - \frac 7 2 x^6 + \frac{35}{4} x^4 - \frac{21}{2} x^2 + \frac{ 17 } 8, \\
  E_8(x) &= x^8 - 4 \, x^7 + 14 \, x^5 - 28 \, x^3 + 17 \, x.
\end{align*}



\paragraph{Symmetry}



Euler polynomials have the following symmetry.
%
\begin{equation*}
\sum_{n = 0}^\infty
E_n(1 - x) \, t^n/n!
=
\frac{ 2 \, e^{(1 - x) \, t} }
{ e^t + 1 }
=
\frac{ 2 \, e^{ x \, (-t)} }
{ e^{-t} + 1 }
=
\sum_{n = 0}^\infty
E_n(x) (-t)^n / n!.
\end{equation*}
%
This shows
\[
E_n(1 - x) = (-)^n \, E_n(x).
\]
Particularly,
\[
E_n(1) = (-)^n \, E_n(0).
\]
and
\begin{equation}
E_{2s - 1}(1) = - \, E_{2s - 1}(0).
\label{eq:Euler1_odd}
\end{equation}



\paragraph{Derivative}




Differentiating Eq. \eqref{eq:Euler_polynomial}
with respect to $x$ yields
\[
\sum_{n = 0}^\infty \frac{ E'_n(x) \, t^{n} } { n! }.
=
\frac{ 2 \, t \, e^{x t} }
{ e^t + 1 }
=
\sum_{n = 0}^\infty \frac{ E_n(x) \, t^{n + 1} } { n! }.
\]
Comparing the coefficients of $t^n$ yields
\[
E'_n(x)
=
n \, E_{n-1}(x).
\]
and
\begin{equation}
  E^{(p)}_n(x)
=
\frac{n!}{(n-p)!} \, E_{n-p}(x).
\label{eq:dEuler_polynomial}
\end{equation}
Particularly,
$E^{(n)}_n(x) = n!$.




\subsubsection{Darboux's formula}



The Darboux's formula\cite{
whittaker, wang_specfunc}
is
%
\begin{align}
\psi^{(2n)}(0) [f(b) - f(a)]
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \psi^{(2n - m)}(1) \, f^{(m)}(b)
    -
    \psi^{(2n - m)}(0) \, f^{(m)}(a)
  \right]
\notag \\
&
+h^{2m + 1}
\int_0^1
  \psi(t) \, f^{(2n+1)}(a + t \, h) \, dt,
\label{eq:Darboux}
\end{align}
where
$\psi(x)$ is a polynomial of $2n$ degrees,
and $h = b - a$.



To show Eq. \eqref{eq:Darboux},
we start from
%
\begin{align*}
&\frac{ d } { dt }
\left[
  (-)^{m - 1} h^m \,
  \psi^{(2 n - m)}(t) \, f^{(m)}(a + t \, h)
\right]
\\
&=
(-)^{m-1} h^m \,
  \psi^{(2 n - m + 1)}(t) \, f^{(m)}(a + t \, h)
-
(-)^m h^{m + 1} \,
  \psi^{(2 n - m)}(t) \, f^{(m + 1)}(a + t \, h).
\end{align*}
%
Summing $m$ from $1$ to $2 n$ yields
%
\begin{align*}
&\sum_{m = 1}^{2 n}
\frac{ d } { dt }
  \left[
    (-)^{m - 1} h^m \,
    \psi^{(2 n - m)}(t) \, f^{(m)}(a + t \, h)
  \right]
\\
&=
h \,
  \psi^{(2 n)}(t) \, f^{(1)}(a + t \, h)
-
h^{2 n + 1} \,
  \psi^{(0)}(t) \, f^{(2 n + 1)}(a + t \, h)
\\
&=
h \,
  \psi^{(2 n)}(0) \, f'(a + t \, h)
-
h^{2 n + 1} \,
  \psi(t) \, f^{(2 n + 1)}(a + t \, h),
\end{align*}
%
where
we have used the fact that
$\psi(x)$ is a polynomial of degree $2n$,
so that
$\psi^{(2 n)}(t) = \psi^{(2 n)}(0)$.
%
Integrating $t$ from $0$ to $1$
yields Eq. \eqref{eq:Darboux}.



\paragraph{Using Bernoulli polynomials}



Using $\psi(t) = B_{2n}(t)/(2n)!$ in Eq. \eqref{eq:Darboux},
we get
\begin{align}
f(b) - f(a)
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \frac{ B_m(1) } {m!} \, f^{(m)}(b)
    -
    \frac{ B_m(0) } {m!} \, f^{(m)}(a)
  \right]
\notag \\
&=
\frac{ h \, \left[
    f'(b) + f'(a)
  \right]
} {2 }
-
\sum_{s = 1}^{n}
\, h^{2 s}
\frac{ B_{2 s} }{ (2 s)! }
    \, \left[f^{(2 s)}(b) - f^{(2 s)}(a)\right]
  + \dots
\notag \\
&=
\frac{ h \, \left[
    f'(b) + f'(a)
  \right]
} {2 }
-
\frac{ h^2 }{ 12 }
    \, \left[f''(b) - f''(a)\right]
  + \dots,
\label{eq:Darboux_Bernoulli}
\end{align}
where we have used
Eqs. \eqref{eq:dBernoulli_polynomial} and \eqref{eq:Bernoulli1_even}.
%
This is the Euler-Maclaurin formula\cite{
arfken, whittaker, wang_specfunc, abramowitz}.
%
%In the main text,
%we set $b = \beta_{i + 1}$,
%$a = \beta_i$.



\paragraph{Using Euler polynomials}



Using $\psi(t) = E_{2n}(t)/(2n)!$ in Eq. \eqref{eq:Darboux},
we get
\begin{align}
f(b) - f(a)
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \frac{ E_m(1) } {m!} \, f^{(m)}(b)
    -
    \frac{ E_m(0) } {m!} \, f^{(m)}(a)
  \right]
\notag \\
&=
-\sum_{s = 1}^{n}
h^{2 s - 1}
\frac{ E_{2 s - 1}(0) }{ (2 s - 1)! }
    \, \left[f^{(2 s - 1)}(b) + f^{(2 s - 1)}(a)\right]
  + \dots
\notag \\
&=
\frac{ h \, \left[
    f'(b) + f'(a)
  \right]
} {2 }
-
\frac{ h^2 }{ 12 }
  \, \frac{ f'''(b) + f'''(a) } { 2 }
  + \dots,
\label{eq:Darboux_Euler}
\end{align}
where we have used
Eqs. \eqref{eq:dEuler_polynomial} and \eqref{eq:Euler1_odd}.
%
%In the main text,
%we set $b = \beta_{i + 1}$,
%$a = \beta_i$,
%and $E_{2s - 1} \rightarrow a_s$.




\subsubsection{Difference from the ``Zwanzig'' expansion}



There is a similar expression,
so-called Zwanzig expression\cite{leach},
that appears to be different from
Eq. \eqref{eq:Darboux_Bernoulli}.
%
This expression essentially goes like this,
%
From the Taylor expansion
%
\begin{align*}
f(b) - f(a)
=
f'(a) \, h
+
f''(a) \, \frac{ h^2 } { 2 }
+
f'''(a) \, \frac{ h^3 } { 6 }
+
\dots
\end{align*}
%
and
\begin{align*}
f(b) - f(a)
=
f'(b) \, h
-
f''(b) \, \frac{ h^2 } { 2 }
+
f'''(b) \, \frac{ h^3 } { 6 }
-
\dots
\end{align*}
%
Now averaging the two expressions yield
%
\begin{align}
f(b) - f(a)
=
h \frac{ f'(a) + f'(b) } { 2 }
-
\frac{ h^2}{ 4 } [ f''(b) - f''(a) ]
+
\frac{ h^3 } { 6 } \frac{ f'''(a) + f'''(b) } { 2 }
+
\dots
\label{eq:zwanzig}
\end{align}
%
Now the coefficient of $h^2$ is $-1/4$
instead of $-1/12$,
which appears to be a paradox.



This problem is due to the inefficiency of
Eq. \eqref{eq:zwanzig}.
%
For one thing,
we see that the second and third term
on the right-hand side of Eq. \eqref{eq:zwanzig}
are of the same order of magnitude.
%
Note particularly that the second term is difference,
while the third is an average.
%


In comparison,
neither Eq. \eqref{eq:Darboux_Bernoulli}
nor \eqref{eq:Darboux_Euler}
suffers from this problem.
%
So the correct coefficient should be $-1/12$.
%
In fact,
if we assume to the leading order
\[
f''(b) - f''(a)
\approx
\frac{ h \, [f'''(a) + f'''(b)] } { 2 },
\]
then we indeed recover the factor
$-1/4 + 1/6 = -1/12$.



\subsection{Euler-Maclaurin expansion in energy}



The Euler-Maclaurin expansion can be applied to
the independent variable $E$.
%
First, by partial integration, we have
%
\begin{align}
\Delta f_{21}
=
\int_{\beta_1}^{\beta_2} E \, d\beta
=
E \, \beta \Big|_1^2
-
\int_{E_1}^{E_2} \beta \, dE
\label{eq:EEM1}
\end{align}
%
Now applying
Eq. \eqref{eq:Darboux_Bernoulli}
to the second term
yields
\begin{align}
\int_{E_1}^{E_2}
  \beta \, dE
&=
\frac{ \Delta E \, \left[
    \beta_1 + \beta_2
  \right]
} {2 }
-
\sum_{s = 1}^{n}
\, {\Delta E}^{2 s}
\frac{ B_{2 s} }{ (2 s)! }
\, \left[\beta^{(2 s - 1)}(E_2) - \beta^{(2 s)}(E_1)\right]
  + \dots.
\end{align}
%
Substitute this in Eq. \eqref{eq:EEM1}
yields
\begin{align}
\Delta f_{21}
=
\frac{
    E_1 + E_2
} {2 }
\Delta \beta
+
\sum_{s = 1}^{n}
\, {\Delta E}^{2 s}
\frac{ B_{2 s} }{ (2 s)! }
\, \left[\beta^{(2 s - 1)}(E_2) - \beta^{(2 s)}(E_1)\right]
  + \dots.
\end{align}
%
For the first term in the sum,
$s = 1$, $B_{2s} = 1/6$,
and $\beta'(E) = -\langle \delta E^2 \rangle^{-1}$.
%
\begin{align}
\Delta f_{21}
=
\frac{
    E_1 + E_2
} {2 }
\Delta \beta
-
\frac{ {\Delta E}^{2} }{ 12 }
\left[
  \langle \delta E^2 \rangle_2^{-1}
-
  \langle \delta E^2 \rangle_1^{-1}
\right]
+ \dots.
\notag
\end{align}
which is Eq. \eqref{eq:df_tgaus}.




\subsubsection{Derivation from density of states}



Below is an old derivation from
the perspective of density of states.


To show this,
we assume that at temperature $\beta_0$,
the distribution can be written as
%
\begin{equation}
\log \rho_0
=
\mathrm{const.}
-\frac{1}{2} a_2 (E - E_0)^2
-\frac{1}{3} a_3 (E - E_0)^3
-\frac{1}{4} a_4 (E - E_0)^4
-\cdots
\end{equation}
%
We also assume that
$a_3$ and $a_4$ are small,
so that the center of the distribution is still $E_0$.



Now at another temperature $\beta$,
the distribution
$\rho \propto \rho_0 \, \exp[-(\beta - \beta_0) \, E]$.
%
We can thus write,
in terms of $\Delta \beta = \beta - \beta_0$,
$\Delta E = E - E_0$ that (dropping the constant term)
%
\begin{align*}
\log \rho
&=
-\Delta \beta \, E
-\frac{1}{2} a_2 \, \Delta E^2
-\frac{1}{3} a_3 \, \Delta E^3
-\frac{1}{4} a_3 \, \Delta E^4
\end{align*}
%
The center of the energy distribution
satisfies the condition
$\partial \log \rho/\partial E = 0$.
%
\begin{equation}
  \Delta \beta
+a_2 \, \Delta E
+a_3 \, \Delta E^2
+a_4 \, \Delta E^3
= 0
\label{eq:TG_betaE}
\end{equation}



Consider the free energy difference of two temperatures,
%
\begin{align}
\Delta f_{21}
&=
\int_{ \beta_1 }^{ \beta_2 } E \, d \Delta \beta
\notag \\
&=
\left.
  E \, \Delta \beta
\right|_{\beta_1}^{\beta_2}
-
\int_{E_1}^{E_2}
  \Delta \beta \, dE
\notag \\
&=
\left.
  E \, \Delta \beta
\right|_{\beta_1}^{\beta_2}
+
\int_{\Delta E_1}^{\Delta E_2}
  (a_2 \, \Delta E + a_3 \, \Delta E^2 + a_4 \, \Delta E^3) \, d\Delta E
\notag \\
&=
\left.
\left(
  E \, \Delta \beta
  +
  a_2 \, \Delta E^2/2 + a_3 \, \Delta E^3/3 + a_4 \, \Delta E^4/4
\right)
\right|_{\beta_1}^{\beta_2}
\notag \\
&=
\left.
\left(
  E \, \Delta \beta
  + a_2 \, \Delta E^2/2 + a_3 \, \Delta E^3/3 + a_4 \, \Delta E^4/4
\right)
\right|_{\beta_1}^{\beta_2}
\notag \\
&=
E_2 \, \Delta \beta_2 - E_1 \, \Delta \beta_1
+
(E_2 - E_1) \,
\big[
  a_2 \, ( \Delta E_2 + \Delta E_1) / 2
\notag\\
&\phantom{=}
  + a_3 \, ( \Delta E_2^2 + \Delta E_2 \Delta E_1 + \Delta E_1^2 )/3
\notag \\
&\phantom{=}
  + a_4 \, ( \Delta E_2^3 + \Delta E_2^2 \Delta E_1 + \Delta E_2 \Delta E_1^2 + \Delta E_1^3 ) / 4
\big].
\label{eq:TG_dfstep1}
\end{align}



Now using Eq. \eqref{eq:TG_betaE} for $\beta_1$ and $\beta_2$,
in the above equation we can eliminate $a_2$ and get
%
\begin{align*}
\Delta f_{21}
&=
\frac { (E_2 + E_1) }{ 2 } \, (\beta_2 - \beta_1)
-
\frac{ 1 } { 12 }
(E_2 - E_1)^3 \,
\left[
  2 \, a_3
  +3 \, a_4 \, ( \Delta E_2 + \Delta E_1 )
\right].
\end{align*}



The width of the distribution is the inverse
of the following
%
\begin{align*}
\langle \delta E^2 \rangle^{-1}
&=
\frac{ \partial^2 \log \rho } { d E^2 }
\\
&=
a_2 + 2 a_3 \Delta E + 3 a_4 \Delta E^2.
\end{align*}
%
Using this formula for $\beta_1$ and $\beta_2$,
and computing the difference yields
%
\begin{align*}
\langle \delta E^2 \rangle_2^{-1}
-
\langle \delta E^2 \rangle_1^{-1}
&=
(E_2 - E_1) [ 2 a_3  + 3 a_4 (\Delta E_1 + \Delta E_2) ].
\end{align*}



Using this result in Eq. \eqref{eq:TG_dfstep1} yields,
\begin{equation*}
\Delta f_{21}
=
\frac{ E_2 + E_1 } { 2 } (\beta_2 - \beta_1)
-
\left(
  \langle \delta E^2 \rangle_2^{-1}
  -
  \langle \delta E^2 \rangle_1^{-1}
\right)
\frac{ (E_2 - E_1)^2 }{ 12 }.
\end{equation*}
%
This is Eq. \eqref{eq:df_tgaus}.



\subsubsection{Gaussian partition (GP)}



Another possible approach is seek the point, $\hat E$,
where the two distribution heights are the same:
%
\begin{align*}
\rho_i(\hat E)
=
\rho_{i+1}(\hat E),
\end{align*}
%
or
\begin{align*}
g(\hat E) \, \exp(-\beta_i \, \hat E + f_i)
=
g(\hat E) \, \exp(-\beta_{i+1} \, \hat E + f_{i+1}).
\end{align*}
%
Then, we have Eq. \eqref{eq:df_gpart}.
%
So far,
this result is exact.
%
If $\rho_i(E)$ and $\rho_{i+1}(E)$
are approximated as Gaussian distributions,
we get Eq. \eqref{eq:Ehat_gpart}.




\subsection{Solution to the BAR equation}



For two temperatures, $\beta_0$ and $\beta_1$,
the BAR or WHAM equation reads
%
\begin{equation*}
\int
  \frac{ \rho_0(E) + \rho_1(E) }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
  \, d E
= 1,
\end{equation*}
%
where $\Delta \beta = \beta_1 - \beta_0$,
and
$\Delta f = f_1 - f_0$.
%
It can be rewritten as
%
\begin{equation*}
\int
  \frac{ \rho_0(E) }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
  \, d E
=
\int
  \frac{ \rho_1(E) }
  { 1 + \exp( \Delta \beta \, E - \Delta f ) }
  \, d E,
\end{equation*}
%
where
$\Delta \beta = \beta_1 - \beta_0$,
and
$\Delta f = f_1 - f_0$.
%
In terms of averages,
\begin{equation}
\left\langle
  \frac{ 2 }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
\right\rangle_0
=
\left\langle
  \frac{ 2 }
  { 1 + \exp( \Delta \beta \, E - \Delta f ) }
\right\rangle_1
\label{eq:bar}
\end{equation}
%
We expand $\Delta f$ in terms of powers of $\Delta \beta$:
\[
\Delta f
=
\sum_{n = 1}^\infty f_n \, \Delta \beta^n.
\]



For convenience,
we define
\[
\phi_n \equiv (f_n - \delta_{n,1} \Delta E) \, \Delta \beta^n
\]
and
\[
\phi = \sum_{n = 1}^\infty \phi_n.
\]
Thus, Eq. \eqref{eq:bar} can be written as
\begin{align*}
\left\langle
  \frac{ 2 }
  { 1 + \exp \phi  }
\right\rangle_0
=
\left\langle
  \frac{ 2 }
  { 1 + \exp -\phi }
\right\rangle_1
\label{eq:bar1}
\end{align*}


We now expand the left-hand side as
%
\begin{align*}
\left\langle
  \frac{ 2 }
  { 1 + \exp \phi  }
\right\rangle_0
&=
\left\langle
\sum_{n = 0}^\infty
E_n(0) \frac{ \phi^n } { n! }
\right\rangle_0
\notag \\
&=
\sum_{n = 0}^\infty
\sum_{n_1 + n_2 + \dots = n}
E_n(0)
\frac{ \langle \phi_1^{n_1} \rangle_0 \,
      \phi_2^{n_2} \, \cdots } { n_1! \, n_2! \, \cdots }.
\end{align*}
%
Note that only $\phi_1$ depends on $E$,
so the averaging only applies to $\phi_1$ and its powers.
%
The right-hand side
can be similarly expanded
with substitutions
$\phi_k \rightarrow -\phi_k$,
and
$\langle \dots \rangle_0 \rightarrow \langle \dots \rangle_1$.



We collect terms proportional $\Delta \beta^M$
and let $M$ increase gradually from 1.
%
Since $\phi_k \propto \Delta \beta^k$,
we are collecting terms with
\begin{equation}
n_1 + 2 \, n_2 + 3 \, n_3 + \dots = M,
\label{eq:sumn_M}
\end{equation}
Also,
for $n \ge 1$,
$E_n(0) = 0$ if $n$ is even,
thus $n$ has to be odd.



\subsubsection{$M = 1$}



For $M = 1$,
we have $n_1 = 1$,
\[
\langle \phi_1 \rangle_0
=
-\langle \phi_1 \rangle_1.
\]
or
\[
f_1 =
\frac{
  \langle E \rangle_0
  +
  \langle E \rangle_1
}{ 2 }.
\]



\subsubsection{$M$ is even}



For $M = 2$,
we have only the term $n_1 = 0, n_2 = 1$,
because the term $n_1 = 2, n_2 = 0$
makes $n = n_1 + n_2$ even,
and $E_n(0) = 0$.
%
Thus,
\[
\phi_2
=
-\phi_2,
\]
or $\phi_2 = 0$.



Generally,
for any even $M$,
$\phi_M = 0$.
%
To see that, we carry an induction on even $M$
%
If $M$ is even,
we find from Eq. \eqref{eq:sumn_M} that
$n_1 + n_3 + \cdots + n_{M - 1}$ must be even.
But
$n = n_1 + n_3 + \cdots + n_{M - 1} + n_M$
must be odd
(note that the induction hypothesis asserts
that $n_2 = n_4 = \cdots = n_{M-2} = 0$).
%
Thus, $n_M$ must be odd,
and to satisfy $\sum_{k = 1}^M k n_k = M$,
we have $n_1 = \dots = n_{M-1} = 0$,
and $n_M = 1$.
%
Thus,
%
\[
E_1(0) \, \phi_M
=
-E_1(0) \, \phi_M,
\]
which shows that $\phi_M = 0$.



\subsubsection{$M = 3$}



There are two relevant terms
$n_3 = 1$,
and $n_1 = 3$.
%
\[
E_1(0) \, \phi_3
+
E_3(0) \, \frac{ \langle \phi_1^3 \rangle_0 } { 3! }
=
-E_1(0) \, \phi_3
-
E_3(0) \, \frac{ \langle \phi_1^3 \rangle_1 }{ 3! }
\]
and
\[
\phi_3
=
\frac{
\langle \phi_1^3 \rangle_0
+
\langle \phi_1^3 \rangle_1
}{24}.
\]
%
Let
$c = \left( \langle E \rangle_1 - \langle E \rangle_0 \right) / 2$,
For $\beta_0$,
$\phi_1 = \Delta \beta \, (c - \delta E)$,
where
$\delta E = E - \langle E \rangle_0$.
%
For $\beta_1$,
$\phi_1 = \Delta \beta \, (-c -\delta E)$
where
$\delta E = E - \langle E \rangle_1$.
%
So
\begin{align*}
\phi_3
&=
-\left(
\frac{
\langle \delta E^3 \rangle_0
+
\langle \delta E^3 \rangle_1
}{24}
+
\frac{
  \left(
    \langle E \rangle_1
    -
    \langle E \rangle_0
  \right)
  \left(
    \langle \delta E^2 \rangle_1
    -
    \langle \delta E^2 \rangle_0
  \right)
}{16}
\right)
\Delta \beta^3
\notag \\
&=
-\left(
\frac{
  \overline{ \langle \delta E^3 \rangle }
}{12}
+
\frac{
  \Delta \langle E \rangle
  \,
  \Delta \langle \delta E^2 \rangle
}{16}
\right) \Delta \beta^3.
\end{align*}




\subsubsection{$M = 5$}



The relevant terms are
%
\begin{enumerate}
\item $n_5 = 1$.
\item $n_3 = 1$, $n_1 = 2$ (so that $n = 3$).
\item $n_1 = 5$ (so that $n = 5$).
\end{enumerate}
%
So
\begin{equation*}
E_1(0) \, \phi_5
+E_3(0) \, \phi_3 \, \frac{ \langle \phi_1^2 \rangle_0 } { 2! }
+E_5(0) \, \frac{ \langle \phi_1^5 \rangle_0 } { 5! }
=
-E_1(0) \, \phi_5
-E_3(0) \, \phi_3 \, \frac{ \langle \phi_1^2 \rangle_1 } { 2! }
-E_5(0) \, \frac{ \langle \phi_1^5 \rangle_1 } { 5! },
\end{equation*}
and
\begin{align*}
\phi_5
&=
\phi_3 \, \frac{ \langle \phi_1^2 \rangle_0 + \langle \phi_1^2 \rangle_1 } { 8 }
-\frac{ \langle \phi_1^5 \rangle_0 + \langle \phi_1^5 \rangle_1 } { 240 }
\\
&=
\phi_3 \, \frac{ \overline{ \langle \phi_1^2 \rangle } } { 4 }
-\frac{ \overline{ \langle \phi_1^5 \rangle } } { 120 }
\end{align*}
%
For $\beta_0$,
$\phi_1 = \Delta \beta \, (c -\delta E)$,
so
$\langle \phi_1^2 \rangle_0 = \Delta \beta^2 \left( c^2 + \langle \delta E^2 \rangle_0 \right)$
and
$\overline{ \langle \phi_1^2 \rangle } = \Delta \beta^2 \left( c^2 + \overline{ \langle \delta E^2 \rangle } \right)$.


Next
\begin{align*}
\langle \phi_1^5 \rangle_0
=
\Delta \beta^5
\left(
c^5 + 6 \, c^3 \langle \delta E^2 \rangle_0
-6 \, c^2 \langle \delta E^3 \rangle_0
+4 \, c \langle \delta E^4 \rangle_0
-\langle \delta E^5 \rangle_0
\right).
\end{align*}
So
\begin{align*}
\overline{ \langle \phi_1^5 \rangle }
=
\Delta \beta^5
\left(
c^5
- 3 \, c^3 \Delta \langle \delta E^2 \rangle
-6 \, c^2 \overline{ \langle \delta E^3 \rangle }
-2 \, c \Delta \langle \delta E^4 \rangle
-\overline{ \langle \delta E^5 \rangle }
\right).
\end{align*}
In sum
\begin{align*}
\phi_5
&=
\Delta \beta^5
\left[
  -\left(
    \frac{ \overline{ \langle \delta E^3 \rangle } }
         { 12 }
    +
    \frac{ c \, \Delta \langle \delta E^2 \rangle }
         { 8 }
  \right)
\, \frac{ c^2 + \overline{ \langle \delta E^2 \rangle } }
        { 4 }
-\frac{
  c^5
  - 3 \, c^3 \Delta \langle \delta E^2 \rangle
  -6 \, c^2 \overline{ \langle \delta E^3 \rangle }
  -2 \, c \Delta \langle \delta E^4 \rangle
  -\overline{ \langle \delta E^5 \rangle }
} { 120 }
\right] \\
&=
\Delta \beta^5
\left[
  -\frac{ \overline{ \langle \delta E^3 \rangle }
          \overline{ \langle \delta E^2 \rangle } }
        { 48 }
+\frac{
  \overline{ \langle \delta E^5 \rangle }
} { 120 }
\right] + \cdots.
\end{align*}
%
Note that the terms involving $c$ or $\Delta \dots$
belong to higher orders.



\subsection{Comparison}


We compared the above equations
%Eqs. \eqref{eq:df_eav}, \eqref{eq:df_eavb}, and \eqref{eq:df_eavc}
with UIM\cite{kastner2005, *kastner2009}
(which is also an approximate method)
on the $32\times 32$ Ising model,
%
using the exact solution\cite{
ferdinand1969}
(since we wish to study
the accuracy, not precision).
%
In Fig. \ref{fig:is2approx}(a),
we show that
Eqs. \eqref{eq:df_eavb},
\eqref{eq:df_eavc},
and UIM
produced more accurate results than
Eq. \eqref{eq:df_eav}.
%
With $\Delta T = 0.2$,
the error of Eq. \eqref{eq:df_eav}
was up to $O(1)$.
%
The accuracy of
Eqs. \eqref{eq:df_eavb}, \eqref{eq:df_eavc} and \eqref{eq:df_tgaus}
increased with decreasing temperature spacing $\Delta T$,
whereas
UIM failed to improve.
%
However, for a small spacing,
Eqs. \eqref{eq:df_eavb}, \eqref{eq:df_eavc} and \eqref{eq:df_tgaus}
were superior.
%
This suggests that with ample data,
UIM should be replaced
by a more accurate alternative, e.g., ST-WHAM or WHAM.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2approx.pdf}
  }
  \caption{
    \label{fig:is2approx}
    (a) and (b) Errors of the dimensionless free energies, $f_i$,
    from Eqs. \eqref{eq:df_eav}, \eqref{eq:df_eavb}, \eqref{eq:df_eavc},
    \eqref{eq:df_tgaus},
    and UIM for the $32\times 32$ Ising model,
    $\Delta T = 0.05$ and $0.2$, respectively.
    %
    The temperatures $T_i$ are uniformly distributed
    in the range $[1.5, 3.1]$.
    %
    (c) Energy histograms at $\Delta T = 0.2$.
    %
    $f_1$ at the lowest temperature $T = 1.5$ is fixed at zero.
    %
    Lines are to guide the eyes.
  }
\end{figure}





\subsection{Use approximate formulae in simulated tempering}



Finally, we give a cautionary comment
on using Eq. \eqref{eq:df_eav}
for the weight in simulated tempering\cite{park2007}.
%
Equation \eqref{eq:df_eavb} shows that
the error of Eq. \eqref{eq:df_eav}
is of order $N \, \Delta \beta_i^3$
of a system of size $N$,
since
$\partial \langle \delta E^2 \rangle / \partial \beta
= -\partial^2 \langle E \rangle / \partial \beta^2$
is extensive.
%
Thus,
if Eq. \eqref{eq:df_eav} is applied to
an $O(1)$ temperature range
with an average spacing $\Delta \beta$,
the accumulative error of $f_i$
is $O(N \, \Delta \beta^2)$.
%
If the simulation temperatures
are arranged
such that the energy distributions
of neighboring temperatures
barely overlap,
%
the average energy difference,
$\Delta \langle E \rangle_i = \langle E \rangle_{i+1} - \langle E \rangle_i$,
between neighboring temperatures
is roughly a multiple of the width of
the energy distribution
$\sqrt{ \langle \delta E^2 \rangle_i } \propto O(\sqrt{N})$.
%
So $\Delta \beta_i$,
given by $\Delta \langle E \rangle_i / |\partial E/\partial \beta|$,
is $O(1/\sqrt{N})$,
%
and the accumulative error
of $f_i$ is $O(1)$.
%(as illustrated in Fig. \ref{fig:is2approx}).
%
Thus, if Eq. \eqref{eq:df_eav}
is used for the weights
in simulated tempering\cite{park2007},
the resulting temperature distribution
may deviate appreciably
from the flat one.
%
One can fix this by choosing
a finer temperature spacing or
use Eq. \eqref{eq:df_eavb}
instead.



\bibliography{simul}
\end{document}
