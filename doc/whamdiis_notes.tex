%\documentclass[reprint,superscriptaddress]{revtex4-1}
\documentclass[aip,jcp,preprint,notitlepage, superscriptaddress]{revtex4-1}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{color}
\begin{document}




\renewcommand{\theequation}{N.\arabic{equation}}

\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}




\title{Notes on WHAM, MBAR and DIIS}

\maketitle


\tableofcontents



\section{Methods: WHAM}



\subsection{Mnemonic derivation of WHAM}



We give a mnemonic derivation of the expression
for the density of states
%
\begin{equation}
g(E)
=
\frac{
  \sum_{k = 1}^K n_k(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
}.
\label{eq:gE_WHAM}
\end{equation}
%

In ensemble $k$,
the normalized energy distribution,
$n_k(E) / N_k$,
is related to the density of states as
%
\begin{equation}
\frac{ n_k(E) } { N_k }
=
g(E) \,
\frac{ \exp(-\beta_k E) } { Z_k }.
\end{equation}
%
Thus,
we have
%
\begin{equation}
g(E)
=
\frac{ n_k(E) }
     { d_k(E) },
\label{eq:gE_single}
\end{equation}
where
$d_k(E) \equiv N_k \exp(-\beta_k E) / Z_k$.
%
Averaging the numerators and denominators of different $k$'s
yields Eq. \eqref{eq:gE_WHAM}.



\subsection{Derivation of Eq. (6) from Eqs. (4) and (5)}



From Eq. (4)
\begin{align*}
f_i
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K n_j(E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K \sum_{\vct x}^{(j)}
      \delta(\E(\vx) - E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
  \int
    \frac{
      \exp(-\beta_j E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    \delta(\E(\vx) - E) \, dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
    \frac{
      \exp(-\beta_i \, \E(\vx))
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k \, \E(\vx) + f_k)
    },
\end{align*}
which is Eq. (6).



\subsection{Derivation of Eq. (7) from Eq. (6)}



From Eq. (6)
\begin{align*}
f_i
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[-\Ham_0(\vx) - \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[-\Ham_0(\vx) - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\delta(\W(\vx) - W) \, dW
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W)
  \exp f_k
}
\delta(\W(\vx) - W)
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  \left[
    \sum_{\vx}^{(j)}
    \delta(\W(\vx) - W)
  \right]
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  n_j(W)
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW,
\end{align*}
which is Eq. (7).




\subsection{Statistical-temperature WHAM}



Taking the logarithm of Eq. \eqref{eq:gE_WHAM} and differentiating,
we get
\begin{equation}
(\log g)'(E)
=
\frac{ \sum_{k = 1}^K n'_k(E) }
     { \sum_{k = 1}^K n_k(E) }
+
\frac{ \sum_{k = 1}^K d_k(E) \, \beta_k }
     { \sum_{k = 1}^K d_k(E) }.
\label{eq:beta0_STWHAM}
\end{equation}
%
By Eq. \eqref{eq:gE_single},
we have
$n_k(E) = d_k(E) \, g(E)$,
which shows that
$d_k(E)$
is proportional to $n_k(E)$
for a fixed $g(E)$.
%
So we can rewrite Eq. \eqref{eq:beta0_STWHAM} as
%
\begin{equation}
(\log g)'(E)
=
\sum_{k = 1}^K c_k(E) \, \left[ (\log n_k)'(E)  + \beta_k \right],
\label{eq:beta_STWHAM}
\end{equation}
%
where
\begin{align}
  c_k(E) \equiv \frac{ n_k(E)  }{ \sum_{l = 1}^{K} n_l(E) },
  \label{eq:ck}
\end{align}
%
This is the main result of ST-WHAM.





\subsection{W-DF method}



The W-DF method by Shen and McCammon\cite{
shen1991, roux1995}
used a similar weighting strategy.
%
\begin{align}
g(E) = \sum_{k = 1}^K c_k(E) \, \frac{ n_k(E) } { d_k(E) },
\end{align}
%
where $c_k(E)$ is given by Eq. \eqref{eq:ck}.
%
Note that
if $c_k(E)$
is replaced by $ d_k(E) / \sum_{l = 1}^K d_l(E)$,
we recover WHAM.



\subsection{W-PMF method}



Another variant is the W-DF method by Woolf and Roux\cite{
woolf1994, crouzy1994, roux1995}.
%
Here,
%
\begin{align}
\log g(E) = \sum_{k = 1}^K c_k(E) \, \log \frac{ n_k(E) } { d_k(E) },
\end{align}
%
where $c_k(E)$ is given by Eq. \eqref{eq:ck}.
%







\section{Methods: DIIS}


\subsection{Determination of coefficients in DIIS}



The coefficients $c_i$ are determined as follows.
%
We wish to minimize
%
\[
S
=
\vct{\hat R}^2 / 2
=
\frac 1 2 \left( \sum_i c_i \, \vct R_i \right)^2.
\]
under the constraint
%
\begin{equation}
\sum_i c_i = 1
\label{eq:c_normalize}
\end{equation}



Thus we shall minimize the function
\[
  S - \lambda \sum_i c_i
\]
with respect all $c_i$.
%
Thus, we get
\begin{equation}
  \sum_j c_j \, (\vct R_j \cdot \vct R_i) - \lambda = 0,
  \label{eq:cj_DIIS}
\end{equation}


Equations \eqref{eq:cj_DIIS} and \eqref{eq:c_normalize}
can be written in matrix form
%
\begin{equation}
\left(
  \begin{array}{ccccc}
    \vct R_1 \cdot \vct R_1 &
    \vct R_1 \cdot \vct R_2 &
    \dots &
    \vct R_1 \cdot \vct R_M &
    -1 \\
    \vct R_2 \cdot \vct R_1 &
    \vct R_2 \cdot \vct R_2 &
    \dots &
    \vct R_2 \cdot \vct R_M &
    -1 \\
    \vdots &
    \vdots &
    &
    \vdots &
    -1 \\
    \vct R_M \cdot \vct R_1 &
    \vct R_M \cdot \vct R_2 &
    \dots &
    \vct R_M \cdot \vct R_M &
    -1 \\
    1 &
    1 &
    \dots &
    1 &
    0
  \end{array}
\right)
\left(
  \begin{array}{c}
    c_1 \\
    c_2 \\
    \vdots \\
    c_M \\
    \lambda
  \end{array}
\right)
=
\left(
  \begin{array}{c}
    0 \\
    0 \\
    \vdots \\
    0 \\
    1
  \end{array}
\right).
\end{equation}





\subsection{Parameters for Figure 1}



The illustration was produced according to the following potential
%
\begin{equation}
F
=
\frac{1}{2} A \, ( x - x_0 )^2
+
\frac{1}{2} B \, ( y - y_0 )^2
+
C \, ( x - x_0 ) ( y - y_0 ),
\label{eq:fig1_F}
\end{equation}
%
where
$x_0 = 3/5$,
$y_0 = 4/5$,
$A = 3/4$,
$B = 5/4$,
and
$C = 1/4$.



The two vectors are
$\vct f_1 = (1, 0)$
and
$\vct f_2 = (0, 1)$.
%
The residual vectors are
$\vct R_1 = (-1/10, 9/10)$
and
$\vct R_2 = (4/10, -1/10)$,
respectively.



We now determine
$\vct{\hat R} = \vct R_1 + \lambda \, (\vct R_2 - \vct R_1)$
with the minimal magnitude
$\| \vct{\hat R} \|$.
%
It is readily shown that
$\lambda$ satisfies
%
\[
\lambda
=
-\frac{
  \Delta \vct R \cdot \vct R_1
}
{
  \Delta \vct R \cdot \Delta \vct R
}
=
-\frac{-19/20}{5/4}
=
\frac{19}{25},
\]
%
where
$\Delta \vct R = \vct R_2 - \vct R_1 = (1/2, -1)$.
%
Thus,
\[
\vct{\hat R}
=
(1 - \lambda) \, \vct R_1
+ \lambda \, \vct R_2
=
\left(
  \frac{7}{25},
  \frac{7}{50}
\right),
\]
\[
\vct{\hat f}
=
(1 - \lambda) \, \vct f_1
+ \lambda \, \vct f_2
=
\left(
  \frac{6}{25},
  \frac{19}{25}
\right),
\]
and
\[
\vct{\hat f}
+
\vct{\hat R}
=
\left(
  \frac{13}{25},
  \frac{9}{10}
\right).
\]



We can complete squares of $F$ as
%
\begin{align*}
F
&=
\frac{1}{16}
\left\{
  \left( 5 + 3 \sqrt 2 \right)
  \left[
    \left( \sqrt 2 - 1 \right) \, \bar x
  +
    \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{1}{16}
\left\{
  \left( 3 - \sqrt 2 \right)
  \left[
    \bar x
  +
    \left( \sqrt 2 + 1 \right) \, \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{7}{2} R^2,
\end{align*}
%
where
%
\begin{align*}
\bar x
&= x - x_0
\\
&=
\left(
  \sqrt{ 5 - 3 \sqrt{2} } \, \cos \theta
  +
  \sqrt{ 5 + 3 \sqrt{2} } \, \sin \theta
\right) R
\\
&=
\left[
  \left( \sqrt 2 - 1 \right) \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  +
  \left( \sqrt 2 + 1 \right) \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right] R,
\end{align*}
%
and
%
\begin{align*}
\bar y
&= y - y_0
\\
&=
\left(
  \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  -
  \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right) R,
\end{align*}



The short axis is achieved at $\theta = 0$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 - \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(\sqrt 2 - 1\right) = \pi/8$.



The long axis is achieved at $\theta = \pi/2$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 + \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(-\sqrt 2 - 1\right) = -3 \, \pi/8$.




\section{Appendix A: Probabilistic derivation of the MBAR equation}



From the unnumbered equation after Eq. (A1),
we get the posterior probability
\begin{align*}
p\left( g | \{ \vct x \} \right)
&=
\frac{
  p\left( \{ \vct x \} | g \right) \, p(g)
}
{
  p\left( \{ \vct x \} \right)
}
\\
&\propto
p\left( \{ \vct x \} | g \right).
\end{align*}
%
Taking the logarithm, we get
%
\begin{align*}
\log p\left( g | \{ \vct x \} \right)
&=
\sum_{k = 1}^K
  \sum_{\vct x}^{(k)}
    \left(
      \log g(\vx) + \log q_k(\vx) - \log Z_k[g]
    \right)
+ C
\\
&=
\sum_{k = 1}^K
\left(
  \sum_{\vx}^{(k)}
    \log g(\vx) - N_k \log Z_k[g]
\right) + C',
\end{align*}
%
where $C$ and $C'$
are constants independent of $g$.
%
We now differentiate it with respect to $g(\vy)$,
%
\begin{align}
\frac{
  \delta \log p\left( g | \{ \vct x \} \right)
}
{
  \delta g(\vy)
}
&=
\sum_{k = 1}^K
\left(
  \sum_{\vct x}^{(k)}
  \frac{ \delta( \vy - \vx ) }
  { g(\vy) }
  -
  \frac {N_k} { Z_k[g] }
  \frac{ \delta Z_k[g] } { \delta g(\vy) }
\right)
= 0.
\label{eq:dpdg}
\end{align}
%
Now since
\begin{equation}
Z_k[g]
=
\int g(\vy) \, q_k(\vy) \, d\vy,
\tag{A1}
\label{eq:Zg}
\end{equation}
we have
\begin{equation*}
\frac{ \delta Z_k[g] } { \delta g(\vy) }
= q_k(\vy).
\end{equation*}
%
Using this in Eq. \eqref{eq:dpdg},
we get
\begin{equation}
g(\vy)
=
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
}.
\tag{A2}
\label{eq:gy}
\end{equation}
%
Finally,
we use Eq. \eqref{eq:gy} in Eq. \eqref{eq:Zg},
%
\begin{align}
Z_i[g]
&=
\int
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
}
q_i(\vy) d\vy
\notag \\
&=
\sum_{j = 1}^K \sum_{\vct x}^{(j)}
\int
\frac{
  \delta(\vx - \vy)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vy) / Z_k[g]
}
q_i(\vy) d\vy
\notag \\
&=
\sum_{j = 1}^K \sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct x) / Z_k[g]
}.
\end{align}




\section{Appendix C: Approximate formulae}


Here, we listed a few approximate formulae.
%
Since WHAM and related methods
are relatively expensive,
here we explore a few cheaper,
albeit less accurate, alternatives
for on-the-fly estimations\cite{park2007}.
%
For a pair of temperatures,
$\beta_i$ and $\beta_{i+1}$,
the free energy difference
can be approximated as\cite{park2007}
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i,
%\tag{C1}
\label{eq:df_eav}
\end{equation}
%
where
$\Delta A_i \equiv A_{i+1} - A_i$,
$\overline{ A }_i \equiv (A_{i+1} + A_i)/2$
for any quantity $A$,
and
$\langle\dots\rangle_i$
denotes an average in trajectory $i$.
%
This formula can be improved
by the Euler-Maclaurin formula\cite{
arfken, *whittaker, *wang_specfunc}
as
%
\begin{equation}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
+
\Delta \langle \delta E^2 \rangle_i
\frac{ \Delta \beta_i^2 }{ 12 },
%\tag{C2}
\label{eq:df_eavb}
\end{equation}
where
$\langle \delta E^m \rangle_k
\equiv \langle (E - \langle E \rangle_k)^m \rangle_k$
for $k = i$ and $i + 1$,
and
$\Delta \langle \delta E^m \rangle_i
= \langle \delta E^m \rangle_{i + 1}
- \langle \delta E^m \rangle_{i}$.
%
More generally,
%
\begin{align*}
\Delta f_i
%&= \int_{\beta_i}^{\beta_{i+1}} f'(\beta) \, d\beta \\
&\approx
\frac{\Delta \beta_i}{2}
\left[
  f'(\beta_{i+1}) + f'(\beta_i)
\right]
\\
&\phantom{=}
-
\sum_{s = 1}^{S}
  B_{2s}
  \frac{ (\Delta \beta_i)^{2s} } { (2 s)! }
  \left[
    f^{(2s)}(\beta_{i+1})
    -
    f^{(2s)}(\beta_i)
  \right],
\end{align*}
%
where $B_{2s}$ are the Bernoulli numbers,
$B_2 = 1/6$, $B_4 = -1/30$, \ldots,
and $S = 1$.



An alternative expansion is
%
\begin{align}
\Delta f_i
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
-
\overline{ \langle \delta E^3 \rangle }_i
\, \frac{ \Delta \beta_i^3 } { 12 }
+ \cdots.
%\tag{C3}
\label{eq:df_eavc}
\end{align}
%
The general form is
\begin{align*}
\Delta f_i
&\approx
-\sum_{s = 1}^{S'}
  a_s
  \frac{ (\Delta \beta_i)^{2s - 1} } { (2 s - 1)! }
  \left[
    f^{(2s-1)}(\beta_{i+1})
    +
    f^{(2s-1)}(\beta_i)
  \right],
\end{align*}
where
$a_s = (1 - 4^s) B_{2s} /s$,
%
and
$a_1 = a_3 = -1/2$,
$a_2 = 1/4$,
\dots.
%
This variant can be similarly derived
by repeated partial integrations
(or by Darboux's formula\cite{
whittaker, wang_specfunc})
using Euler polynomials\cite{
wang_specfunc, abramowitz}
in place of Bernoulli ones.




If the WHAM or BAR equation
is applied to the two temperature,
$\Delta f_i$ satisfies
(assuming equal population)
%
\begin{equation*}
\left\langle
\frac{ 1 }
{ 1 + e^{-\Delta \beta_i E + \Delta f_i} }
\right\rangle_i
=
\left\langle
\frac{ 1 }
{ 1 + e^{\Delta \beta_i E - \Delta f_i} }
\right\rangle_{i+1}.
\end{equation*}
%
Expanding the solution
in powers of $\Delta \beta_i$
yields
\begin{align}
\Delta f_i
&
\approx
\overline{ \langle E \rangle }_i \, \Delta \beta_i
%
-\overline{ \langle \delta E^3 \rangle }_i
\, \frac{ \Delta \beta_i^3 } { 12 }
%
-\Delta \langle E \rangle_i
\, \Delta \langle \delta E^2 \rangle_i
\, \frac{ \Delta \beta_i^3 } { 16 }
\notag \\
&
\phantom{\approx}
%
-\overline{ \langle \delta E^2 \rangle }_i
\, \overline{ \langle \delta E^3 \rangle }_i
\, \frac{ \Delta \beta_i^5 } { 48 }
%
+\overline{ \langle \delta E^5 \rangle }_i
\, \frac{ \Delta \beta_i^5 } { 120 }
+ \cdots.
\notag
%\label{eq:df_eavbar}
\end{align}
%
The first two terms, up to $O(\Delta \beta_i^3)$,
are the same as those in Eq. \eqref{eq:df_eavc},
although higher-order terms differ.
%





%
Below, we show how to obtain them from systematic expansions.







\subsection{Bernoulli polynomials and numbers}



The Bernoulli numbers are defined as
%
\begin{equation}
  \frac{ t }
  {e^t - 1}
=
\sum_{n = 0}^\infty
  B_n \frac{ t^n } { n! }.
\label{eq:Bernoulli_number}
\end{equation}
%
The first few $B_n$ are\cite{
  whittaker, arfken, abramowitz, wang_specfunc}
$B_0 = 1$,
$B_1 = -1/2$,
$B_2 = 1/6$,
$B_4 = -1/30$,
$B_6 = 1/42$,
$B_8 = -1/30$,
\dots,
$B_3 = B_5 = B_7 = \dots = 0$.



\subsubsection{Bernoulli polynomials}



The Bernoulli polynomials are defined as
%
\begin{equation}
  \frac{ t \, e^{x t} }
  {e^t - 1}
=
\sum_{n = 0}^\infty
  B_n(x) \frac{ t^n } { n! }.
\label{eq:Bernoulli_polynomial}
\end{equation}
%
The $x = 0$ case
recovers the Bernoulli numbers,
$B_n \equiv B_n(0)$.
%
The first few polynomials are\cite{
  whittaker, arfken, abramowitz, wang_specfunc}
\begin{align*}
B_0(x) &= 1, \\
B_1(x) &= x - \frac 1 2, \\
B_2(x) &= x^2 - x + \frac 1 6, \\
B_3(x) &= x^3 - \frac 3 2 x^2 + \frac 1 2 x, \\
B_4(x) &= x^4 - 2 \, x^3 + x^2 - \frac{1}{30}, \\
B_5(x) &= x^5 - \frac 5 2 x^4 + \frac 5 3 x^3 - \frac{1}{6} x, \\
B_6(x) &= x^6 - 3 \, x^5 + \frac 5 2 x^4 - \frac 1 2 x^2 + \frac{1}{42}, \\
B_7(x) &= x^7 - \frac 7 2 x^6 + \frac 7 2 \, x^5 - \frac 7 6 x^3 + \frac 1 6 x, \\
B_8(x) &= x^8 - 4 \, x^7 + \frac{14} 3 x^6 - \frac 7 3 \, x^4 + \frac 2 3 x^2 - \frac{1}{30}. \\
\end{align*}




\subsubsection{Symmetry}



Bernoulli polynomials have the following symmetry.
%
\begin{equation*}
\sum_{n = 0}^\infty
B_n(1 - x) \, t^n/n!
=
\frac{ 2 \, t \, e^{(1 - x) \, t} }
{ e^t - 1 }
=
\frac{ 2 \, (-t) \, e^{ x \, (-t)} }
{ e^{-t} - 1 }
=
\sum_{n = 0}^\infty
B_n(x) (-t)^n / n!.
\end{equation*}
%
This shows
\[
B_n(1 - x) = (-)^n \, B_n(x).
\]
Particularly,
\[
B_n(1) = (-)^n \, B_n(0).
\]
and $B_1(1) = -B_1(0) = 1/2$, and
\begin{align}
B_{2s}(1) = B_{2s}(0).
\label{eq:Bernoulli1_even}
\end{align}



\subsubsection{Derivative}




Differentiating Eq. \eqref{eq:Euler_polynomial}
with respect to $x$ yields
\[
\sum_{n = 0}^\infty \frac{ B'_n(x) \, t^{n} } { n! }.
=
\frac{ t^2 \, e^{x t} }
{ e^t - 1 }
=
\sum_{n = 0}^\infty \frac{ B_n(x) \, t^{n + 1} } { n! }.
\]
Comparing the coefficients of $t^n$ yields
\[
B'_n(x)
=
n \, B_{n-1}(x).
\]
and
\begin{equation}
  B^{(p)}_n(x)
=
\frac{n!}{(n-p)!} \, B_{n-p}(x).
\label{eq:dBernoulli_polynomial}
\end{equation}
Particularly,
$B^{(n)}_n(x) = n!$.



\subsection{Euler polynomials}



The Euler polynomials are similarly defined
%
\begin{equation}
  \frac{ 2 \, e^{x t} }
  {e^t + 1}
=
\sum_{n = 0}^\infty
  E_n(x) \frac{ t^n } { n! }.
\label{eq:Euler_polynomial}
\end{equation}
%
Again, we shall first study the $x = 0$ case.
%
\begin{equation}
  \frac{ 2 }
  {e^t + 1}
=
\sum_{n = 0}^\infty
  E_n(0) \frac{ t^n } { n! }.
\label{eq:Euler0}
\end{equation}



\subsubsection{Evaluate $E_n(0)$}



Since
\[
\frac{ 2 } {e^t + 1} - 1
=
-\frac{ e^t - 1 } {e^t + 1}
=
-\tanh \frac t 2,
\]
is an odd function,
$E_{n}(0) = 0$
for $n = 2, 4, 6, \dots$.



To evaluate $E_n(0)$,
we multiply Eq. \eqref{eq:Euler0}
by
\[
e^t + 1
=
\sum_{m = 0}^\infty
  (1 + \delta_{m0}) \frac{ t^m }{ m! }.
\]
This yields
\[
E_n(0)
=
-\frac{1}{2}
\sum_{k = 0}^{n - 1}
{n \choose k} E_k(0).
\]
The first values are
$E_0(0) = 1$,
$E_1(0) = -1/2$,
$E_3(0) = 1/4$,
$E_5(0) = -1/2$,
$E_7(0) = 17/8$,
\dots,
%
and
%
\begin{equation}
\frac 2 {e^t + 1}
=
1 - \frac 1 2 t
+ \frac 1 4 \frac{ t^3 }{ 3! }
- \frac 1 2 \frac{ t^5 }{ 5! }
+ \frac{ 17 }{ 8 } \frac{ t^7 }{ 7! }
- \dots.
\label{eq:Euler0_first}
\end{equation}


The numbers can also be derived from the Bernoulli numbers.
%
Expanding both sides of
\[
t \times \frac{ 2 }{ e^t + 1}
=
2 \times \left(
  \frac{ t }{ e^t - 1 }
  -
  \frac{ 2 \, t } { e^{2 \, t} - 1 }
\right)
\]
as series of $t$ yields
\[
E_n(0)
=
\frac{ 2 \, B_{n + 1} } { n + 1 }
\left( 1 - 2^{n + 1} \right).
\]
%
For $n = 2 s - 1$,
\[
E_{2 s - 1}(0)
=
\frac{ B_{2 s} } { s }
\left( 1 - 4^s \right).
\]



\subsubsection{First few Euler polynomials}




Multiplying Eq. \eqref{eq:Euler0_first}
by $e^{x t} = \sum_{m = 0}^\infty x^m \, t^m/m!$ yields
the first few Euler polynomials\cite{
abramowitz, wang_specfunc}
\begin{align*}
  E_0(x) &= 1, \\
  E_1(x) &= x -\frac 1 2, \\
  E_2(x) &= x^2 - x, \\
  E_3(x) &= x^3 - \frac 3 2 x^2 + \frac 1 4, \\
  E_4(x) &= x^4 - 2 \, x^3  + x, \\
  E_5(x) &= x^5 - \frac 5 2 x^4 + \frac 5 2 x^2 -\frac 1 2, \\
  E_6(x) &= x^6 - 3 \, x^5 + 5 \, x^3 - 3 \, x, \\
  E_7(x) &= x^7 - \frac 7 2 x^6 + \frac{35}{4} x^4 - \frac{21}{2} x^2 + \frac{ 17 } 8, \\
  E_8(x) &= x^8 - 4 \, x^7 + 14 \, x^5 - 28 \, x^3 + 17 \, x.
\end{align*}



\subsubsection{Symmetry}



Euler polynomials have the following symmetry.
%
\begin{equation*}
\sum_{n = 0}^\infty
E_n(1 - x) \, t^n/n!
=
\frac{ 2 \, e^{(1 - x) \, t} }
{ e^t + 1 }
=
\frac{ 2 \, e^{ x \, (-t)} }
{ e^{-t} + 1 }
=
\sum_{n = 0}^\infty
E_n(x) (-t)^n / n!.
\end{equation*}
%
This shows
\[
E_n(1 - x) = (-)^n \, E_n(x).
\]
Particularly,
\[
E_n(1) = (-)^n \, E_n(0).
\]
and
\begin{equation}
E_{2s - 1}(1) = - \, E_{2s - 1}(0).
\label{eq:Euler1_odd}
\end{equation}



\subsubsection{Derivative}




Differentiating Eq. \eqref{eq:Euler_polynomial}
with respect to $x$ yields
\[
\sum_{n = 0}^\infty \frac{ E'_n(x) \, t^{n} } { n! }.
=
\frac{ 2 \, t \, e^{x t} }
{ e^t + 1 }
=
\sum_{n = 0}^\infty \frac{ E_n(x) \, t^{n + 1} } { n! }.
\]
Comparing the coefficients of $t^n$ yields
\[
E'_n(x)
=
n \, E_{n-1}(x).
\]
and
\begin{equation}
  E^{(p)}_n(x)
=
\frac{n!}{(n-p)!} \, E_{n-p}(x).
\label{eq:dEuler_polynomial}
\end{equation}
Particularly,
$E^{(n)}_n(x) = n!$.




\subsection{Darboux's formula}



The Darboux's formula\cite{
whittaker, wang_specfunc}
is
%
\begin{align}
\psi^{(2n)}(0) [f(b) - f(a)]
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \psi^{(2n - m)}(1) \, f^{(m)}(b)
    -
    \psi^{(2n - m)}(0) \, f^{(m)}(a)
  \right]
\notag \\
&
+h^{2m + 1}
\int_0^1
  \psi(t) \, f^{(2n+1)}(a + t \, h) \, dt,
\label{eq:Darboux}
\end{align}
where
$\psi(x)$ is a polynomial of $2n$ degrees,
and $h = b - a$.



To show Eq. \eqref{eq:Darboux},
we start from
%
\begin{align*}
&\frac{ d } { dt }
\left[
  (-)^{m - 1} h^m \,
  \psi^{(2 n - m)}(t) \, f^{(m)}(a + t \, h)
\right]
\\
&=
(-)^{m-1} h^m \,
  \psi^{(2 n - m + 1)}(t) \, f^{(m)}(a + t \, h)
-
(-)^m h^{m + 1} \,
  \psi^{(2 n - m)}(t) \, f^{(m + 1)}(a + t \, h).
\end{align*}
%
Summing $m$ from $1$ to $2 n$ yields
%
\begin{align*}
&\sum_{m = 1}^{2 n}
\frac{ d } { dt }
  \left[
    (-)^{m - 1} h^m \,
    \psi^{(2 n - m)}(t) \, f^{(m)}(a + t \, h)
  \right]
\\
&=
h \,
  \psi^{(2 n)}(t) \, f^{(1)}(a + t \, h)
-
h^{2 n + 1} \,
  \psi^{(0)}(t) \, f^{(2 n + 1)}(a + t \, h)
\\
&=
h \,
  \psi^{(2 n)}(0) \, f'(a + t \, h)
-
h^{2 n + 1} \,
  \psi(t) \, f^{(2 n + 1)}(a + t \, h),
\end{align*}
%
where
we have used the fact that
$\psi(x)$ is a polynomial of degree $2n$,
so that
$\psi^{(2 n)}(t) = \psi^{(2 n)}(0)$.
%
Integrating $t$ from $0$ to $1$
yields Eq. \eqref{eq:Darboux}.



\subsubsection{Using Bernoulli polynomials}



Using $\psi(t) = B_{2n}(t)/(2n)!$ in Eq. \eqref{eq:Darboux},
we get
\begin{align}
f(b) - f(a)
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \frac{ B_m(1) } {m!} \, f^{(m)}(b)
    -
    \frac{ B_m(0) } {m!} \, f^{(m)}(a)
  \right]
\notag \\
&=
\frac{ h \, \left[
    f'(b) + f'(a)
  \right]
} {2 }
-
\sum_{s = 1}^{n}
\, h^{2 s}
\frac{ B_{2 s} }{ (2 s)! }
    \, \left[f^{(2 s)}(b) - f^{(2 s)}(a)\right]
  + \dots,
\label{eq:Darboux_Bernoulli}
\end{align}
where we have used
Eqs. \eqref{eq:dBernoulli_polynomial} and \eqref{eq:Bernoulli1_even}.
%
This is the Euler-Maclaurin formula\cite{
arfken, whittaker, wang_specfunc, abramowitz}.
%
%In the main text,
%we set $b = \beta_{i + 1}$,
%$a = \beta_i$.



\subsubsection{Using Euler polynomials}



Using $\psi(t) = E_{2n}(t)/(2n)!$ in Eq. \eqref{eq:Darboux},
we get
\begin{align}
f(b) - f(a)
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \frac{ E_m(1) } {m!} \, f^{(m)}(b)
    -
    \frac{ E_m(0) } {m!} \, f^{(m)}(a)
  \right]
\notag \\
&=
-\sum_{s = 1}^{n}
h^{2 s - 1}
\frac{ E_{2 s - 1}(0) }{ (2 s - 1)! }
    \, \left[f^{(2 s - 1)}(b) + f^{(2 s - 1)}(a)\right]
  + \dots,
\label{eq:Darboux_Euler}
\end{align}
where we have used
Eqs. \eqref{eq:dEuler_polynomial} and \eqref{eq:Euler1_odd}.
%
%In the main text,
%we set $b = \beta_{i + 1}$,
%$a = \beta_i$,
%and $E_{2s - 1} \rightarrow a_s$.



\subsection{Solution to the BAR equation}



For two temperatures, $\beta_0$ and $\beta_1$,
the BAR or WHAM equation reads
%
\begin{equation*}
\int
  \frac{ \rho_0(E) + \rho_1(E) }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
  \, d E
= 1,
\end{equation*}
%
where $\Delta \beta = \beta_1 - \beta_0$,
and
$\Delta f = f_1 - f_0$.
%
It can be rewritten as
%
\begin{equation*}
\int
  \frac{ \rho_0(E) }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
  \, d E
=
\int
  \frac{ \rho_1(E) }
  { 1 + \exp( \Delta \beta \, E - \Delta f ) }
  \, d E,
\end{equation*}
%
where
$\Delta \beta = \beta_1 - \beta_0$,
and
$\Delta f = f_1 - f_0$.
%
In terms of averages,
\begin{equation}
\left\langle
  \frac{ 2 }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
\right\rangle_0
=
\left\langle
  \frac{ 2 }
  { 1 + \exp( \Delta \beta \, E - \Delta f ) }
\right\rangle_1
\label{eq:bar}
\end{equation}
%
We expand $\Delta f$ in terms of powers of $\Delta \beta$:
\[
\Delta f
=
\sum_{n = 1}^\infty f_n \, \Delta \beta^n.
\]



For convenience,
we define
\[
\phi_n \equiv (f_n - \delta_{n,1} \Delta E) \, \Delta \beta^n
\]
and
\[
\phi = \sum_{n = 1}^\infty \phi_n.
\]
Thus, Eq. \eqref{eq:bar} can be written as
\begin{align*}
\left\langle
  \frac{ 2 }
  { 1 + \exp \phi  }
\right\rangle_0
=
\left\langle
  \frac{ 2 }
  { 1 + \exp -\phi }
\right\rangle_1
\label{eq:bar1}
\end{align*}


We now expand the left-hand side as
%
\begin{align*}
\left\langle
  \frac{ 2 }
  { 1 + \exp \phi  }
\right\rangle_0
&=
\left\langle
\sum_{n = 0}^\infty
E_n(0) \frac{ \phi^n } { n! }
\right\rangle_0
\notag \\
&=
\sum_{n = 0}^\infty
\sum_{n_1 + n_2 + \dots = n}
E_n(0)
\frac{ \langle \phi_1^{n_1} \rangle_0 \,
      \phi_2^{n_2} \, \cdots } { n_1! \, n_2! \, \cdots }.
\end{align*}
%
Note that only $\phi_1$ depends on $E$,
so the averaging only applies to $\phi_1$ and its powers.
%
The right-hand side
can be similarly expanded
with substitutions
$\phi_k \rightarrow -\phi_k$,
and
$\langle \dots \rangle_0 \rightarrow \langle \dots \rangle_1$.



We collect terms proportional $\Delta \beta^M$
and let $M$ increase gradually from 1.
%
Since $\phi_k \propto \Delta \beta^k$,
we are collecting terms with
\begin{equation}
n_1 + 2 \, n_2 + 3 \, n_3 + \dots = M,
\label{eq:sumn_M}
\end{equation}
Also,
for $n \ge 1$,
$E_n(0) = 0$ if $n$ is even,
thus $n$ has to be odd.



\subsubsection{$M = 1$}



For $M = 1$,
we have $n_1 = 1$,
\[
\langle \phi_1 \rangle_0
=
-\langle \phi_1 \rangle_1.
\]
or
\[
f_1 =
\frac{
  \langle E \rangle_0
  +
  \langle E \rangle_1
}{ 2 }.
\]



\subsubsection{$M$ is even}



For $M = 2$,
we have only the term $n_1 = 0, n_2 = 1$,
because the term $n_1 = 2, n_2 = 0$
makes $n = n_1 + n_2$ even,
and $E_n(0) = 0$.
%
Thus,
\[
\phi_2
=
-\phi_2,
\]
or $\phi_2 = 0$.



Generally,
for any even $M$,
$\phi_M = 0$.
%
To see that, we carry an induction on even $M$
%
If $M$ is even,
we find from Eq. \eqref{eq:sumn_M} that
$n_1 + n_3 + \cdots + n_{M - 1}$ must be even.
But
$n = n_1 + n_3 + \cdots + n_{M - 1} + n_M$
must be odd
(note that the induction hypothesis asserts
that $n_2 = n_4 = \cdots = n_{M-2} = 0$).
%
Thus, $n_M$ must be odd,
and to satisfy $\sum_{k = 1}^M k n_k = M$,
we have $n_1 = \dots = n_{M-1} = 0$,
and $n_M = 1$.
%
Thus,
%
\[
E_1(0) \, \phi_M
=
-E_1(0) \, \phi_M,
\]
which shows that $\phi_M = 0$.



\subsubsection{$M = 3$}



There are two relevant terms
$n_3 = 1$,
and $n_1 = 3$.
%
\[
E_1(0) \, \phi_3
+
E_3(0) \, \frac{ \langle \phi_1^3 \rangle_0 } { 3! }
=
-E_1(0) \, \phi_3
-
E_3(0) \, \frac{ \langle \phi_1^3 \rangle_1 }{ 3! }
\]
and
\[
\phi_3
=
\frac{
\langle \phi_1^3 \rangle_0
+
\langle \phi_1^3 \rangle_1
}{24}.
\]
%
Let
$c = \left( \langle E \rangle_1 - \langle E \rangle_0 \right) / 2$,
For $\beta_0$,
$\phi_1 = \Delta \beta \, (c - \delta E)$,
where
$\delta E = E - \langle E \rangle_0$.
%
For $\beta_1$,
$\phi_1 = \Delta \beta \, (-c -\delta E)$
where
$\delta E = E - \langle E \rangle_1$.
%
So
\begin{align*}
\phi_3
&=
-\left(
\frac{
\langle \delta E^3 \rangle_0
+
\langle \delta E^3 \rangle_1
}{24}
+
\frac{
  \left(
    \langle E \rangle_1
    -
    \langle E \rangle_0
  \right)
  \left(
    \langle \delta E^2 \rangle_1
    -
    \langle \delta E^2 \rangle_0
  \right)
}{16}
\right)
\Delta \beta^3
\notag \\
&=
-\left(
\frac{
  \overline{ \langle \delta E^3 \rangle }
}{12}
+
\frac{
  \Delta \langle E \rangle
  \,
  \Delta \langle \delta E^2 \rangle
}{16}
\right) \Delta \beta^3.
\end{align*}




\subsubsection{$M = 5$}



The relevant terms are
%
\begin{enumerate}
\item $n_5 = 1$.
\item $n_3 = 1$, $n_1 = 2$ (so that $n = 3$).
\item $n_1 = 5$ (so that $n = 5$).
\end{enumerate}
%
So
\begin{equation*}
E_1(0) \, \phi_5
+E_3(0) \, \phi_3 \, \frac{ \langle \phi_1^2 \rangle_0 } { 2! }
+E_5(0) \, \frac{ \langle \phi_1^5 \rangle_0 } { 5! }
=
-E_1(0) \, \phi_5
-E_3(0) \, \phi_3 \, \frac{ \langle \phi_1^2 \rangle_1 } { 2! }
-E_5(0) \, \frac{ \langle \phi_1^5 \rangle_1 } { 5! },
\end{equation*}
and
\begin{align*}
\phi_5
&=
\phi_3 \, \frac{ \langle \phi_1^2 \rangle_0 + \langle \phi_1^2 \rangle_1 } { 8 }
-\frac{ \langle \phi_1^5 \rangle_0 + \langle \phi_1^5 \rangle_1 } { 240 }
\\
&=
\phi_3 \, \frac{ \overline{ \langle \phi_1^2 \rangle } } { 4 }
-\frac{ \overline{ \langle \phi_1^5 \rangle } } { 120 }
\end{align*}
%
For $\beta_0$,
$\phi_1 = \Delta \beta \, (c -\delta E)$,
so
$\langle \phi_1^2 \rangle_0 = \Delta \beta^2 \left( c^2 + \langle \delta E^2 \rangle_0 \right)$
and
$\overline{ \langle \phi_1^2 \rangle } = \Delta \beta^2 \left( c^2 + \overline{ \langle \delta E^2 \rangle } \right)$.


Next
\begin{align*}
\langle \phi_1^5 \rangle_0
=
\Delta \beta^5
\left(
c^5 + 6 \, c^3 \langle \delta E^2 \rangle_0
-6 \, c^2 \langle \delta E^3 \rangle_0
+4 \, c \langle \delta E^4 \rangle_0
-\langle \delta E^5 \rangle_0
\right).
\end{align*}
So
\begin{align*}
\overline{ \langle \phi_1^5 \rangle }
=
\Delta \beta^5
\left(
c^5
- 3 \, c^3 \Delta \langle \delta E^2 \rangle
-6 \, c^2 \overline{ \langle \delta E^3 \rangle }
-2 \, c \Delta \langle \delta E^4 \rangle
-\overline{ \langle \delta E^5 \rangle }
\right).
\end{align*}
In sum
\begin{align*}
\phi_5
&=
\Delta \beta^5
\left[
  -\left(
    \frac{ \overline{ \langle \delta E^3 \rangle } }
         { 12 }
    +
    \frac{ c \, \Delta \langle \delta E^2 \rangle }
         { 8 }
  \right)
\, \frac{ c^2 + \overline{ \langle \delta E^2 \rangle } }
        { 4 }
-\frac{
  c^5
  - 3 \, c^3 \Delta \langle \delta E^2 \rangle
  -6 \, c^2 \overline{ \langle \delta E^3 \rangle }
  -2 \, c \Delta \langle \delta E^4 \rangle
  -\overline{ \langle \delta E^5 \rangle }
} { 120 }
\right] \\
&=
\Delta \beta^5
\left[
  -\frac{ \overline{ \langle \delta E^3 \rangle }
          \overline{ \langle \delta E^2 \rangle } }
        { 48 }
+\frac{
  \overline{ \langle \delta E^5 \rangle }
} { 120 }
\right] + \cdots.
\end{align*}
%
Note that the terms involving $c$ or $\Delta \dots$
belong to higher orders.



\subsection{Comparison}


We compared the above equations
%Eqs. \eqref{eq:df_eav}, \eqref{eq:df_eavb}, and \eqref{eq:df_eavc}
with UIM\cite{kastner2005, *kastner2009}
(which is also an approximate method)
on the $32\times 32$ Ising model,
%
using the exact solution\cite{
ferdinand1969}
(since we wish to study
the accuracy, not precision).
%
In Fig. \ref{fig:is2approx}(a),
we show that
Eqs. \eqref{eq:df_eavb},
\eqref{eq:df_eavc},
and UIM
produced more accurate results than
Eq. \eqref{eq:df_eav}.
%
With $\Delta T = 0.2$,
the error of Eq. \eqref{eq:df_eav}
was up to $O(1)$.
%
The error of
Eqs. \eqref{eq:df_eavb} and \eqref{eq:df_eavc}
increased with temperature spacing $\Delta T$,
whereas
UIM was less sensitive.
%
However, for a small spacing,
Eqs. \eqref{eq:df_eavb} and \eqref{eq:df_eavc}
were superior.
%
This suggests that with ample data,
UIM should be replaced
by a more accurate alternative, e.g., ST-WHAM or WHAM.



\begin{figure}[h]
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/is2approx.pdf}
  }
  \caption{
    \label{fig:is2approx}
    (a) and (b) Errors of the dimensionless free energies, $f_i$,
    from Eqs. \eqref{eq:df_eav}, \eqref{eq:df_eavb}, \eqref{eq:df_eavc},
    and UIM for the $32\times 32$ Ising model,
    $\Delta T = 0.05$ and $0.2$, respectively.
    %
    The temperatures $T_i$ are uniformly distributed
    in the range $[1.5, 3.1]$.
    %
    (c) Energy histograms at $\Delta T = 0.2$.
    %
    $f_1$ at the lowest temperature $T = 1.5$ is fixed at zero.
    %
    Lines are to guide the eyes.
  }
\end{figure}





\subsection{Use approximate formulae in simulated tempering}



Finally, we give a cautionary comment
on using Eq. \eqref{eq:df_eav}
for the weight in simulated tempering\cite{park2007}.
%
Equation \eqref{eq:df_eavb} shows that
the error of Eq. \eqref{eq:df_eav}
is of order $N \, \Delta \beta_i^3$
of a system of size $N$,
since
$\partial \langle \delta E^2 \rangle / \partial \beta
= -\partial^2 \langle E \rangle / \partial \beta^2$
is extensive.
%
Thus,
if Eq. \eqref{eq:df_eav} is applied to
an $O(1)$ temperature range
with an average spacing $\Delta \beta$,
the accumulative error of $f_i$
is $O(N \, \Delta \beta^2)$.
%
If the simulation temperatures
are arranged
such that the energy distributions
of neighboring temperatures
barely overlap,
%
the average energy difference,
$\Delta \langle E \rangle_i = \langle E \rangle_{i+1} - \langle E \rangle_i$,
between neighboring temperatures
is roughly a multiple of the width of
the energy distribution
$\sqrt{ \langle \delta E^2 \rangle_i } \propto O(\sqrt{N})$.
%
So $\Delta \beta_i$,
given by $\Delta \langle E \rangle_i / |\partial E/\partial \beta|$,
is $O(1/\sqrt{N})$,
%
and the accumulative error
of $f_i$ is $O(1)$.
%(as illustrated in Fig. \ref{fig:is2approx}).
%
Thus, if Eq. \eqref{eq:df_eav}
is used for the weights
in simulated tempering\cite{park2007},
the resulting temperature distribution
may deviate appreciably
from the flat one.
%
One can fix this by choosing
a finer temperature spacing or
use Eq. \eqref{eq:df_eavb}
instead.



\bibliography{simul}
\end{document}
