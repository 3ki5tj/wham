%\documentclass[reprint,superscriptaddress]{revtex4-1}
\documentclass[aip,jcp,preprint,notitlepage, superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{color}
\begin{document}




\renewcommand{\theequation}{N.\arabic{equation}}

\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}




\title{Notes on WHAM, MBAR and DIIS}

\maketitle



\section{Methods: WHAM}



\subsection{Mnemonic derivation of WHAM}



We give a mnemonic derivation of the expression
for the density of states
%
\begin{equation}
g(E)
=
\frac{
  \sum_{k = 1}^K n_k(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
}.
\label{eq:gE_WHAM}
\end{equation}
%

In ensemble $k$,
the normalized energy distribution,
$n_k(E) / N_k$,
is related to the density of states as
%
\begin{equation}
\frac{ n_k(E) } { N_k }
=
g(E) \,
\frac{ \exp(-\beta_k E) } { Z_k }.
\end{equation}
%
Thus,
we have
%
\begin{equation}
g(E)
=
\frac{ n_k(E) }
     { d_k(E) },
\label{eq:gE_single}
\end{equation}
where
$d_k(E) \equiv N_k \exp(-\beta_k E) / Z_k$.
%
Taking the average over $k$ with
weight $d_k(E) / \sum_{j = 1}^K d_j(E)$
yields Eq. \eqref{eq:gE_WHAM}.



\subsection{Derivation of Eq. (5) from Eqs. (3) and (4)}



From Eq. (3)
\begin{align*}
f_i
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K n_j(E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K \sum_{\vct x}^{(j)}
      \delta(\E(\vx) - E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
  \int
    \frac{
      \exp(-\beta_j E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    \delta(\E(\vx) - E) \, dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
    \frac{
      \exp(-\beta_i \, \E(\vx))
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k \, \E(\vx) + f_k)
    },
\end{align*}
which is Eq. (5).



\subsection{Derivation of Eq. (6) from Eq. (5)}


From Eq. (5)
\begin{align*}
f_i
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[-\Ham_0(\vx) - \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[-\Ham_0(\vx) - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\delta(\W(\vx) - W) \, dW
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W)
  \exp f_k
}
\delta(\W(\vx) - W)
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  \left[
    \sum_{\vx}^{(j)}
    \delta(\W(\vx) - W)
  \right]
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  n_j(W)
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW,
\end{align*}
which is Eq. (6).




\subsection{Statistical-temperature WHAM}



Taking the logarithm of Eq. \eqref{eq:gE_WHAM} and differentiating,
we get
\begin{equation}
(\log g)'(E)
=
\frac{ \sum_{k = 1}^K n'_k(E) }
     { \sum_{k = 1}^K n_k(E) }
+
\frac{ \sum_{k = 1}^K d_k(E) \, \beta_k }
     { \sum_{k = 1}^K d_k(E) }.
\label{eq:beta0_STWHAM}
\end{equation}
%
By Eq. \eqref{eq:gE_single},
we have
$n_k(E) = d_k(E) \, g(E)$,
which shows that
$d_k(E)$
is proportional to $n_k(E)$
for a fixed $g(E)$.
%
So we can rewrite Eq. \eqref{eq:beta0_STWHAM} as
%
\begin{equation}
(\log g)'(E)
=
\sum_{k = 1}^K c_k(E) \, \left[ (\log n_k)'(E)  + \beta_k \right],
\label{eq:beta_STWHAM}
\end{equation}
%
where
$c_k(E) \equiv n_k(E) / \sum_{l = 1}^{K} n_l(E)$.
%
This is the main result of ST-WHAM.





\section{Methods: DIIS}


\subsection{Determination of coefficients in DIIS}



The coefficients $c_i$ are determined as follows.
%
We wish to minimize
%
\[
S
=
\vct{\hat R}^2 / 2
=
\left( \sum_i c_i \, \vct R_i \right)^2 / 2,
\]
under the constraint
%
\begin{equation}
\sum_i c_i = 1
\label{eq:c_normalize}
\end{equation}



Thus we shall minimize the function
\[
  S - \lambda \sum_i c_i
\]
with respect all $c_i$.
%
Thus, we get
\begin{equation}
  \sum_j c_j \, (\vct R_j \cdot \vct R_i) - \lambda = 0,
  \label{eq:cj_DIIS}
\end{equation}


Equations \eqref{eq:cj_DIIS} and \eqref{eq:c_normalize}
can be written in matrix form
%
\begin{equation}
\left(
  \begin{array}{ccccc}
    \vct R_1 \cdot \vct R_1 &
    \vct R_1 \cdot \vct R_2 &
    \dots &
    \vct R_1 \cdot \vct R_M &
    -1 \\
    \vct R_2 \cdot \vct R_1 &
    \vct R_2 \cdot \vct R_2 &
    \dots &
    \vct R_2 \cdot \vct R_M &
    -1 \\
    \vdots &
    \vdots &
    &
    \vdots &
    -1 \\
    \vct R_M \cdot \vct R_1 &
    \vct R_M \cdot \vct R_2 &
    \dots &
    \vct R_M \cdot \vct R_M &
    -1 \\
    1 &
    1 &
    \dots &
    1 &
    0
  \end{array}
\right)
\left(
  \begin{array}{c}
    c_1 \\
    c_2 \\
    \vdots \\
    c_M \\
    \lambda
  \end{array}
\right)
=
\left(
  \begin{array}{c}
    0 \\
    0 \\
    \vdots \\
    0 \\
    1
  \end{array}
\right).
\end{equation}





\subsection{Parameters for Figure 1}



The illustration was produced according to the following potential
%
\begin{equation}
F
=
\frac{1}{2} A \, ( x - x_0 )^2
+
\frac{1}{2} B \, ( y - y_0 )^2
+
C \, ( x - x_0 ) ( y - y_0 ),
\label{eq:fig1_F}
\end{equation}
%
where
$x_0 = 3/5$,
$y_0 = 4/5$,
$A = 3/4$,
$B = 5/4$,
and
$C = 1/4$.



The two vectors are
$\vct f_1 = (1, 0)$
and
$\vct f_2 = (0, 1)$.
%
The residual vectors are
$\vct R_1 = (-1/10, 9/10)$
and
$\vct R_2 = (4/10, -1/10)$,
respectively.



We now determine
$\vct{\hat R} = \vct R_1 + \lambda \, (\vct R_2 - \vct R_1)$
with the minimal magnitude
$\| \vct{\hat R} \|$.
%
It is readily shown that
$\lambda$ satisfies
%
\[
\lambda
=
-\frac{
  \Delta \vct R \cdot \vct R_1
}
{
  \Delta \vct R \cdot \Delta \vct R
}
=
-\frac{-19/20}{5/4}
=
\frac{19}{25},
\]
%
where
$\Delta \vct R = \vct R_2 - \vct R_1 = (1/2, -1)$.
%
Thus,
\[
\vct{\hat R}
=
(1 - \lambda) \, \vct R_1
+ \lambda \, \vct R_2
=
\left(
  \frac{7}{25},
  \frac{7}{50}
\right),
\]
\[
\vct{\hat f}
=
(1 - \lambda) \, \vct f_1
+ \lambda \, \vct f_2
=
\left(
  \frac{6}{25},
  \frac{19}{25}
\right),
\]
and
\[
\vct{\hat f}
+
\vct{\hat R}
=
\left(
  \frac{13}{25},
  \frac{9}{10}
\right).
\]



We can complete squares of $F$ as
%
\begin{align*}
F
&=
\frac{1}{16}
\left\{
  \left( 5 + 3 \sqrt 2 \right)
  \left[
    \left( \sqrt 2 - 1 \right) \, \bar x
  +
    \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{1}{16}
\left\{
  \left( 3 - \sqrt 2 \right)
  \left[
    \bar x
  +
    \left( \sqrt 2 + 1 \right) \, \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{7}{2} R^2,
\end{align*}
%
where
%
\begin{align*}
\bar x
&= x - x_0
\\
&=
\left(
  \sqrt{ 5 - 3 \sqrt{2} } \, \cos \theta
  +
  \sqrt{ 5 + 3 \sqrt{2} } \, \sin \theta
\right) R
\\
&=
\left[
  \left( \sqrt 2 - 1 \right) \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  +
  \left( \sqrt 2 + 1 \right) \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right] R,
\end{align*}
%
and
%
\begin{align*}
\bar y
&= y - y_0
\\
&=
\left(
  \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  -
  \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right) R,
\end{align*}



The short axis is achieved at $\theta = 0$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 - \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(\sqrt 2 - 1\right) = \pi/8$.



The long axis is achieved at $\theta = \pi/2$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 + \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(-\sqrt 2 - 1\right) = -3 \, \pi/8$.




\section{Appendix A: Probabilistic derivation}



From the unnumbered equation after Eq. (A1),
we get the posterior probability
\begin{align*}
p\left( g | \{ \vct x \} \right)
&=
\frac{
  p\left( \{ \vct x \} | g \right) \, p(g)
}
{
  p\left( \{ \vct x \} \right)
}
\\
&\propto
p\left( \{ \vct x \} | g \right).
\end{align*}
%
Taking the logarithm, we get
%
\begin{align*}
\log p\left( g | \{ \vct x \} \right)
&=
\sum_{k = 1}^K
  \sum_{\vct x}^{(k)}
    \left(
      \log g(\vct x) + \log q_k(\vct x) - \log Z_k[g]
    \right)
+ C
\\
&=
\sum_{k = 1}^K
\left(
  \sum_{\vct x}^{(k)}
  \log g(\vct x)
- N_k \log Z_k[g]
\right) + C',
\end{align*}
%
where $C$ and $C'$
are constants independent of $g$.
%
We now differentiate it with respect to $g(\vct y)$,
%
\begin{align}
\frac{
  \delta \log p\left( g | \{ \vct x \} \right)
}
{
  \delta g(\vct y)
}
&=
\sum_{k = 1}^K
\left(
  \sum_{\vct x}^{(k)}
  \frac{ \delta( \vct y - \vct x ) }
       { g(\vct y) }
  -
  \frac {N_k} { Z_k[g] }
  \frac{ \delta Z_k[g] } { \delta g(\vct y) }
\right)
= 0.
\label{eq:dpdg}
\end{align}
%
Now since
\begin{equation}
Z_k[g]
=
\int g(\vct y) \, q_k(\vct y) \, d\vct y,
\tag{A1}
\label{eq:Zg}
\end{equation}
we have
\begin{equation*}
\frac{ \delta Z_k[g] } { \delta g(\vct y) }
= q_k(\vct y).
\end{equation*}
%
Using this in Eq. \eqref{eq:dpdg},
we get
\begin{equation}
g(\vct y)
=
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vct x - \vct y)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct y) / Z_k[g]
}.
\tag{A2}
\label{eq:gy}
\end{equation}
%
Finally,
we use Eq. \eqref{eq:gy} in Eq. \eqref{eq:Zg},
%
\begin{align}
Z_i[g]
&=
\int
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vct x - \vct y)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct y) / Z_k[g]
}
q_i(\vct y) d\vct y
\notag \\
&=
\sum_{j = 1}^K \sum_{\vct x}^{(j)}
\int
\frac{
  \delta(\vct x - \vct y)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct y) / Z_k[g]
}
q_i(\vct y) d\vct y
\notag \\
&=
\sum_{j = 1}^K \sum_{\vct x}^{(j)}
\frac{
  q_i(\vct x)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct x) / Z_k[g]
}.
\end{align}




\section{Appendix C: Approximate formulae}




\subsection{Bernoulli polynomials and numbers}



The Bernoulli numbers are defined as
%
\begin{equation}
  \frac{ t }
  {e^t - 1}
=
\sum_{n = 0}^\infty
  B_n \frac{ t^n } { n! }.
\label{eq:Bernoulli_number}
\end{equation}
%
The first few $B_n$ are\cite{
  whittaker, arfken, abramowitz, wang_specfunc}
$B_0 = 1$,
$B_1 = -1/2$,
$B_2 = 1/6$,
$B_4 = -1/30$,
$B_6 = 1/42$,
$B_8 = -1/30$,
\dots,
$B_3 = B_5 = B_7 = \dots = 0$.



\subsubsection{First few Bernoulli polynomials}



The Bernoulli polynomials are defined as
%
\begin{equation}
  \frac{ t \, e^{x t} }
  {e^t - 1}
=
\sum_{n = 0}^\infty
  B_n(x) \frac{ t^n } { n! }.
\label{eq:Bernoulli_polynomial}
\end{equation}
%
The $x = 0$ case
recovers the Bernoulli numbers,
$B_n \equiv B_n(0)$.
%
The first few polynomials are\cite{
  whittaker, arfken, abramowitz, wang_specfunc}
\begin{align*}
B_0(x) &= 1, \\
B_1(x) &= x - \frac 1 2, \\
B_2(x) &= x^2 - x + \frac 1 6, \\
B_3(x) &= x^3 - \frac 3 2 x^2 + \frac 1 2 x, \\
B_4(x) &= x^4 - 2 \, x^3 + x^2 - \frac{1}{30}, \\
B_5(x) &= x^5 - \frac 5 2 x^4 + \frac 5 3 x^3 - \frac{1}{6} x, \\
B_6(x) &= x^6 - 3 \, x^5 + \frac 5 2 x^4 - \frac 1 2 x^2 + \frac{1}{42}, \\
B_7(x) &= x^7 - \frac 7 2 x^6 + \frac 7 2 \, x^5 - \frac 7 6 x^3 + \frac 1 6 x, \\
B_8(x) &= x^8 - 4 \, x^7 + \frac{14} 3 x^6 - \frac 7 3 \, x^4 + \frac 2 3 x^2 - \frac{1}{30}. \\
\end{align*}




\subsubsection{Symmetry}



Bernoulli polynomials have the following symmetry.
%
\begin{equation*}
\sum_{n = 0}^\infty
B_n(1 - x) \, t^n/n!
=
\frac{ 2 \, t \, e^{(1 - x) \, t} }
{ e^t - 1 }
=
\frac{ 2 \, (-t) \, e^{ x \, (-t)} }
{ e^{-t} - 1 }
=
\sum_{n = 0}^\infty
B_n(x) (-t)^n / n!.
\end{equation*}
%
This shows
\[
B_n(1 - x) = (-)^n \, B_n(x).
\]
Particularly,
\[
B_n(1) = (-)^n \, B_n(0).
\]
and $B_1(1) = -B_1(0) = 1/2$, and
\begin{align}
B_{2s}(1) = B_{2s}(0).
\label{eq:Bernoulli1_even}
\end{align}



\subsubsection{Derivative}




Differentiating Eq. \eqref{eq:Euler_polynomial}
with respect to $x$ yields
\[
\sum_{n = 0}^\infty \frac{ B'_n(x) \, t^{n} } { n! }.
=
\frac{ t^2 \, e^{x t} }
{ e^t - 1 }
=
\sum_{n = 0}^\infty \frac{ B_n(x) \, t^{n + 1} } { n! }.
\]
Comparing the coefficients of $t^n$ yields
\[
B'_n(x)
=
n \, B_{n-1}(x).
\]
and
\begin{equation}
  B^{(p)}_n(x)
=
\frac{n!}{(n-p)!} \, B_{n-p}(x).
\label{eq:dBernoulli_polynomial}
\end{equation}
Particularly,
$B^{(n)}_n(x) = n!$.



\subsection{Euler polynomials}



The Euler polynomials are similarly defined
%
\begin{equation}
  \frac{ 2 \, e^{x t} }
  {e^t + 1}
=
\sum_{n = 0}^\infty
  E_n(x) \frac{ t^n } { n! }.
\label{eq:Euler_polynomial}
\end{equation}
%
Again, we shall first study the $x = 0$ case.
%
\begin{equation}
  \frac{ 2 }
  {e^t + 1}
=
\sum_{n = 0}^\infty
  E_n(0) \frac{ t^n } { n! }.
\label{eq:Euler0}
\end{equation}



\subsubsection{Evaluate $E_n(0)$}



Since
\[
\frac{ 2 } {e^t + 1} - 1
=
-\frac{ e^t - 1 } {e^t + 1}
=
-\tanh \frac t 2,
\]
is an odd function,
$E_{n}(0) = 0$
for $n = 2, 4, 6, \dots$.



To evaluate $E_n(0)$,
we multiply Eq. \eqref{eq:Euler0}
by
\[
e^t + 1
=
\sum_{m = 0}^\infty
  (1 + \delta_{m0}) \frac{ t^m }{ m! }.
\]
This yields
\[
E_n(0)
=
-\frac{1}{2}
\sum_{k = 0}^{n - 1}
{n \choose k} E_k(0).
\]
The first values are
$E_0(0) = 1$,
$E_1(0) = -1/2$,
$E_3(0) = 1/4$,
$E_5(0) = -1/2$,
$E_7(0) = 17/8$,
\dots,
%
and
%
\begin{equation}
\frac 2 {e^t + 1}
=
1 - \frac 1 2 t
+ \frac 1 4 \frac{ t^3 }{ 3! }
- \frac 1 2 \frac{ t^5 }{ 5! }
+ \frac{ 17 }{ 8 } \frac{ t^7 }{ 7! }
- \dots.
\label{eq:Euler0_first}
\end{equation}


The numbers can also be derived from the Bernoulli numbers
%
Expanding both sides of
\[
t \times \frac{ 2 }{ e^t + 1}
=
2 \times \left(
  \frac{ t }{ e^t - 1 }
  -
  \frac{ 2 \, t } { e^{2 \, t} - 1 }
\right)
\]
as series of $t$ yields
\[
E_n(0)
=
\frac{ 2 \, B_{n + 1} } { n + 1 }
\left( 1 - 2^{n + 1} \right).
\]
%
For $n = 2 s - 1$,
\[
E_{2 s - 1}(0)
=
\frac{ B_{2 s} } { s }
\left( 1 - 4^s \right).
\]



\subsubsection{First few Euler polynomials}




Multiplying Eq. \eqref{eq:Euler0_first}
by $e^{x t} = \sum_{m = 0}^\infty x^m \, t^m/m!$ yields
the first few Euler polynomials\cite{
abramowitz, wang_specfunc}
\begin{align*}
  E_0(x) &= 1, \\
  E_1(x) &= x -\frac 1 2, \\
  E_2(x) &= x^2 - x, \\
  E_3(x) &= x^3 - \frac 3 2 x^2 + \frac 1 4, \\
  E_4(x) &= x^4 - 2 \, x^3  + x, \\
  E_5(x) &= x^5 - \frac 5 2 x^4 + \frac 5 2 x^2 -\frac 1 2, \\
  E_6(x) &= x^6 - 3 \, x^5 + 5 \, x^3 - 3 \, x, \\
  E_7(x) &= x^7 - \frac 7 2 x^6 + \frac{35}{4} x^4 - \frac{21}{2} x^2 + \frac{ 17 } 8, \\
  E_8(x) &= x^8 - 4 \, x^7 + 14 \, x^5 - 28 \, x^3 + 17 \, x.
\end{align*}



\subsubsection{Symmetry}



Euler polynomials have the following symmetry.
%
\begin{equation*}
\sum_{n = 0}^\infty
E_n(1 - x) \, t^n/n!
=
\frac{ 2 \, e^{(1 - x) \, t} }
{ e^t + 1 }
=
\frac{ 2 \, e^{ x \, (-t)} }
{ e^{-t} + 1 }
=
\sum_{n = 0}^\infty
E_n(x) (-t)^n / n!.
\end{equation*}
%
This shows
\[
E_n(1 - x) = (-)^n \, E_n(x).
\]
Particularly,
\[
E_n(1) = (-)^n \, E_n(0).
\]
and
\begin{equation}
E_{2s - 1}(1) = - \, E_{2s - 1}(0).
\label{eq:Euler1_odd}
\end{equation}



\subsubsection{Derivative}




Differentiating Eq. \eqref{eq:Euler_polynomial}
with respect to $x$ yields
\[
\sum_{n = 0}^\infty \frac{ E'_n(x) \, t^{n} } { n! }.
=
\frac{ 2 \, t \, e^{x t} }
{ e^t + 1 }
=
\sum_{n = 0}^\infty \frac{ E_n(x) \, t^{n + 1} } { n! }.
\]
Comparing the coefficients of $t^n$ yields
\[
E'_n(x)
=
n \, E_{n-1}(x).
\]
and
\begin{equation}
  E^{(p)}_n(x)
=
\frac{n!}{(n-p)!} \, E_{n-p}(x).
\label{eq:dEuler_polynomial}
\end{equation}
Particularly,
$E^{(n)}_n(x) = n!$.




\subsection{Darboux's formula}



The Darboux's formula\cite{
whittaker, wang_specfunc}
is
%
\begin{align}
\psi^{(2n)}(0) [f(b) - f(a)]
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \psi^{(2n - m)}(1) \, f^{(m)}(b)
    -
    \psi^{(2n - m)}(0) \, f^{(m)}(a)
  \right]
\notag \\
&
+h^{2m + 1}
\int_0^1
  \psi(t) \, f^{(2n+1)}(a + t \, h) \, dt,
\label{eq:Darboux}
\end{align}
where
$\psi(x)$ is a polynomial of $2n$ degrees,
and $h = b - a$.



To show Eq. \eqref{eq:Darboux},
we start from
%
\begin{align*}
&\frac{ d } { dt }
\left[
  (-)^{m - 1} h^m \,
  \psi^{(2 n - m)}(t) \, f^{(m)}(a + t \, h)
\right]
\\
&=
(-)^{m-1} h^m \,
  \psi^{(2 n - m + 1)}(t) \, f^{(m)}(a + t \, h)
-
(-)^m h^{m + 1} \,
  \psi^{(2 n - m)}(t) \, f^{(m + 1)}(a + t \, h).
\end{align*}
%
Summing $m$ from $1$ to $2 n$ yields
%
\begin{align*}
&\sum_{m = 1}^{2 n}
\frac{ d } { dt }
  \left[
    (-)^{m - 1} h^m \,
    \psi^{(2 n - m)}(t) \, f^{(m)}(a + t \, h)
  \right]
\\
&=
h \,
  \psi^{(2 n)}(t) \, f^{(1)}(a + t \, h)
-
h^{2 n + 1} \,
  \psi^{(0)}(t) \, f^{(2 n + 1)}(a + t \, h)
\\
&=
h \,
  \psi^{(2 n)}(0) \, f'(a + t \, h)
-
h^{2 n + 1} \,
  \psi(t) \, f^{(2 n + 1)}(a + t \, h),
\end{align*}
%
where
we have used the fact that
$\psi(x)$ is a polynomial of degree $2n$,
so that
$\psi^{(2 n)}(t) = \psi^{(2 n)}(0)$.
%
Integrating $t$ from $0$ to $1$
yields Eq. \eqref{eq:Darboux}.



\subsubsection{Using Bernoulli polynomials}



Using $\psi(t) = B_{2n}(t)/(2n)!$ in Eq. \eqref{eq:Darboux},
we get
\begin{align}
f(b) - f(a)
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \frac{ B_m(1) } {m!} \, f^{(m)}(b)
    -
    \frac{ B_m(0) } {m!} \, f^{(m)}(a)
  \right]
\notag \\
&=
\frac{ h \, \left[
    f'(b) + f'(a)
  \right]
} {2 }
-
\sum_{s = 1}^{n}
\, h^{2 s}
\frac{ B_{2 s} }{ (2 s)! }
    \, \left[f^{(2 s)}(b) - f^{(2 s)}(a)\right]
  + \dots,
\label{eq:Darboux_Bernoulli}
\end{align}
where we have used
Eqs. \eqref{eq:dBernoulli_polynomial} and \eqref{eq:Bernoulli1_even}.
%
In the main text,
we set $b = \beta_{i + 1}$,
$a = \beta_i$.



\subsubsection{Using Euler polynomials}



Using $\psi(t) = E_{2n}(t)/(2n)!$ in Eq. \eqref{eq:Darboux},
we get
\begin{align}
f(b) - f(a)
&=
\sum_{m = 1}^{2n}
  (-)^{m - 1} h^m
  \left[
    \frac{ E_m(1) } {m!} \, f^{(m)}(b)
    -
    \frac{ E_m(0) } {m!} \, f^{(m)}(a)
  \right]
\notag \\
&=
-\sum_{s = 1}^{n}
h^{2 s - 1}
\frac{ E_{2 s - 1}(0) }{ (2 s - 1)! }
    \, \left[f^{(2 s - 1)}(b) + f^{(2 s - 1)}(a)\right]
  + \dots,
\label{eq:Darboux_Euler}
\end{align}
where we have used
Eqs. \eqref{eq:dEuler_polynomial} and \eqref{eq:Euler1_odd}.
%
In the main text,
we set $b = \beta_{i + 1}$,
$a = \beta_i$,
and $E_{2s - 1} \rightarrow a_s$.



\subsection{Solution to the BAR equation}



For two temperatures, $\beta_0$ and $\beta_1$,
the BAR or WHAM equation reads
%
\begin{equation*}
\int
  \frac{ \rho_j(E) + \rho_{j+1}(E) }
  { 1 + \exp( -\Delta \beta_j \, E + \Delta f_j ) }
  \, d E
= 1,
\end{equation*}
%
or
%
\begin{equation*}
\int
  \frac{ \rho_0(E) }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
  \, d E
=
\int
  \frac{ \rho_1(E) }
  { 1 + \exp( \Delta \beta \, E - \Delta f ) }
  \, d E,
\end{equation*}
%
where
$\Delta \beta = \beta_1 - \beta_0$,
and
$\Delta f = f_1 - f_0$.
%
In terms of averages,
\begin{equation}
\left\langle
  \frac{ 2 }
  { 1 + \exp( -\Delta \beta \, E + \Delta f ) }
\right\rangle_0
=
\left\langle
  \frac{ 2 }
  { 1 + \exp( \Delta \beta \, E - \Delta f ) }
\right\rangle_1
\label{eq:bar}
\end{equation}
%
We expand $\Delta f$ in terms of powers of $\Delta \beta$:
\[
\Delta f
=
\sum_{n = 1}^\infty f_n \, \Delta \beta^n.
\]



For convenience,
we define
\[
\phi_n \equiv (f_n - \delta_{n,1} \Delta E) \, \Delta \beta^n
\]
and
\[
\phi = \sum_{n = 1}^\infty \phi_n.
\]
Thus, Eq. \eqref{eq:bar} can be written as
\begin{align*}
\left\langle
  \frac{ 2 }
  { 1 + \exp \phi  }
\right\rangle_0
=
\left\langle
  \frac{ 2 }
  { 1 + \exp -\phi }
\right\rangle_1
\label{eq:bar1}
\end{align*}


We now expand the left-hand side as
%
\begin{align*}
\left\langle
  \frac{ 2 }
  { 1 + \exp \phi  }
\right\rangle_0
&=
\left\langle
\sum_{n = 0}^\infty
E_n(0) \frac{ \phi^n } { n! }
\right\rangle_0
\notag \\
&=
\sum_{n = 0}^\infty
\sum_{n_1 + n_2 + \dots = n}
E_n(0)
\frac{ \langle \phi_1^{n_1} \rangle_0 \,
      \phi_2^{n_2} \, \cdots } { n_1! \, n_2! \, \cdots }.
\end{align*}
%
Note that only $\phi_1$ depends on $E$,
so the averaging only applies to $\phi_1$ and its powers.
%
The right-hand side
can be similarly expanded
with substitutions
$\phi_k \rightarrow -\phi_k$,
and
$\langle \dots \rangle_0 \rightarrow \langle \dots \rangle_1$.



We collect terms proportional $\Delta \beta^M$
and let $M$ increase gradually from 1.
%
Since $\phi_k \propto \Delta \beta^k$,
we are collecting terms with
\begin{equation}
n_1 + 2 \, n_2 + 3 \, n_3 + \dots = M,
\label{eq:sumn_M}
\end{equation}
Also,
for $n \ge 1$,
$E_n(0) = 0$ if $n$ is even,
thus $n$ has to be odd.



\subsubsection{$M = 1$}



For $M = 1$,
we have $n_1 = 1$,
\[
\langle \phi_1 \rangle_0
=
-\langle \phi_1 \rangle_1.
\]
or
\[
f_1 =
\frac{
  \langle E \rangle_0
  +
  \langle E \rangle_1
}{ 2 }.
\]



\subsubsection{Parity}



For $M = 2$,
we have only the term $n_1 = 0, n_2 = 1$,
because the term $n_1 = 2, n_2 = 0$
makes $n = n_1 + n_2$ even,
and $E_n(0) = 0$.
%
Thus,
\[
\phi_2
=
-\phi_2,
\]
or $\phi_2 = 0$.



Generally,
for any even $M$,
$\phi_M = 0$.
%
To see that, we carry an induction on even $M$
%
If $M$ is even,
we find from Eq. \eqref{eq:sumn_M} that
$n_1 + n_3 + \cdots + n_{M - 1}$ must be even.
But
$n = n_1 + n_3 + \cdots + n_{M - 1} + n_M$
must be odd
(note that the induction hypothesis asserts
that $n_2 = n_4 = \cdots = n_{M-2} = 0$).
%
Thus, $n_M$ must be odd,
and to satisfy $\sum_{k = 1}^M k n_k = M$,
we have $n_1 = \dots = n_{M-1} = 0$,
and $n_M = 1$.
%
Thus,
%
\[
E_1(0) \, \phi_M
=
-E_1(0) \, \phi_M,
\]
which shows that $\phi_M = 0$.



\subsubsection{$M = 3$}



There are two relevant terms
$n_3 = 1$,
and $n_1 = 3$.
%
\[
E_1(0) \, \phi_3
+
E_3(0) \, \langle \phi_1^3 /3! \rangle_0
=
-E_1(0) \, \phi_3
-
E_3(0) \, \langle \phi_1^3 /3! \rangle_1
\]
and
\[
\phi_3
=
\frac{
\langle \phi_1^3 \rangle_0
+
\langle \phi_1^3 \rangle_1
}{24}.
\]
%
Let
$c = \left( \langle E \rangle_1 - \langle E \rangle_0 \right) / 2$,
For $\beta_0$
$\phi_1 = \Delta \beta \, (c - \delta E)$,
where
$\delta E = E - \langle E \rangle_0$.
%
For $\beta_1$,
$\phi_1 = \Delta \beta \, (-c -\delta E)$
where
$\delta E = E - \langle E \rangle_1$.
%
So
\[
\phi_3
=
-
\frac{
\langle \delta E^3 \rangle_0
+
\langle \delta E^3 \rangle_1
}{24}
-
\frac{
  \left(
    \langle E \rangle_1
    -
    \langle E \rangle_0
  \right)
  \left(
    \langle \delta E^2 \rangle_1
    -
    \langle \delta E^2 \rangle_0
  \right)
}{16}.
\]





\bibliography{simul}
\end{document}
