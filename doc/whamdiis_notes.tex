%\documentclass[reprint,superscriptaddress]{revtex4-1}
\documentclass[aip,jcp,preprint,superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{color}
\begin{document}




\renewcommand{\theequation}{N.\arabic{equation}}

\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}




\title{Notes}

\maketitle



\section{Mnemonic derivation of WHAM}



We give a mnemonic derivation of the expression
for the density of states
%
\begin{equation}
g(E)
=
\frac{
  \sum_{k = 1}^K n_k(E)
}
{
  \sum_{k = 1}^K N_k \, \exp(-\beta_k E) / Z_k
}.
\label{eq:gE_WHAM}
\end{equation}
%

In ensemble $k$,
the normalized energy distribution,
$n_k(E) / N_k$,
is related to the density of states as
%
\begin{equation}
\frac{ n_k(E) } { N_k }
=
g(E) \,
\frac{ \exp(-\beta_k E) } { Z_k }.
\end{equation}
%
Thus,
we have
%
\begin{equation}
g(E)
=
\frac{ n_k(E) }
     { d_k(E) },
\label{eq:gE_single}
\end{equation}
where
$d_k(E) \equiv N_k \exp(-\beta_k E) / Z_k$.
%
Taking the average over $k$ with
weight $d_k(E) / \sum_{j = 1}^K d_j(E)$
yields Eq. \eqref{eq:gE_WHAM}.



\section{Derivation of Eq. (5) from Eqs. (3) and (4)}



From Eq. (3)
\begin{align*}
f_i
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K n_j(E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \int
    \frac{
      \sum_{j = 1}^K \sum_{\vct x}^{(j)}
      \delta(\E(\vx) - E) \, \exp(-\beta_i E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
  \int
    \frac{
      \exp(-\beta_j E)
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k E + f_k)
    }
    \delta(\E(\vx) - E) \, dE
\\
&=
-\log
  \sum_{j = 1}^K \sum_{\vct x}^{(j)}
    \frac{
      \exp(-\beta_i \, \E(\vx))
    }
    {
      \sum_{k = 1}^K N_k \, \exp(-\beta_k \, \E(\vx) + f_k)
    },
\end{align*}
which is Eq. (5).



\section{Derivation of Eq. (6) from Eq. (5)}


From Eq. (5)
\begin{align*}
f_i
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  q_i(\vx)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vx) \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[-\Ham_0(\vx) - \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[-\Ham_0(\vx) - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp[- \lambda_i \W(\vx)]
}
{
  \sum_{k = 1}^K N_k \,
  \exp[ - \lambda_k \W(\vx)]
  \exp f_k
}
\delta(\W(\vx) - W) \, dW
\\
&=
-\log
\sum_{j = 1}^K
\sum_{\vx}^{(j)}
\int
\frac{
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W)
  \exp f_k
}
\delta(\W(\vx) - W)
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  \left[
    \sum_{\vx}^{(j)}
    \delta(\W(\vx) - W)
  \right]
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW
\\
&=
-\log
\int
\frac{
  \sum_{j = 1}^K
  n_j(W)
  \exp(- \lambda_i W)
}
{
  \sum_{k = 1}^K N_k \,
  \exp( - \lambda_k \W + f_k )
}
\, dW,
\end{align*}
which is Eq. (6).




\section{Statistical-temperature WHAM}



Taking the logarithm of Eq. \eqref{eq:gE_WHAM} and differentiating,
we get
\begin{equation}
(\log g)'(E)
=
\frac{ \sum_{k = 1}^K n'_k(E) }
     { \sum_{k = 1}^K n_k(E) }
+
\frac{ \sum_{k = 1}^K d_k(E) \, \beta_k }
     { \sum_{k = 1}^K d_k(E) }.
\label{eq:beta0_STWHAM}
\end{equation}
%
By Eq. \eqref{eq:gE_single},
we have
$n_k(E) = d_k(E) \, g(E)$,
which shows that
$d_k(E)$
is proportional to $n_k(E)$
for a fixed $g(E)$.
%
So we can rewrite Eq. \eqref{eq:beta0_STWHAM} as
%
\begin{equation}
(\log g)'(E)
=
\sum_{k = 1}^K c_k(E) \, \left[ (\log n_k)'(E)  + \beta_k \right],
\label{eq:beta_STWHAM}
\end{equation}
%
where
$c_k(E) \equiv n_k(E) / \sum_{l = 1}^{K} n_l(E)$.
%
This is the main result of ST-WHAM.




\section{Determination of coefficients in DIIS}



The coefficients $c_i$ are determined as follows.
%
We wish to minimize
%
\[
S
=
\vct{\hat R}^2 / 2
=
\left( \sum_i c_i \, \vct R_i \right)^2 / 2,
\]
under the constraint
%
\begin{equation}
\sum_i c_i = 1
\label{eq:c_normalize}
\end{equation}



Thus we shall minimize the function
\[
  S - \lambda \sum_i c_i
\]
with respect all $c_i$.
%
Thus, we get
\begin{equation}
  \sum_j c_j \, (\vct R_j \cdot \vct R_i) - \lambda = 0,
  \label{eq:cj_DIIS}
\end{equation}


Equations \eqref{eq:cj_DIIS} and \eqref{eq:c_normalize}
can be written in matrix form
%
\begin{equation}
\left(
  \begin{array}{ccccc}
    \vct R_1 \cdot \vct R_1 &
    \vct R_1 \cdot \vct R_2 &
    \dots &
    \vct R_1 \cdot \vct R_M &
    -1 \\
    \vct R_2 \cdot \vct R_1 &
    \vct R_2 \cdot \vct R_2 &
    \dots &
    \vct R_2 \cdot \vct R_M &
    -1 \\
    \vdots &
    \vdots &
    &
    \vdots &
    -1 \\
    \vct R_M \cdot \vct R_1 &
    \vct R_M \cdot \vct R_2 &
    \dots &
    \vct R_M \cdot \vct R_M &
    -1 \\
    1 &
    1 &
    \dots &
    1 &
    0
  \end{array}
\right)
\left(
  \begin{array}{c}
    c_1 \\
    c_2 \\
    \vdots \\
    c_M \\
    \lambda
  \end{array}
\right)
=
\left(
  \begin{array}{c}
    0 \\
    0 \\
    \vdots \\
    0 \\
    1
  \end{array}
\right).
\end{equation}





\section{Parameters for Figure 1}



The illustration was produced according to the following potential
%
\begin{equation}
F
=
\frac{1}{2} A \, ( x - x_0 )^2
+
\frac{1}{2} B \, ( y - y_0 )^2
+
C \, ( x - x_0 ) ( y - y_0 ),
\label{eq:fig1_F}
\end{equation}
%
where
$x_0 = 3/5$,
$y_0 = 4/5$,
$A = 3/4$,
$B = 5/4$,
and
$C = 1/4$.



The two vectors are
$\vct f_1 = (1, 0)$
and
$\vct f_2 = (0, 1)$.
%
The residual vectors are
$\vct R_1 = (-1/10, 9/10)$
and
$\vct R_2 = (4/10, -1/10)$,
respectively.



We now determine
$\vct{\hat R} = \vct R_1 + \lambda \, (\vct R_2 - \vct R_1)$
with the minimal magnitude
$\| \vct{\hat R} \|$.
%
It is readily shown that
$\lambda$ satisfies
%
\[
\lambda
=
-\frac{
  \Delta \vct R \cdot \vct R_1
}
{
  \Delta \vct R \cdot \Delta \vct R
}
=
-\frac{-19/20}{5/4}
=
\frac{19}{25},
\]
%
where
$\Delta \vct R = \vct R_2 - \vct R_1 = (1/2, -1)$.
%
Thus,
\[
\vct{\hat R}
=
(1 - \lambda) \, \vct R_1
+ \lambda \, \vct R_2
=
\left(
  \frac{7}{25},
  \frac{7}{50}
\right),
\]
\[
\vct{\hat f}
=
(1 - \lambda) \, \vct f_1
+ \lambda \, \vct f_2
=
\left(
  \frac{6}{25},
  \frac{19}{25}
\right),
\]
and
\[
\vct{\hat f}
+
\vct{\hat R}
=
\left(
  \frac{13}{25},
  \frac{9}{10}
\right).
\]



We can complete squares of $F$ as
%
\begin{align*}
F
&=
\frac{1}{16}
\left\{
  \left( 5 + 3 \sqrt 2 \right)
  \left[
    \left( \sqrt 2 - 1 \right) \, \bar x
  +
    \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{1}{16}
\left\{
  \left( 3 - \sqrt 2 \right)
  \left[
    \bar x
  +
    \left( \sqrt 2 + 1 \right) \, \bar y
  \right]^2
  +
  \left( 3 + \sqrt 2 \right)
  \left[
    \bar x
  -
    \left( \sqrt 2 - 1 \right) \, \bar y
  \right]^2
\right\}
\notag \\
%
&=
\frac{7}{2} R^2,
\end{align*}
%
where
%
\begin{align*}
\bar x
&= x - x_0
\\
&=
\left(
  \sqrt{ 5 - 3 \sqrt{2} } \, \cos \theta
  +
  \sqrt{ 5 + 3 \sqrt{2} } \, \sin \theta
\right) R
\\
&=
\left[
  \left( \sqrt 2 - 1 \right) \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  +
  \left( \sqrt 2 + 1 \right) \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right] R,
\end{align*}
%
and
%
\begin{align*}
\bar y
&= y - y_0
\\
&=
\left(
  \sqrt{ 3 + \sqrt{2} } \, \cos \theta
  -
  \sqrt{ 3 - \sqrt{2} } \, \sin \theta
\right) R,
\end{align*}



The short axis is achieved at $\theta = 0$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 - \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(\sqrt 2 - 1\right) = \pi/8$.



The long axis is achieved at $\theta = \pi/2$,
with
$\sqrt{ {\bar x}^2 + {\bar y}^2 } = \sqrt{ 8 + \sqrt 8 } R$,
and
$\phi = \arctan(\bar y/\bar x) = \arctan \left(-\sqrt 2 - 1\right) = -3 \, \pi/8$.




\section{Probabilistic derivation}



From the unnumbered equation after Eq. (A1),
we get the posterior probability
\begin{align*}
p\left( g | \{ \vct x \} \right)
&=
\frac{
  p\left( \{ \vct x \} | g \right) \, p(g)
}
{
  p\left( \{ \vct x \} \right)
}
\\
&\propto
p\left( \{ \vct x \} | g \right).
\end{align*}
%
Taking the logarithm, we get
%
\begin{align*}
\log p\left( g | \{ \vct x \} \right)
&=
\sum_{k = 1}^K
  \sum_{\vct x}^{(k)}
    \left(
      \log g(\vct x) + \log q_k(\vct x) - \log Z_k[g]
    \right)
+ C
\\
&=
\sum_{k = 1}^K
\left(
  \sum_{\vct x}^{(k)}
  \log g(\vct x)
- N_k \log Z_k[g]
\right) + C',
\end{align*}
%
where $C$ and $C'$
are constants independent of $g$.
%
We now differentiate it with respect to $g(\vct y)$,
%
\begin{align}
\frac{
  \delta \log p\left( g | \{ \vct x \} \right)
}
{
  \delta g(\vct y)
}
&=
\sum_{k = 1}^K
\left(
  \sum_{\vct x}^{(k)}
  \frac{ \delta( \vct y - \vct x ) }
       { g(\vct y) }
  -
  \frac {N_k} { Z_k[g] }
  \frac{ \delta Z_k[g] } { \delta g(\vct y) }
\right)
= 0.
\label{eq:dpdg}
\end{align}
%
Now since
\begin{equation}
Z_k[g]
=
\int g(\vct y) \, q_k(\vct y) \, d\vct y,
\tag{A1}
\label{eq:Zg}
\end{equation}
we have
\begin{equation*}
\frac{ \delta Z_k[g] } { \delta g(\vct y) }
= q_k(\vct y).
\end{equation*}
%
Using this in Eq. \eqref{eq:dpdg},
we get
\begin{equation}
g(\vct y)
=
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vct x - \vct y)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct y) / Z_k[g]
}.
\tag{A2}
\label{eq:gy}
\end{equation}
%
Finally,
we use Eq. \eqref{eq:gy} in Eq. \eqref{eq:Zg},
%
\begin{align}
Z_i[g]
&=
\int
\frac{
  \sum_{j = 1}^K \sum_{\vct x}^{(j)} \delta(\vct x - \vct y)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct y) / Z_k[g]
}
q_i(\vct y) d\vct y
\notag \\
&=
\sum_{j = 1}^K \sum_{\vct x}^{(j)}
\int
\frac{
  \delta(\vct x - \vct y)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct y) / Z_k[g]
}
q_i(\vct y) d\vct y
\notag \\
&=
\sum_{j = 1}^K \sum_{\vct x}^{(j)}
\frac{
  q_i(\vct x)
}
{
  \sum_{k = 1}^K N_k \, q_k(\vct x) / Z_k[g]
}.
\end{align}




%\bibliography{simul}
\end{document}
